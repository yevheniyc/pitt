Source: https://www.coursera.org/learn/the-art-of-data-visualization/lecture/lQslM/loading-and-preparing-time-series-data-in-python

English (auto)
In this lesson, we're going to see how we can actually process
some of the time series data that we talked about
in the previous lesson using Python libraries.
So we are going to first start by learning how we can
load and prepare time series data in Python.
So why do we want to use Python for time series analysis?
Python brings a host of benefits regarding time series analysis.
It's very user-friendly to use.
It has extensive library support that you can actually
combine different libraries to do these different processing.
It creates code that is reusable and can actually
make use of codes that other people have developed as well.
So it actually provides a lot of benefits
to learn how to do time series analysis in Python.
So first, let's look at how we explore time series data.
So we're going to go through the code that basically
goes step by step into this loading the data
and processing of the data.
The first step is to load the data.
And because this is time series data,
you want to make sure that your data has a column.
In this case, you can see here that we
are using Pandas data library in Python to load the data.
And then what we are doing is that we
would make sure that there is actually
a column that has the index of date
in what you can see in the second line as well.
Then we can use the data functions
to explore the data set that we just loaded.
This is helpful to understand what
the structure of your data set is
and help to check for the overall structure
and getting an initial understanding of what
the data distribution is.
So we can use functions like data.head,
which shows the first few rows to see
what is in the data.
We can use the data.info, which shows the data types
and checks for missing values and provides information
on that.
And then you can use data.describe
to view summary statistics as well.
So here is some examples.
The data set that we are using here,
which is available for you.
And you can actually follow along
and look at these parts of the code
as we go through the slides.
So the data set is a water usage data set.
The first one, when we look at the data.head
to see the first few rows, you can
see that the data has these 1, 2, 3, 4, 5, 6 columns in addition
to the date column.
And you can see information that is showing the total gallons
of water that are used on a specific date,
the gallons of water that are used
in residential areas, in multifamily households,
in commercial places, in industrial places,
and in public authority places.
Then the data.info provides additional information
about the data.
You can see that here basically saying how many maybe missing
values exist, that in this case does not actually
have any missing values.
And basically showing that 600 non-values
exist for each of our columns and the type that
is basically just a number for each of those.
The next one is showing, again, the count for each column,
the average standard deviation minimum, the 25 percentiles,
the 50 percentiles, and the 75 percentile on the maximum.
So this gives you a lot of view of your data as well.
Now, if there are actually any missing values
before you start looking at your time series,
you want to handle those missing values.
And there are different things that you can
do with the missing values.
You can either remove any rows that one value is missing.
So this line of code, the drop, and A basically
saying that if there is any value in one row missing,
it will basically just remove that entire row.
You can also drop columns similar.
So by adding this access equal 1,
changes this function to dropping columns.
And in this case, would drop columns that missing any rows.
You can also specify how many non-values
are required to retain a row or a column
instead of just at least one.
So you can provide that threshold as one parameter.
And you can also remove based on a specific column.
So maybe you don't care if one column is missing,
but some other columns are more important.
So you can also say which columns basically are important
and you want to drop the data in case that missing value exists.
So this is a good thing to do at the beginning of using your data
and processing the data to make sure
that the data is in order before you start
doing any time series analysis.
As we mentioned in the previous lesson,
you want to also check for stationarity
because that's the basis for a lot of assumption
of the analysis.
And there are statistical models that
allow you to test for that.
So here, you can use the stats models library of Python
to load statistical functions for testing,
which is AD Fuller.
That is basically ADF.
That's a test function from the stats model library
that checks if the time series has
a unique route, which is indicative
of non-stationarity.
So what you want to look here is for the p-value,
testing the statistical significance of this ADF test.
And if the p-value is less than 0.05,
then the data is likely stationary
and you can go ahead.
That's kind of the, if you run this kind of code,
you are actually going to get the p-value
and that will give you information
about whether your time series is stationary or not.
And sometimes you also need to do different sorts
of transformation on the data, whether that's
to smooth the data or to resample it,
to, for example, look at seasonality or cyclicality.
So you can do different kinds of transformation.
Here, we're going to look at the log transformation that
often is used to smooth the data and resampling that
could be actually used to look at the seasonality
or cyclicality of the time series data.
For the log transformation, you can
apply natural logarithms, which is base e,
to each of the data points.
So that's what the log transformation would do.
What it does conceptually is that it stabilizes variance.
Log transformation reduces changing variance
by compressing the large values more than the smaller values.
So you don't have those very strong peaks in the data
and it's easier to then look at the patterns
and the trends in the data.
It reduces the odd layers by actually removing
those large spikes.
And so then if there are actually larger spikes,
you know kind of what the noise in the data
and what the odd layers in the data is.
It improves the linearity of the data,
usually makes the data more linear,
and can make the exponential or multiplicative trends
linear and easier to observe.
And that kind of leads into simplifying trends.
And all you do is basically just use the log function
from non-Py library of Python to do
any sort of log transformation on any
of the columns of the data.
For the resampling, that's the process
of changing the frequency of your time series in your data.
And you can do both downsampling and upsampling.
Downsampling means that you are aggregating the data
into a lower frequency.
And upsampling means that you're interpolating the data
to a higher frequency.
So here is how you can do resampling.
Again, we are going to be making use of Python libraries
of non-Py and pandas for the data set.
And so what you can do is here, just
using kind of an example data series
that we are just kind of creating a random data
that we are creating here.
And then just basically creating the date column for it
as well.
And then so in this, this is just creating the time series.
And here we are doing the downsamplings and upsampling.
So in the first one, you can see that we
are downsampling to a weekly frequency
by saying that use the resample and the W,
which is basically a notation for the weekly data.
For the upsampling, what we are doing
is that we are converting daily data to hourly data.
And in this case, you basically are doing interpolation
because there is data that has not been maybe measured.
So that's why you need to do interpolation and filling
kind of the missing values with these methods
of interpolation.
Here, what you do is that you basically
provide then what the distribution of the data
you expect to be and use kind of that distribution
to create the missing values.
So here we are going from the daily data
into the hourly data.
So that would be like if this is your original data, which
is basically daily.
When you do downsampling, you're going to have weekly data.
And when you do upsampling, you're
going to have this hourly data.
And these are the key resampling frequency
that you can use in Pandas, D for daily, W for weekly,
M for monthly, Q for quarterly, H for hourly.
And you can have T or mean for minutes and S for seconds.
The next kinds of processing that you
might want to do for time series data
is what is known as time series decomposition, which
is a powerful technique to separate different components
of the time series.
And this could be additive or multiplicative decomposition.
So when it's additive is when the components combine
as the trend plus seasonality plus the residuals.
And multiplicative when the components
interact as trend times seasonality times the residual.
So that's kind of what the structure of your data is.
Or you can decompose the data into these two different models
and look at those decomposition.
And here is the Python code that would
allow you to do decomposition.
So the seasonal decompose function
from the stats model library is used
to decompose the time series into its component
of the trend and seasonality and residuals, which
as you can see here in this line,
then you provide your data and you
can provide whether the model is additive or multiplicative.
And what is kind of the period of the time series
in this case is basically saying that it's 12 months.
And then you can actually plot the decompositions as well.
So we just talked about these are
if you look at the different parameters
of the decomposition, as I mentioned,
the first parameter is the data and the time series.
The second one is the model.
The last one is the length of one cycle in the data.
So 12 for monthly data with yearly seasonality.
You can also have the extrapolate trend,
which extrapolates the trend to match
the length of the input series.
So when you do decomposition, these
are the kinds of output that you get.
You can look at the observed.
So this is basically on the top, the actual.
So we are looking at those total gallons of water usage.
So the top graph shows the original data.
Then the second one from the top shows the trend.
The third one shows the seasonal aspects of it at the time.
And then the residuals and noise that
exists basically in the data.
So you can see, for example, that these two spikes here
are some sort of trends.
But also they are represented in the residuals.
So those are basically the noise.
And they are not part of the other patterns.