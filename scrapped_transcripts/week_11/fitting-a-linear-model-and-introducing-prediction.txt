Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/SgmvF/fitting-a-linear-model-and-introducing-prediction

English
Hello everyone.
As you can see, I have Jupyter opened to this week's directory for the class.
We will continue working with linear models.
We know how to fit a linear model, whether using linear or nonlinear features.
But now we will talk about making predictions and
visualizing those predictions from our fitted models.
This first video will use the simple linear relationship data.
The next one will show how to make predictions and visualize them for
the nonlinear relationship data.
If you don't have Jupyter started, please pause the recording,
launch Jupyter and then in the correct directory create
a new Jupyter notebook, a new Python 3 kernel.
Let's name this notebook week 10 LM for
linear model predictions_linear.
Again, the linear here is denoting we are working with linear relationships.
Next, let's put in our header information CMPINF 2100 Week 10,
making predictions with linear models.
We will make predictions for the linear relationship example.
Okay, let's import the modules.
So just as with the previous recording,
we need the big four import numpy as NP, import pandas as PD,
import Mac plotlib.py plot as PLT, and import Sea aborne as SNS.
We also need stats models to fit the model.
Import statsmodels.formula.API as SMF.
Again, statsmodels formula.SMF.
Let's read the data assigned
to the DF object, PD read CSV.
Again,, we are starting with the linear data CSV.
Now, the previous recording discussed, or
the previous series of recordings discussed,
how to fit the model with Stat's models formula interface.
I'm going to use that again, but I won't talk about it in detail, so
please watch that previous recording before seeing how to make the predictions.
So let's fit the linear model with
the linear relationship between
the single input x and the mean output of y.
I'll assign this to the object
Lm Fit SMF.ols, formula data.fit.
Again, the data argument is Df, the formula argument y tilde x.
Something that is important though, about this formula.
Notice our dataframe DF has a lot of other columns in them or inside it,
and yet the formula is what tells the function which columns to look at.
Meaning we could have as many things as we want inside the data,
and yet the formula can focus on just a few variables, a few columns.
We run this and we now have access to all of the same pieces
of information as we talked about in the previous recordings,
the estimates, the standard errors,
the p-values, and the confidence intervals.
And here I'm just changing those confidence intervals
name to something that's easier to read.
We're not going to visualize the coefficient summary here because that was
discussed in the previous recording.
Instead, what I'm interested in is making predictions.
Unfortunately, predictions are the least used tools for
interpreting model behavior.
Predictions can be made on data that
do not include observed outputs.
We can create our own data sets and
visualize the predictive trends,
predictive uncertainty,
the predictive behavior for any input value.
Essentially, what I'm trying to say
is I make data to study predictions oftentimes
as frequently as I fit a model to a given data set.
Or another way to put it, you don't have to
just predict the data used to fit the model.
We can study the behavior on brand new data that was not used to fit it.
Let's create a data set that
has more input locations
than the training set.
So I'm going to make a data frame called dfvs,
and this dfvs, again will be a data frame.
And I'll create it using the dictionary key value pair
where the input is named x, and I'll use NP linspace.
The minimum value in the sequence will come
from the minimum value in my training set.
The maximum value in the sequence will come from
the maximum value in my training set.
If I want to, I can even add in some buffer like I can go slightly
less then the overall min and I can go slightly greater
than the overall maximum, but I can define the bounds.
I can use more input values than what we have in the training data.
If I display this data frame, there are 101 rows.
Here are some representative values, and
if I compare that to the data, the training data used
to fit the model, you can see there are a lot of input
values that did not appear in the training set.