Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/Xjc5K/analyzing-the-fitted-model

English
Now, all of the other attributes and
methods for LM fit are the same.
If I want the estimates themselves,
I can use dot params.
If I want the standard errors,
I can use dot BSE.
If I want to know the p-value,
and if I therefore want to know if
the feature is statistically significant,
I can compare the p-value
to the common convention of 0.05.
Also, if I want the actual 95% confidence interval,
I can apply the lm.comf_int method and then
rename the columns where zero is CI lower,
and one is CI upper.
Again, rather than just
displaying all of this information as text,
rather than having this table,
I prefer to visualize
the coefficient summaries with a coefficient plot,
so I prefer to package it up this way.
This plot is not of my invention.
It's a common plot used in
many applied statistics and applications.
It's created with Matplotlib,
as we saw in the previous video,
and I'm just going to use
the approximation to the 95% confidence interval
via the standard error.
I'm not going to go through the steps in this video to
visualize the full correct 95% confidence interval.
Let's create the coefficient plot using
the approximation to the 95% confidence interval.
Initialize the figure and axis object, ax.errorbar,
and then I need ax- axv line,
and then I will set the x-axis label and then PLT show.
For the error bar, the y argument is LM_fit.params.
Index, so the name of the coefficients,
the x argument LM_fit.params,
the value of the estimates,
and the x error is
approximately two times the standard error.
Then Format 0. I want the markers,
not the connected lines.
I want black markers.
I want black edges on the error bars.
I want the error bar edge line width,
and then the marker size to be this.
The vertical reference line will be at x = 0.
I want a dashed line style,
I want a wide line width,
and I want a gray line color,
and the x-axis label is coefficient value.
Again, the feature that
is getting multiplied by the slope is not x here.
The coefficient is multiplying
the sine function applied
to x or the feature derived from x.
The fact that this 95% confidence interval
does not contain zero is a statement saying,
we are very confident that the sign,
the S-I-G-N of the coefficient is negative.
We are confident that the trend will
decrease as the feature increases.
Again, because we know
the real answers or
the true coefficients for this problem,
let's compare the estimates to the true values.
I'm just going to copy this to save time, paste it in,
and now ax.scatter where we're going to
show for the y argument
that will be the coefficient names and the x argument,
just as in the previous video,
we need to apply the n unique function
to df.true_intercept, and then again, the np.unique
function to the df.true_slope,
and I want this to be a rather large red marker.
In this case, the estimate for the slope
multiplying the feature derived from
x is really close to its actual value,
so the coefficient estimate
is really close to the right answer but again,
what really matters is the right answer
the true values contained
within the 95% confidence interval.
For the intercept, it's again not estimated as well,
but it's right at the boundary
of the 95% confidence interval.
The real answer is still within the margin of error.
Our estimates are therefore pretty close
to the actual values that were used to generate the data.
Again, the primary purpose of this recording
is you can use all of the same strategies
and techniques that we talked about in
the previous video even when you
want to fit a model with
a nonlinear relationship or with
a feature derived from the input.
The benefit of the stats models formula interface is
the feature can be derived right in the formula string,
and you do not need to modify
the dataset you are working
with or providing to the model.
Again, just to finalize this,
remember, df, this is not changed.
I did not calculate
the np.sine(x) and store it in this DataFrame.
That feature, np.sine(x) was
calculated for me from the formula.
I really like the formula interface
because I can very easily try out
different nonlinear functions without
touching the underlying data,
without adding new columns to my DataFrame.
You can assess if those features are statistically
significant if you're confident of their signs,
negative or positive,
using the exact same procedures that
we talked about with the conventional best-fit lines,
a linear relationship.