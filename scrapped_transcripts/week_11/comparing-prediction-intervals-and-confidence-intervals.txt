Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/HiWA5/comparing-prediction-intervals-and-confidence-intervals

English
That best fit line is drawing in between
the scatter or variability of the training data.
But remember, this is a toy problem.
Let me copy this code and
include the true trend as context.
Because again, we know the true answer.
Include the true trend ax.plot df_viz_copy.x,
my_intercept plus my_slope times
df_viz_copy.x color equals crimson.
Do you see that red line?
That's the real relationship between the trend,
the mean output, and the input.
I forgot to set the labels,
ax.set_ xlabel x, ax.set_ ylabel y,
because I have the training set here.
Our model is not perfect.
Our predicted trend is not perfect.
We are uncertain about what can happen on average.
The real answer in red, you can see,
does not perfectly follow our best fit line,
but the real answer is contained
inside that 95% confidence interval.
Meaning, based on the available data that we have,
we are correctly identifying
that the average output decreases as
the input decreases and the output
to input relationship is consistent with the true answer.
If we would collect more data points,
we'd have a bigger data size.
Our best fit line would be better.
The confidence interval would
get smaller, it would shrink.
But this is not the complete story.
This is just expressing the uncertainty on the average,
the uncertainty on the mean output.
But we also have the Sigma parameter.
Remember, when we created this dataset,
there was some level of variability around the average.
That variability is everything that
the model cannot account for.
This is unexplained variation.
Our confidence interval is not
really representing the unexplained variation.
If we return back to lm pred summary,
there were those two other columns
on the far right hand side,
labeled obs ci lower and obs ci upper.
These stand for the uncertainty or confidence intervals
on the observation.
More commonly, this is known as
the prediction interval or the PI.
The prediction interval tells us where we think
a single measurement can exist.
It's really this prediction interval for
why I spent so much time talking about
these ribbons at the beginning of this week.
Because the prediction interval is where we will
learn how uncertain are
we about an individual data point.
That uncertainty is greater
than the uncertainty on the average.
Just look at this top row here
in our lm pred summary data frame.
The confidence interval on the average
is basically between three and 6.5.
But the prediction interval,
which by default is the 95% prediction interval,
you can see is between 1.7 and 7.9,
saying there's a 95% chance,
a single measurement will be between these bounds.
Whereas the confidence interval on
the mean says there's a 95% chance,
the average is between these bounds.
Let's visualize this. Let's see it.
We can build off of what we have here above.
I'll first start with
the case that does not include the true answer.
This is consistent with even real applications.
We need to include the prediction interval
as a second ribbon
in our visualizations for regression problems.
Let me copy all of that code from above,
but I will set the labels,
set_x label x, ax_set_y label y.
This prediction interval or
the uncertainty on a single measurement or observation,
this should be set first.
It's the wider, it's the larger uncertainty interval.
Ax.fill between df_viz_copy.x,
lm_pred_summary_obs_ci_lower,
lm_pred_summary_obs_ci_upper.
I like to set
the face color for the prediction interval to be orange.
We can make it more transparent
0.75 and set the edge color to be orange.