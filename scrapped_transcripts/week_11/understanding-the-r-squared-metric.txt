Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/vlno0/understanding-the-r-squared-metric

English
Now, this graph, it's really important,
and I highly recommend always creating it
, but quite often,
we want a number or a quantitative metric for
the correlation between the model predictions
and the observed output.
This single number can be quantified by calculating
the correlation coefficient between
the model fits and the observed output.
We already know how to
calculate the correlation coefficient.
We just need to select
the appropriate columns, in this case,
y and fitted and then apply the.corr method.
If you want to be exact,
you could say numeric only equal to true,
but it really doesn't matter because
we're working in a regression task.
The output is continuous,
and the fitted value
that's getting predicted is continuous.
Now, we get this matrix produced,
but all we really care about
is the correlation coefficient
between the observed output and the fitted values.
Let's use the iloc method to grab row 0, Column 1.
We now have the correlation coefficient
between the predicted and observed values.
Correlation coefficient can be negative or positive.
Let's square the correlation coefficient.
Again, identify the observed output,
the fitted or model predicted
values on the training set and pull out
the correlation coefficient between
those two square the valves.
The squared correlation coefficient between
the model prediction and
the observed output is incredibly important.
It has a special name.
We just calculated the models R-squared.
R-squared is so important,
it's already provided to
you as an attribute of the fitted model.
Now, if you look closely,
we have the exact same values
down to 10, 11, 12 decimal
places as what the stats model object has.
We have differences in
the bottom of the last few volumes.
That's because stats models calculates
R-squared using a particular formula.
I'm not even going to show you
that formula because I personally think it's nonsense.
R-squared is talked about as if it's some magic thing.
You'll hear it described in terms of
unexplained variation, explained variation, bias,
variance, all these crazy things for
people trying to give it this magic interpretation.
But this graph tells you the magic interpretation.
R-squared comes from the
predicted versus observed figure.
This figure tells you,
does the observed output increase
with a increase in the predicted value from the model?
If you have a general relationship,
a general linear relationship that
the observed output is generally
increasing as the predicted trend is increasing,
then R-squared will be high.
However, if there is a lack of
general linear relationship between
the prediction and the observed output,
then R-squared will be low.
Again, this is between the prediction and the output,
not the input and the output.
This figure works even if you have
non-linear features in your model.
Ideally, R-squared is equal to one,
and a really bad model has an R-squared of zero.
But please be careful.
We will see later when we work with
Scikit-learn that depending on the formula that you use,
you can have negative R-squared values.
Scikit-learn uses a formula
that provides negative R-squared,
but negative R-squared makes no sense.
It really is supposed to be 0 and 1.
The closer to one, the better,
the closer to zero, the worse it is.
I mainly wanted you to see
this really important
regression performance metric, R-squared.
It comes from the predicted versus observed figure.
How linearly related is
the prediction of the trend
with the observed random output.
The better they are linearly related, the closer to one,
the R-squared will be because it
literally comes from the correlation coefficient squared.