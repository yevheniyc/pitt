Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/JGj1Q/creating-and-fitting-the-model

English
Hello, everyone. As you can see,
I have Jupyter open to this week's class of directory.
You have learned how to fit
linear models for linear and non linear relationships.
You have learned how to make
predictions with linear and nonlinear relationships.
But now, let's take
a step back and let's consider how do we
actually measure the performance of our linear models,
or in more general terms, our regression models.
This example will use
the linear data CSV file for simplicity.
If you don't have Jupyter launched,
please pause the recording
and launch Jupyter and navigate to
the correct directory and then launch
a new Python 3 Jupyter notebook.
Let's go ahead and name this notebook
week_10_regression_metrics
because these performance metrics
work even if you're not using linear models,
they work for regression tasks.
Let's go ahead and set the headers CMPINF 2,100 Week
10, regression performance metrics.
We will calculate the performance metrics
for a linear model,
but these metrics can be used even if you
are using more advanced models for regression.
This is a very important set of examples.
Let's go ahead and import the modules.
As with the previous recordings,
you need numpy as np.
You need pandas is pd.
You need matplotlib.pyplot as
plt and you need seaborn sns.
We will do all the model fitting,
though, with stats models.
You must import
statmodels.formula.api as smf.
Then read in the data,
and we will use
the linear relationship dataset for this notebook.
df, gets assigned pd,
read_csv, week_10_linear_data.csv.
Again, we've worked with
this dataset a handful of times now,
so we're not going to explore it,
but we will predict the random output
y using the input variable x.
Again, this data frame includes the actual real answer,
the trend, as well as
the parameters that generated that dataset.
Now, before we can measure performance,
we must fit the linear model,
and we will fit the linear model
using the formula interface.
Let's assign it to the ln_fit object, smf.ols formula.
There's a data argument, and then you fit.
The data is df and the formula y tilde x.
Again, I'm not going to go into details about how do
you interpret this model because this was already done.
We have the coefficient estimates,
we have the coefficient standard errors,
we have the coefficient p values,
and we have the coefficient confidence intervals,
which we can rename to the more appropriate names
as the ci_lwrI and the ci_upr.
There we go. We can
also visualize the coefficient summaries
using a coefficient plot.
This time, I will define a function called my_coefplot.
def my_coefpplot,
where the argument is the the fitted model object.
In this fitted model object,
we will have a figure created.
This figure will have an error bar,
where the y argument,
the y axis is going to come from the mod.params.index.
The x axis will be the params and the x error will
be the approximation to
the 95% confidence interval two times the BSE.
Then the format will be a marker,
the color will be black,
the edge color will be black,
the edge line width,
specify here and the marker size.
As discussed in that previous video,
we always want to include in the reference line of
F0 because statistical significance is ultimately asking,
are we confident in the sign S-I-G-N of the coefficient,
whether it's negative or positive?
We'll set the x axis label to be
a coefficient value and then plt.show.
my_coefplot for ln_fit here
but sometimes what I like to do
is provide a default figure size.
That way, I do not need to set the figure size.
But if I want to, I can override it.
This lets me control
the height really of this figure
because at the end of the semester,
when we have many inputs,
it will be nice to be able to make this figure taller.
This function, my coefplot is using the approximation to
the 95% confidence interval to
visually show which inputs are statistically significant.
This function can be used for any linear model.
It doesn't matter how many parameters
or how many coefficients are getting estimated.