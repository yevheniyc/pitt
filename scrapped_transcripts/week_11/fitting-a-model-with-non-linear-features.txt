Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/7CjVg/fitting-a-model-with-non-linear-features

English
Hello everyone.
As you can see, I already have Jupyter opened up for this week of the course.
In the previous video we saw how to make predictions using the example
that had a linear relationship between the trend and the input.
We will now see how making predictions when there are nonlinear
features are all the exact same style of steps that you have to use.
We will use the exact same set of methods,
the exact same ways of visualizing everything.
The only difference is that the formula will be changing.
If you don't have Jupyter started, please pause the recording,
launch Jupyter and navigate to the correct directory.
And once you have done that,
start a new Python 3 Kernel to create a new Jupyter notebook.
Let's name this notebook week_10_lm for
linear model _predictions_nonlinear, and again,
the nonlinear here is referring to the nonlinear output to
input relationship, even though this is a linear model.
Let's now put in the header information.
CMPINF 2100 Week 10,
making predictions with linear
models we will make predictions for
the nonlinear trend to input relationship.
Thus, we we are making predictions when
a nonlinear feature is derived from the input.
Let's go ahead and import the modules.
You need the big 4, Numpy as np, Pandas as ppd,
matplotlib.pyplot is plt, and Seaborn sns.
And as we have used for several recordings this week,
you also need statsmodels.formula.api as smf,
statsmodels.formula.api as smf.
Next, let's read the data.
We will assign the data to the df object, read_csv,
where we read in the week_10_nonlinear_data.csv.
Again, we're not going to visually explore this data because we created it and
those visualizations were done in previous recordings.
Let's fit the linear model.
The model includes a nonlinear feature
derived from the input, the stats,
the smf.ols formula applies that nonlinear
function directly in the formula.
So, as discussed previously, when we fit and
interpreted the coefficients for this data set,
we will apply the function directly in the formula.
We do not need to create the feature when we're working with stats models.
The model object will be lm_fit, the result of smf.ols,
where there is a formula and a data that we will fit.
Again, the data is df, but the formula is now
the output y is a function of np.sin(x).
And if you're confused as to why this function is being typed in the string for
the formula, please go back and watch the recording that discussed fitting and
interpreting this nonlinear relationship.
We fit the model, we have access to all methods and
attributes associated with the linear model.
We have the coefficient estimates, we have the coefficient standard errors,
we have the coefficient p values, we have the coefficient confidence intervals.
And again, I'm renaming those confidence intervals to be something
that's easier to read and interpret rather than the default.
I won't make the coefficient plot visualization that's in the previous
recording on working with this nonlinear data.
But as you can see, the np.sin applied to x
is a statistically significant feature.
Because the 95% confidence interval on the coefficient does not contain 0,
the sign of the coefficient is definitely negative.
The p value is less than the 0.05.
So we know that this is a statistically significant feature.
And by the way, since this is a statistically significant feature,
clearly we need the input.
But now, let's make predictions to study the trends and the uncertainty.
Create a dataset just to visualize the predictive
behavior of the output given the input.
Just as in the previous recording, this data frame will be named df_viz for
visualization.
I will make it using pd dataframe using
the dictionary key value pair, so the input is x and
I will create the input values using np.linspace,
just as I did in the previous example.
I need the minimum value of x on the training set and
the maximum value of x on the training set.
I need to specify the number of points between them,
and just as in the previous video, I could add in a little buffer so
slightly less than the minimum and slightly greater than the maximum.
Again, the number of predictions points, the number
of predictions we are about to make does not need to equal the training set.
You can make more predictions or fewer predictions, and
you can make predictions at input values that do not appear in the training data.