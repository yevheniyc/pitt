Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/redEf/generating-random-nonlinear-output-data-with-uncertainty-analysis

English
And now let's generate the random output data.
Generate a small number of random output
measurements at given input locations.
We will use more input locations than
the linear relationship example.
So let's create a data frame, df pd data frame.
Again, using the dictionary where the key is x np.linspace.
Let's go from -3 to 3, but
this time how about 1 15 uniformly
spaced measurements between -3 and 3?
Here are the inputs, the trend is still
the intercept plus the slope times
the feature derived from the input.
So np sign applied to the x column from df.
Again, intercept plus slope times feature.
Initialize the random number generator and
then call the appropriate, distribute
the appropriate generator function.
Rg = np.random.default_rng 2100.
So we're setting the seed for reproducibility and
now add in a new column to df, which is y is just equal to the normal or
Gaussian random number generator.
Which has a location, a mean of df trend, a scale or
standard deviation of my sigma, size,
the number of random measurements is equal to the number of rows of df.
Again, the random output, although it's expected to have this value on average,
our single measurement does not need to be that value.
Visualize the randomly generated outputs around the trend.
Let me scroll back up and just copy this code to speed things up so
I don't have to make all of the objects again, which,
by the way, just to give a plug for the visualization class.
These kinds of figures, which are composites that have lines and
ribbons and markers, way easier to make in R than in Python.
I will just say it, I don't care if that gets people mad, I don't care,
it's easier to make these figures in R.
Okay, so let's now include the randomly generated small data.
We need the scatter method from the ax object dfx dfy,
and we'll use black or the black markers.
Okay, do you see these black dots?
Notice how they are not located along some same straight line.
They are moving, following the nonlinear
output to input curve, But
they are all pretty much contained within this 95% uncertainty interval.
There's a 95% chance the random measurements
will be inside this outer ribbon that we're showing.
There's a 68% chance they will be in the inner ribbon.
So let's count, we got 1, 2, 3, 4, 5, 6, 7,
8, 8 out of 15, that's just over half, are inside that inner ribbon.
All of them, as we can see, are within that 95% uncertainty interval.
Again, if we would have thousands of measurements,
5% would be outside that ribbon.
This figure, think of it as, I don't know,
maybe we're going to go outside and measure the temperature one day.
When we go outside and record that temperature from our thermometer,
we could expect the temperature to be at some value.
But there's some level of variation where the single measurement could be.
There are some days in May that are hotter than average.
There are some days in May that are cooler than average,
just like how there are some days in maybe November that are cooler than average,
while there are some days in November that are hotter than average.
The mean, the average or the trend is what we are trying to ultimately model.
Any variation around that trend is uncontrollable or
unexplainable or purely random.
That's what sigma represents, the standard deviation of our Gaussian.
Also, too, the standard deviation of
the Gaussian is not applied to the entire data set.
That sigma value is local,
it's around the trend.
To drive that point home, if I apply the STD
method to the y column in the df data frame,
do you see, it's not 0.33.
Because the marginal standard deviation of the output,
that marginal standard deviation is ignoring the input.
It doesn't care what the input is,
whereas the sigma value that we specified, it's local.
It's around the mean, which is focused on one value of the input.
When we know the input sigma is some constant number,
sigma is some unexplainable variation that is not attributed to
the change of the input, and that sigma is the same regardless of the input.
Okay, let's save the randomly generated data.
And as with the previous example,
let's include the true coefficients and
sigma that made the data.
So we have that for bookkeeping purposes.
Let's add in three columns the true intercept,
which is my intercept, the true slope,
which is my slope, and the true sigma, which is my sigma.
Again, all of these parameters are constant because the intercept,
slope and sigma do not depend on the input.
So then, df_to_csv, let's name this file week_10_nonlinear_data,
because we have a nonlinear output to input relationship,
and set the index to false.
If we check our file manager, you will see we now have
another csv file, week_10_nonlinear_data.
Okay, so again, I hope you liked this video.
I hope it reinforced those major assumptions with this
concept that linear models do not just mean straight lines.
You can actually have nonlinear output to input relationships
because we can derive features from our inputs.
But that's everything for this video,
please let me know if you have any questions.
Take care.