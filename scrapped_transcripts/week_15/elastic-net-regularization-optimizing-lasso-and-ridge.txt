Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/Mfnwy/elastic-net-regularization-optimizing-lasso-and-ridge

English
Let's fit an elastic net penalty or
penalized model with a mixing fraction of 0.5.
So if we would use 0.5, this is an even mixing between the two penalties.
So I'll name this one Enet,
Enet even mix logistic regression.
We will fit it, ysk revel same data set we fit all the other models on.
We're using the hack fit intercept equals to false,
but now the penalty is not l 2, it's not l 1,
it's not none, it's elastic net.
The solver must be saga, just like l 1, just like lasso.
We must set the c parameter.
So let's just use the default, so
we'll say with a mixing fraction of 0.5 and
the default c value, and now include l 1 ratio 0.5.
Lastly, max iter equals 25,001.
There we go.
So you can see we have even more arguments.
So let's put the fit on the next line.
But penalty is elastic net c and l 1 ratio.
Elastic net needs both to be identified.
It needs both.
I'm right now using the default c with an even mixing between the two.
You could check the training set score, the training set accuracy.
It's somewhere in between.
Or it's somewhere in between,
what we saw with the default c 4 lasso,
and the default c for rich.
Because it involves lasso, it is capable of
setting co-efficients direct identically equal to 0.
You can see there are some truths, but Ridge is present.
Tuning elasticnet,
requires tuning both the c
parameter and the l 1 ratio.
So you actually need to tune both, not just one.
Let's use the same stratified 5 fold
CV approach that we used previously.
I'll name this Enet tune logistic regression CV.
We will fit it on the data set.
Again, we're using the hack fit intercept equals false.
We need to set the penalty elasticnet.
The solver is saga, max iter 25,001.
Let's go ahead and put fit on a new line.
Okay, now the CV is our k fold that we set up,
the stratified k fold that we used in the previous recordings.
But now when we set the tuning for elasticnet,
we not only need to use the c's, the cs per argument,
but we also need the l 1 ratios, the l 1 ratios argument.
C's can be a number, and I could use 101 here.
If I did, I would use the same number of c's that we used for Lasso and Ridge.
But I'm also tuning l 1 ratio, which must be provided as an array.
So I'm going to use nplinspace to try from 0 to 1, seven unique values.
Meaning if I would use 101 c's and seven unique values of l 1 ratio,
there would be 707 models fit per fold.
707 are then fit five times because of five fold CV.
I can't even do that in my head.
But it's what, like 3500 models?
Man, that is a 4000 models, that is a ton of models.
So instead, just to make it run faster,
I'm only going to use 25 unique values of c.
Now this will take a while because I'm literally trying out
all these combinations of c and the mixing fraction.
If the l 1 ratio is 0,
the elastic net has turned into ridge.
However, if the l 1 ratio is 1,
elastic net has turned into Lasso.
So if you're not sure whether you should use Ridge or
Lasso in a situation, you actually can
use elastic net to figure it out for you.
It does the figuring it out because you are literally
trying out many different combinations of the c.
And the l 1 ratio parameters.
c again controls the values of the co-efficients.
Large c allows overfitting, very tiny c has underfitting no trends.
l1 ratio, can you use Lasso or none?
Once it has completed,
you can check the tuned c.
You can check the tuned l 1 ratio.
And this tells us the tuned
elastic net model is closer
to Lasso than Ridge.
That's why the tuned co-efficients, most of them are 0.
Now, unfortunately, I'm not going to visualize,
we will not visualize the elastic net cross validation
results because it takes much more data manipulation to create the figure.
Because the scores are stored in a 3D array.
The rows are the number of folds, the columns are the number
of correspond to the number of unique c values, and
the third dimension corresponds to the unique values of the l 1ratio.
So visualizing these results are actually much more complex and challenging.
So we're not going to do that here.
In this course, I wanted to introduce the elastic net model to you so
that you could see another approach to try and simplify your very complex models.
Okay, elastic net, know I mentioned in the previous recording how important lasso is.
Elasticnet is really a generalization of it.
You're not sure if you can use lasso, so just let elasticnet figure it out for you.