Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/JtoCO/pipelines-in-scikit-learn

English
This is the pipelines module.
Pipeline manages all actions for preprocessing,
training, cross-validation, and tuning of models.
This is actually my favorite part of Scikit-learn.
In all honesty, if Scikit-learn did not have pipeline,
I would not recommend ever
using Python for data science and machine learning.
Pipeline in Python is what saves Scikit-learn.
Without it, most of Scikit-learn would not be correct.
To get it, we must import from sklearn.pipeline,
import Pipeline with a capital
P. The pipeline module lets us specify recipes,
workflows or, as the name states, pipelines of actions.
These actions are called steps.
However, before creating the actions,
let's separate our input and output.
Let's create dedicated objects.
We will focus on models with linear additive features,
but we will not use
dmatrices this time to create the features.
Instead, we will manually separate them.
Primarily because this is actually quite easy,
let's make a DataFrame x inputs, sonar_df,
where we select the D types for all numerics do a D copy.
Likewise, let's select the output,
sonar_df.loc all rows for the response column.
Next, convert the DataFrames to NumPy arrays.
I'll name the inputs x_train,
x inputs to NumPy.
The output y_train,
y output, I need a response column.
I'm going to convert that to NumPy,
and then let's apply.ravel that
way the output is now one D array.
Notice, I did not convert
the output first to one or zero.
I'm keeping it in with its category labels.
Our pipeline will consist of two steps.
The first step will standardize,
and the second step will fit an elastic net model.
We therefore need to import the appropriate functions.
From sklearn.preprocessing, import StandardScaler.
That's the function we used back when we were doing
exploratory data analysis, especially for clustering.
From sklearn.linear_model, import logistic regression.
Let's initialize the elastic net model
for specifying the steps.
I'll name this object enet_to_fit.
Logistic regression.
Penalty, elastic net.
Solver, saga. Now, this time,
something we didn't do in the previous
recording for elastic net,
let's set the random state to 202.
This helps further control how
the saga algorithm fits this model.
Let's just specify it, max iter 25,001.
Now very importantly, fit intercept, true.
Our x train array
does not include the bias or intercept column.
Therefore, we need the typical or conventional style from
Scikit-learn which estimates the intercept
separately from the rest of the coefficients.
That's why we're setting fit intercept to truth.
Now specify the steps of the workflow or pipeline.
I'll name this enet_wflow.
The pipeline function has
one major argument called steps,
and we specify the steps within a list.
Each step is a tuple.
I have two steps, so I have two tuples.
The zeroth argument to the tuple is the name of the step.
The one argument is the function to apply.
So my zeroth step will be standardized my inputs,
and I'm making this name,
and then I'm applying this function.
Likewise, the zeroth argument
for the next step will be enet,
that will be the name, and I want to apply enet_to_fit.
Notice, because I initialized this object,
I'm providing the object,
enet_to_fit, without parentheses.
But I need to initialize StandardScaler,
and that's why it has the parentheses.
Now, when you run this, nothing actually
happens because we haven't fit anything,
we've just specified the steps.
If you display it, you get this nice diagram that
tells us our pipeline consists of
StandardScaler and this logistic regression model,
and here are the settings
of that logistic regression model.
But if you look closely,
when I set logistic regression,
I did not specify
the tuning parameters c and l1 ratio
when the elastic net model was initialized.
That is because we will iterate training
and testing the model for
many combinations of these two parameters.
We will use cross-validation to identify the best combo.