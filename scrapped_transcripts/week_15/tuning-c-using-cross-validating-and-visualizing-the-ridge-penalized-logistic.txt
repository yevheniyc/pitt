Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/K54iW/tuning-c-using-cross-validating-and-visualizing-the-ridge-penalized-logistic

English
The rest of the arguments are very, very similar.
Let's name this ridge_tune, = LogisticRegressionCV.
We will ultimately fit it, so
we will fit it to the training set, x_sk and y_sk.revel.
So the fitting arguments are the same.
We also want to use our hack, where the fit_intercept = False.
We will cross-validate it using our Stratified5Fold scheme.
So the CV argument is set to kf.
So LogisticRegressionCV has a CV argument just like cross-val score.
But now we must specify the penalty, going to use l2 for ridge.
And now, instead of setting a single value for C, we will set how many C's?
And I'm using C's very specifically because the argument is capital C,
lowercase s.
It looks like a plural C.
So Cs = 101.
I want to try out 101 unique values of C from some lower bound to an upper bound.
And the default lower bound,
Is 10 to the minus 4, and the default upper bound is 10,000.
So that's why we actually tried those values.
Then set solver = lbfgs, max_iter = 25001,
and let's put the fit on the line below.
So look at the arguments to LogisticRegressionCV, it looks very,
very similar to LogisticRegression.
But instead of setting a single value or unique value of C,
we're saying, how many values of C will you use?
When you run this, it will take a few minutes
because we are not fitting one model.
We aren't even fitting 101 models,
because we're applying a cross-validation procedure.
We are fitting 101 models because we have 101 values of C five times.
We're therefore fitting and testing 505 models.
You can see it's done, so it didn't take all that long on my machine.
The tuned value of C, the C that
maximizes the accuracy on new data,
is contained in the C_attribute.
So ridge_tune.C_ is this one value.
This is the one value of C that maximizes the performance,
even though it tried out all 101 values shown here.
So again, 10 to the minus 4,10 to the positive 4.
We can visualize the results
via the .scores_attribute.
And this attribute, it's rather complex.
So ridge_tune.scores, it's a dictionary.
And the keys, you're going to go all the way back to the beginning of the semester.
The keys of this dictionary are the classes, we are predicting the event.
And the event is named 1.0,
if you remember, the y = 1 is the event.
So the scores, Associated with,
This key are contained in a numpy.ndarray.
This ndarray has as many rows as cross-validation folds and
as many columns as unique values of C.
Let's visualize the results as a line chart,
and we will use Maplotlib directly here.
And we will just use Matplotlib directly,
because this is actually set up easier to use in Matplotlib than Seaborn.
We would need to do some data manipulation for it.
We need to transpose
the scores at NumPy array so
that we have one line for
each cross-validation fold.
We used 5-fold CV, so
there must be five lines in the following figure.
So if you're making this figure and you're not sure if you did it right,
the first thing that you should check is how many lines are there.
Let's go ahead and initialize the figure and axis objects,
And then we'll use the plot method.
I'll set the xlabel to be C, and I'll set the ylabel
to be cross-validation Accuracy, plt.show.
The x argument here will be ridge_tune.Cs, and I'm accessing that with the Cs_.
This is an attribute, and
then ridge_tune.scores_.
I need all the values for the key 1.0 and I need to transpose it.
Okay, so here, let me just show you that,
ridge_tune.scores_[1.0].T.shape.
So now I have five columns rather than five rows.
Now, I like to set the color here to be gray, and you'll see why in a minute.
But let's count, we have one, two, three,
four, five lines because I have five-fold CV.
However, it's very hard to see what's going on for the tiny values of C.
We can only really visualize the large values of C.