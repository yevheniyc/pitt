Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/cWNrT/lasso-for-feature-selection-and-model-simplification

English
This will take a little while because the LASSO penalty
is more complex than the RIDGE penalty.
Again, we're using five five-fold CV,
101 unique values of C
so 101*5 there are 505 models trained and fit here.
Those models, again, are defined by their C values.
The optimal value for C,
LASSO tuned C, underscore.
This is the value that optimizes the LASSO model.
The cross-validation performance can be
visualized just like
the RIDGE cross-validation performance.
Let me scroll up. This was a plot we made in
the previous recording so
I'm just going to copy and paste it.
I'm copying it just to save
a little time but
I will replace ridge tune
with LASSO tune everywhere here.
You see I have LASSO tune.
Large C is on the right very small C
is on the left so we're allowed to overfit on the right,
we're allowed to underfit,
no trends on the left.
Somewhere in the middle
maximizes the performance on average.
Here's the performance when we're allowed to overfit,
here's the performance, when we're allowed to underfit.
In the middle, we are getting
the best performance possible that is
occurring because the LASSO penalty
has literally turned off most of the coefficients.
Most of these coefficients are zero so which
specific features are not turned off?
That's really the key question when you're using LASSO.
Let's add in a new column to
co-compare called lasso_tune C,
lasso_tune_coef.ravel
PD series index.compare.index.
Dot compare very good.
We now exactly see which ones have
been turned off so let's
identify all coefficients where
the tuned lasso does not equal zero.
This is it. These are the only coefficients
and thus the only features that the model feels
are needed to maximize the performance on
new data on average.
Last week, when we went through and applied
cross-validation to all of the models,
for example, at the end,
we did so with cross bowel score we
found one of our models
that did the best on average, that's model 6.
Model 6 had a cross-validation accuracy
on average of around 72%.
If we examine what's going on here with LASSO it's
giving us an on average cross-validation accuracy
also of around 72%.
It did that by
literally finding a C value that allows it to turn
off all of these different features
to create a much simpler model,
a model that does not have 81 effective features.
It has just 1,2,3,4,5,6,7 effective features.
The tuned LASSO model is much
simpler with just seven effective features instead of 81.
Let's visualize the simplified model
by making predictions with the tuned LASSO model.
Name it pred_grid_ lasso_tune and in the previous video,
we made the X grid object,
so I can use the predict Proba method with X grid.
We also saw how to use the classes attribute to
identify the column associated,
with predicting the probability of
the event and let's now add
in a column to
our visualization grid, pred_prob_lasso_tune,
pred_grid_llasso_tune, all rows,
but just for the column where classes equals one,
force it to be a one D array and wrap that around with
a panda series where the index
is the index to the visualization grid.
Great. We can now visualize it,
relplot data equals dfviz x = x1
, y = pred_prob_lasso_tune.
U = x5,
call equals x2, kind equals line,
estimator equals none,
units equals x5 all that equals three.
Great. Last week, when we did this all more manually,
and we identified one and only one model out of
our set we created these parabola-like shapes.
Well, now, thanks to LASSO,
we've used one and only one model,
but use the C parameter to
turn off a whole lot of coefficients,
a whole lot of features that are giving
us very similar trends.
These are very close with
the most important similarity being the x5,
the categorical input has no impact here.
It's doing next to nothing.
There's only one feature where that x5 shows up,
and it's really small.
We can't even see its impact.
It looks just like that model we identified individually.
Again, this creates actually
a secondary way of going
about and identifying your best model.
You start incredibly complex.
You begin with all of these features,
but you then let the LASSO penalty try
and turn them off by tuning the C parameter.
You rely on cross-validation
because you don't know what value of C to
use you need to find the value of C that
maximizes the performance on new data.