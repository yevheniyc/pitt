Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/wEos4/c-parameter-tuning-in-ridge-penalty-logistic-regression-for-optimal-performance

English
Let's make this more obvious by sorting
the coefficients by the statsmodels
result or the no penalty result in descending order.
Campare.sort_values_by_sk_none, ascending false.
I kept these with their signs,
so here are the positives and here are the negatives.
The most negative value statsmodels.
No penalty from scikit-learn very
large C. These are all very close to each other.
Again, very very C values
produce models similar to having no penalty.
The coefficients are just minimizing the loss.
But as C decreases,
the magnitude of the coefficient,
gets smaller and smaller and
smaller until the magnitude is basically zero.
We can see that too on
the most positive coefficient, seven,
but then decrease C. Eventually,
C is so small,
the estimates are very close to zero.
Very small C values produce
models similar to the intercept only model,
which has no features.
Large C is like no penalty.
Small C is like the intercept only model,
no features in the model.
Essentially, the C parameter
allows us to control complexity,
which means the C parameter
effectively governs overfitting.
Very complex models are allowed to overfit.
Very simple models cannot.
The value of C will dictate how much
we're okay essentially with overfit.
Decreasing C will enable
the complex model to behave like a simpler one.
This, of course, leads us to the very natural question.
Here, I'll put it three hash tags,
what value of C should we use?
We do not know.
The C parameter is not
estimated when the coefficients are estimated,
the C parameter is specified by the user.
It controls how the coefficients are estimated.
Therefore, it is set before the model is fit.
We must use cross validation
to tune the value of C. Remember,
C is effectively dictating the complexity.
You could therefore think of each unique value of
C as governing a model.
Large C was the most complex model.
It was like the last formula in our list.
Very, very tiny C,
this is your intercept only model,
the zero formula in our list.
It's like we are allowing the model to
see every single formula that we tried out.
We don't know the best value of C,
we must use cross validation to identify
the value of C that will give
us the best performance on new data.
Thus, we select the value of
C that maximizes performance on new data on average.
That's why we could only discuss
regularization after discussing cross validation
because cross validation is an essential tool for
working with regularization or penalization methods
because you don't know the value of C to use.
Let's go ahead and import
a cross validation scheme because
this is a classification problem,
we need stratified k-fold,
not regular k-fold,
and let's use five-fold cross validation
just like last week.
I'll sign this to k-f,
stratified k-fold, splits is five,
shuffle equals true, random state.
Let's go ahead and use the very long integer
from last week just to
show you don't have to use 101,948,3156.
To confirm it, there are
five splits in this cross validation scheme.
We can apply cross vowel score to cross
validate each value of
C as we learned how to do last week.
However, scikit-learn has a built in
function for applying
cross validation to logistic regression.
Okay. So we actually don't even
need to set up our own function.
We don't need to use cross val score,
even though cross val score is very simple.
Instead, we can import from
SKLearn.linear model logistic regression.
But instead of using logistic regression,
you can use logistic regression CV.
The CV stands for cross validation.
This is the built in cross validation method
for tuning C for a given penalty.
We can specify the number of unique C values to
try and identify the
optimal or tuned value via cross validation.