Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/RQ6ix/understanding-the-impact-of-c-parameter-in-ridge-penalty

English
But now af I would run it as it is,
I would use the ridge penalty with the default c.
However, I'm using the hack, so fit intercept is false.
But I never like to use the default settings because I feel it's easy to
forget what we're actually using.
The ridge penalty is named l2,
this is the default value, this is an l, not a1,
a lowercase l, l2 that stands for ridge.
I also like to set the solver to be lbfgs and
max iter 25,001.
As you can see, the code now actually is pretty big, so let's go ahead,
and right before the fit put in the slash, go to a new line so we can continue on.
So the assumptions are set here, and
then we fit the model down below, You run it.
The model is now fit, we can check the training set score,
provide the training set xsk y to SK revel,
and we get a value of 80%.
Now if you to remember, remember the,
Result the score when we forced SK
learn to behave like stats models and
thus only minimize the loss and have no penalty.
The fit SK none score
was, 81%.
Now, although this doesn't seem like a big change, what matters
right now is there is a change, this is the training set performance.
The training set performance is going down by us using the ridge penalty at
the default oops, I actually forgot to type it in.
Let's go back at the default c.
The c parameter by default is 1.0, there we go, I forgot to do that,
so let's go back up here and let's include it in at the end, or
actually here we'll set c right next to penalty l2 C is 1.0,
so we remember penalty and C go together.
The score doesn't change because this was the default c value.
Why does the training set accuracy change?
Because the Ridge coefficient estimates do not
equal the estimates that minimize the loss,
Ridge default C coef even though we're using the ridge penalty,
this is still a logistic regression object,
therefore it still has the dot co if attribute.
And because we used the hack of fit intercept is false, and the bias or
intercept column included in the feature array,
all coefficients including the intercept are estimated here.
Now, from a quick glance, I don't see any coefficients
on the order of four, five, or six in magnitude.
I can see some near one, but I don't see any with values of seven or
negative six, so by using the ridge penalty with a value of c of one,
notice we have changed the coefficients.
The coefficient estimates are different compared to stats models,
But to really understand the impact of c,
we need to try extreme cases.
Let's first set c to a very large value.
Okay, I'm going to push c to the extreme, let's use a very large c.
So I'm going to name this object ridge large c logistic regression,
we will fit it on the exact same data set, exact same.
We are not fitting the intercept using the convention the typical way, so fit
intercept is false, the intercept again is included with all the other coefficients.
The penalty is still l2.
The c value instead of one, I want 10,000,
so you should have one followed by four zeros,
then solver is lbfgs max iter 25,001.
And again, let's put the escape character so
that the fitting looks like it's on the next line.
If I scroll up, all of the arguments to logistic regression are the same
except the value of c, I pushed c all the way to 10,000.
Let's first check the score, the training set accuracy,
The training set accuracy is not 80%.
The training set accuracy is now
Exact same thing as when we did not have the penalty.
The training statisty is the same
as when no penalty is applied,
when the c parameter is very large,
The coefficients ridge large c coefficient.
Just printing them out.
I see a value of seven, I see threes, I see fours here,
fives in magnitude, we are back to the large coefficients.
So large coifs, here's the six in magnitude.
So large values for c allow large
coefficient estimates.
Let's now try a or let's now try the other extreme.
A very small value for c.