Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/zYdEH/advanced-model-optimization-and-feature-engineering-techniques-for-enhanced

English
This grid, I'll name pc_lasso_grid.
Again, it's a dictionary, so curly braces.
Two new parameters,
so two items, key value pairs.
The first one, pca__n_components.
PCA, because that's what I named it.
See here, PCA, PCA.
n_components,
that's the name of the argument I want to tune
in the PCA method.
Then lasso__c.
Lasso is my name,
double underscore the c parameter.
Now, it's up to me to decide
how many principal components to try out
here when we could do something small like two or three.
There are 60 inputs.
We could try something very high, 50, 55.
But let's just try out a handful of values.
I'm just going to type these in in a list,
3, 5, 7, 9,
11, 13, 15. Let's go up to 17.
Why not? I'm going from 3 to 17 in increments of two.
Now the C parameter, again,
in the log space from -10 to 10, num.
Here we'll do 17 unique values,
but we need to remove the logs of np.exp.
We therefore have 1, 2, 3,
4, 5, 6, 7, 8.
We have 8*17,
136 models times 5,
136*5, 680 models that we're now going to fit.
So pc_lasso_search is assigned GridSearchCV.
pc_lasso_workflow, param_grid,
pc_lasso_grid, cv = kf,
and now run the search,
pc_lasso_search_results =
pc_lasso_search.fit (x_train, y_train).
There are 680 models that are now being fit.
Each of those models
has a different number of principal components and
a different value of C. This model is
incredibly fast because it's never using all 60 inputs,
it's using three features derived from the 60,
then five, then seven,
then nine, so on and so forth.
The best parameters, we needed
17 principal components with
this value for C.
What kind of performance did that give us?
Seventy-nine percent.
If we check our elastic net model, 79.8%.
Our principal component, therefore,
is not doing any better than using all
of the original variables and
using Lasso to try and turn them off.
If we check best_estimator_.named_steps(
"lasso") here's the intercept,
pc_lasso_search_results.best_estimator_.named_steps(
"lasso").coef_,
there are only a handful that had been turned off.
It's saying it needs all of these principal components.
Let's do one more.
What if I want to interact principal components?
An interaction is a multiplication or product.
What would happen if we
interacted all pairs of the original inputs?
In the formula interface,
we would type this out as literally typing
out every single input, and here,
I'll just do a handful of them,
and then raising that to a power.
This would be the formula for
all main effects and
all pairwise interactions between these four inputs.
It will take even longer to write out all of them.
However, there is a function that can generate
all main effects and
all pairwise products interactions for us.
Scikit-learn classifies this as a preprocessing action.
We must import the polynomial features function.
This function has an argument degree
which specifies the degree of the polynomial.
It has an argument interaction only,
which specifies if we are
only creating the main effects and interactions,
not raising inputs to polynomial powers.
This is a very handy way
to to create all of the pairwise products.
To do that, let's call polynomial features degree-2,
interaction_only = Tue,
and then it has this include_bias argument.
Bias here is the interceptor bias column,
that column of 1,
so let's not return it fit and transform x train.
Now, to run this, I'm only
doing this because I want to show you the shape.
There are over 1,800 columns generated by
creating main effect and
pairwise products between the 60 inputs.
We therefore cannot interact
all pairs of the original inputs.
But if we could derive
a smaller number of features such as through PCA,
how many could we interact?