Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/a0Qn0/scikit-learns-logistic-regression-defaults

English
Hello class.
In this video, I'm not going to program live.
Instead, I'm going to highlight the key issues
associated with a programming example.
Specifically, this recording is all about why Scikit Learn uses
the ridge penalty as the default option for logistic regression.
The reason is because of an issue known as linear separability.
Now, the last couple weeks we've been using a larger example for
logistic regression, binary classification.
But this example goes all the way back to the 11th week,
our initial example into classification,
which had a single continuous input and a single binary outcome.
By this point,
we know how to fit this logistic regression model with stats models.
Here's the model that has a linear relationship
between the input and the log odds ratio.
You know how to check the coefficient estimates.
These are the coefficients that minimize the loss.
You know how to check the standard errors.
You know how to check for statistical significance using p values or
through the confidence intervals.
So the input here is statistically significantly positive.
You also know how to use Scikit Learns logistic
regression via the formula interface.
And here I've set up all of the same procedures that
we've used over the last few weeks,
which makes use of what I call the hack approach,
where I use the same formula to make the feature array
that includes the intercept column, the bias column.
Then I set fit intercept to false in the call to logistic regression.
You've also learned how to force Scikit Learn to behave
just like stats models by setting the penalty to none.
This will produce the coefficient estimates that minimize the loss,
just like stats models.
But this week you learned about regularization or penalization.
You learned that the default approach in Scikit Learn
the penalty equals to l2 is the ridge penalty.
This penalty includes a parameter called c,
which has a default value of 1,
even though I did not specify that here,
this value of c is used by default,
as discussed in the example on ridge.
Remember how I mentioned when I'm actually using Scikit Learn for
logistic regression, I like to set the c parameter so
I know exactly what I'm using, rather than just relying on the defaults and
potentially forgetting it's there.
It's the c parameter with the ridge penalty that causes
the default estimates from Scikit Learn to be different numbers than stats models.
Because these estimates are calculated differently,
they are not minimizing a loss.