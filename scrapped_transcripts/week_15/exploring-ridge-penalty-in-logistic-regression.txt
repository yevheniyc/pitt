Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/6983P/exploring-ridge-penalty-in-logistic-regression

English
Hello, everyone.
As you can see, I have Jupyter Notebook opened up to this week for the course.
Here is the notebook that we created in the previous recording.
As I mentioned in the previous recording, I am not starting a new notebook.
Instead, I'm going to reopen the notebook we created previously.
So if you don't have Jupyter open, please pause the recording and do so.
And when you come back,
I want you to click on the week 14 regularize intro notebook.
Now, in this notebook, instead of starting to work and
start typing, I want to first save the file as a new name.
So if I would come up here and click on this box and change the notebook,
it would literally just save over, but I don't want to rename.
I want to create a second notebook.
So go to file, save as, and let's name this one week 14,
regularize ridge, R-I-D-G-E, and then save.
If you then check in the file manager, you will now see two Jupyter
notebooks with the ridge one green for being opened, okay?
And then let's go ahead and just to show this is different,
let's put in a ridge right here in the headers, all right?
Now go to Kernel > Restart & Clear Output.
So we will clear everything, letting us effectively start from scratch.
And then just scroll all the way down to the bottom, select this cell where
we were talking about why we need the hack, go up to Cell > Run All Above.
So it will rerun everything, bringing us back to where we were, and
we now have everything in memory.
Excellent, we're now ready to continue.
So let's go into this cell that we were previously typing in and
put what kinds of penalties exist.
There are many varieties of penalties or
regularization approaches.
We will demonstrate the three main types,
but there are still even more.
We will cover ridge, lasso, and
elastic net to wrap up the semester.
As the name of the notebook states,
this recording is all about the ridge penalty.
The ridge penalty, also known as the l2 norm penalty.
That's its fancy name, but I will always just call it ridge.
The ridge penalty is the default penalty
used by the logistic regression function.
Therefore, if you do not set the penalty argument,
the ridge penalty is assumed.
Okay, this is what's actually the default operation.
However, I always recommend manually
setting the penalty or the penalty argument so
that you know exactly what penalty you are working with.
So even though you can just leave it in its default,
I don't recommend that even if you want to use the ridge or
the l2 norm penalty, I explicitly set it so
you know exactly what's going on.
Now, all regularization or penalty or
penalization methods include an additional
parameter that controls how the coefficients are estimated.
The logistic regression function, and by the way,
you don't have to type everything I'm typing here.
I'm doing this so that you have the key facts in one place,
but you don't actually have to be typing all of these words.
But the logistic regression
function names this parameter C.
The C parameter has a default value of one.
Therefore, if you leave C at its, or if you just call
logistic regression with its default arguments,
you are assuming the ridge penalty and the C parameter is one.
The C parameter must always have a value for
ridge, so it's always going to have a value.
That's why I never recommend using just the default arguments.
Instead, I feel you should manually set it so
you know exactly what value of C you are actually using.
But what this means is we need to
understand how the C parameter
impacts the coefficient estimates
as part of the ridge penalty.
And although we could go into the math, which you will see in comp 2120,
we can actually see the effect by just running the code trying it out.
So let's try three different values of
circle on our problem for the ridge penalty.
So we're actually just going to run the code to find out what happens.
Let's begin with the default_c
parameter value for ridge, okay?
We are going to fit logistic regression again in scikit learn, but
I'm going to name this object ridge, since it's the ridge penalty, default_c.
So this is coming from the default_c value.
We will initialize the method just as we did previously,
and we will fit the method just as we did previously,
where we used X_sk and Y_sk.
X_sk was created in the previous recording.
It's the feature array associated with that most complex model.
This is why we're building off of everything rather
than having to retype all the objects in each video.