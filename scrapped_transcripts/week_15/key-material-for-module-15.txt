Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/IEgaj/key-material-for-module-15

English
Hello, everyone. Welcome to
the 14th and last week of the semester.
The previous week was a really important one.
We learned how to fairly select
the best model using a re
sampling procedure known as cross validation.
This was really important
because we had previously learned
that if we only
consider the performance on the training set,
the best model will always
appear to be the most complex model.
These models that are complex can overfit,
thus memorize the training data
and not do well on new data.
That's why last week was so important.
As a quick summary,
we tried out many models starting with very simple,
with the simplest being an intercept
only or constant model up to ones that are very complex.
They have a lot of interactions
between continuous inputs,
interactions with categorical inputs,
non linear features derived from continuous inputs,
having all of the continuous features
interact with categoricals.
We really did create a very complex model.
Cross validation splits the data
randomly into training and test sets.
This splitting happens multiple times.
Each split is referred to as a fold and
each fold has a dedicated training and test set.
The model is fit on the training set.
You then evaluate assess
or score the model on the test set,
meaning the test set is never used to fit the model.
The test set is never used to calculate or estimate
the coefficients when you are trying
to identify the performance of the models.
We ultimately calculate the representative performance
as averaging the scores or the metrics across the folds,
and we do that for all of the models.
The best model is the model that has
the best average score on the test sets,
so the average performance on the new data.
Now, we trained,
we specified all these models
via formulas to really support interpreting.
The point of doing this
was so that you could literally see what
features were being included to increase the complexity,
and the cross validation
allowed us to find out if the complexity
was worth it because if we needed interactions,
or if we needed non linear features,
those models would have
better average performance on a test set.
However, if the performance on average is
worse than a simpler model like linear additive features,
then we know that those additional interactions
and other kinds of features are not worth it.
There is an alternative approach though,
and that's really the crux of this week.
This alternative approach starts
with a single very complex model.
Rather than starting with
a really easy model and then adding complexity,
we begin right out of the gate with a very complex model,
and we know that this model overfits the training data.
In this alternative view,
we will remove the features that are
unimportant by forcing the coefficients to be zero.
This alternative approach is known
as regularization or penalization.
Now I'm using the term alternative,
but I like to think of it as you first
manually specify simple too complex models,
find out which of the most complex models are overfit,
and then you come and use
regularization on that most complex model.
You'll see why that is in the examples for this week.
Now, this may sound like this is something brand new,
why are we wrapping up
the semester with this brand new thing?
The coefficients, they are estimated differently,
but all other concepts are the same.
All other concepts are the same.
In order to use this alternative approach, though,
you will learn that there is a parameter that the model,
the regularization model requires the user to specify.
So this parameter is something that cannot be
estimated and can only be tuned through cross-validation.
We could only talk about
this regularization approach after
we learned about cross-validation.
That's why we had to have
last week before this material for this week.
The other reason why we are wrapping up
the semester with regularization is
that it really requires all concepts
of the second half to be able to use this procedure.
We need to be able to
estimate coefficients via fitting the models.
We need to be able to calculate
performance metrics or score the models,
and we need to be able to repeat
that process across multiple folds in order to calculate
the average performance to therefore give us
a fair estimate for how the model will do on new data.
The benefit of this procedure is that
the final model will only
consist of the most important features.
Throughout the second half of the semester,
there have been a lot of questions
around statistical significance,
and one of the things that I've tried to stress there is
statistical significance does not
necessarily mean a variable is important.
It just means we know that
the sign is either positive or negative.
I kept saying, hold off,
hold off for importance,
and this is why.
Because if a feature stays in the regularized model,
that means the model feels it needs
that feature to predict the output as best as possible.
One of the recommendations I have for this week
is you could focus on the recordings,
if you are working on your final projects,
you don't have to necessarily
program everything for this week.
That said, in some of the final projects,
the data are so complicated that
they really need to have a lot of pre-processing actions.
The example that I typically use in lecture,
the data are already ready for us
to apply methods like cross-validation.
But you remember back when we were clustering,
when we needed to pre-process or standardize
the inputs or the variables before we could do anything.
Well, in many situations,
that's what we have to do as well.
We actually need to pre-process within
the folds for cross-validation correctly.
This is very difficult to do on your own.
And so because of that,
I demonstrate how to use
the pipeline module to combine pre-processing,
training, cross-validation and tuning
to maximize the model performance.
I demonstrate that on the Sonar dataset from earlier in
the semester because this dataset has 60 inputs,
have different magnitudes and scales,
and they are highly correlated.
Therefore, I demonstrate how to use
PCA as a pre-processing step,
and I use cross-validation to tune
the number of principal components
that need to be used in the model.
Bring it back to my previous point,
if you're working on a final project that
doesn't involve a lot of work with PCA,
it's okay if you don't program
along with me in this example.
Focus on using
the regularization procedures for this week.
However, if you know that you are
working on a project that involves PCA,
I really recommend going through this recording following
along with me because it will
really help on your final project.
The last thing is,
there's one very critical topic about
logistic regression that we haven't addressed yet.
We saw in a previous week,
fitting logistic regression models
with stats models that had
errors where the model was crashing within the folds.
But when we went to Pychit learn,
we did not see any crashes.
The last example discusses
why this is not a bug or a typo.
There is a real reason for this.
This is an important conceptual issue that I
feel you should know and
understand or should know and be able to talk about.