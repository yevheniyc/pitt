Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/rOzul/refining-and-simplifying-models-with-ridge-penalized-logistic-regression

English
When we first worked on
this problem back in the 12th week,
and we were training these multiple models,
we created a prediction grid,
an input grid to examine
the predicted probability with respect to x1
for multiple values of x2
and each categorical input value.
Let's use this exact same input grid.
I'm literally going to copy
this code from the 12th week of the semester.
Use the same input grid as the grid used in week 12,
so I'm just literally copying and pasting it,
checking the unique values.
I know it's been a while,
but if we go back to the 12th week the semester and
check our predictions from this very complex model,
they look like this.
Now, after all, we have this model fit
so let's make the predictions from
the complex model that
estimates the coefficients by only minimizing the loss,
so the stats models.
Let's make a deep copy,
a hard copy of the input grid,
dfviz, pred, prob,
penalty, none, fit stats,
predict, input grid, dfviz.
There we go. Now let's visualize it, SNS, relplot,
data equals dfviz,
x equals x1,
y equals pred, prob,
penalty none, hue equals x5.
If you're wondering why I'm using these arguments,
this is what we said several weeks ago.
Back then, this was talked
about why these arguments were set the way they were.
But the purpose was,
I wanted to examine
the trends of the predicted probability,
primarily with respect to x1,
but by trying to examine
the influence of these other two inputs.
Back then, we saw there were
some really crazy relationships going on.
Look at this blue curve,
starts out really zero,
climbs all the way up, and has a really weird wiggle.
The green one starts zero, climbs to near one,
drops back down, and then you
can see the behavior this way.
But let's now make predictions with
the tuned ridge penalized logistic regression model.
We need to create.
We need the feature array for the new data.
We actually have to import from Patsy,
the D matrix function.
As discussed last week,
we can convert the formula into
the appropriate format by removing the y,
the output variable from the formula.
I'm sub-setting the string to do that.
This was discussed last week to show how to do that.
Then data equals input grid,
x grid shape, 81 columns,
just like the number of columns in the training set.
As discussed last week as well,
when we make the predictions from
the [inaudible] learn result or model,
predict underscore proba,
we actually get an array,
one column for each class.
We can use the classes' attribute to
identify which column is the class of interest.
The Value 1 is the event.
Let's now add in the column here.
I'll name it pred prob ridge tune.
It gets assigned.
Pred, grid, ridge, tune, all rows.
But for the column where the classes attribute equals 1,
apply Ravel to force it to be a 1D array
and then wrap a Panda series function around it.
That way we have converted it to a Panda series.
Now, just to show you I can use the same code,
let me copy the code from before,
but just change the y argument to pred prob ridge tune.
These shapes are not the same as what we had before.
Let me just pull up the predictions from a few weeks ago,
so you can get the more direct comparison.
We have converted our much more complex model
into something that actually
looks more closely to our optimal model.
They're not identically the same, but remember,
last week when we used
cross validation to cross
validate all of these psychic learn models,
we identified that Model 6 as being the best.
But now we have converted
the much more complex last model
into something that's a little more
consistent with what we saw from our optimal model.
It is not the same
because we have effectively cherry picked
the features by trying
to turn off the ones that don't seem to matter as much.
I know that's a lot, but if
you have been paying close attention to this video,
we've actually had to use
all the skills from the last few weeks.
We have fit logistic regression models.
We had to use cross validation in order to
identify which model does best on new data.
But now instead of literally
typing in many different formulas,
instead of typing in 101 formulas,
we started with a very complex model and converted it
into something similar through the C parameter,
because that allows us to
control the coefficient estimates.
I'm going to go ahead and save this.
That's all for.