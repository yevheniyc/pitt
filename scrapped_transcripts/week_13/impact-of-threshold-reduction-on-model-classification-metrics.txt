Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/fMyR2/impact-of-threshold-reduction-on-model-classification-metrics

English
We just saw what happened
when we increased the threshold.
Let's now decrease the threshold below 50%.
Previously, we raised 50-75.
Let's now change,
let's change the threshold from 50%-25% and
classify by comparing
the predicted probability to just 25%.
I'll name this new class,
pred class_low, we call it high threshold,
so this will be the low threshold.
It gets the result of np.where(df_copy.pred_probability).
We're still working with the same predicted probability.
We are not changing the model at all,
even though we are comparing that
predicted probability to just 25%.
If the predicted probability is greater
than 25% return the event,
otherwise return the non event.
Look at index location Key 113.
The input here is very negative.
The predicted probability is only 25.3%.
When we had a 50% threshold, well,
25.3 is less than 50%,
so we classify the non event.
When we had the 75% threshold, well,
25.3 is less than 75%,
so we classify the non event.
But now, when the threshold is 25%,
25.3 is greater than 25.
We are now classifying the event,
even though the models predicting
a probability of only 25.3%.
We can examine the confusion matrix just as before.
By changing our classifications now,
when we take the cross tabulation between
the observed output and the classifications,
and let's conclude in the margins.
Setting that we're annotating and setting the font size.
Oops, and let's set the four map so
everything is consistent. There we go.
Now, if I scroll up, again,
you should see differences in
the colors within those main four combinations.
But just as before,
let me put all the confusion matrices side by side.
Again, you don't have to do this right now.
You don't even have to do it after watching the video.
I'm doing this because I want you to
see the impact of changing the threshold.
Laying out the confusion matrices side by side like this,
I think makes it very clear.
Again, first, focus on
the counts for the observed output.
The threshold has no impact on
the number of events we observe or the number of
non events that we observe because
the threshold is impacting the predicted classes.
When the threshold was high,
the number of predicted events is low.
But when the threshold is low, now,
the number of predicted events increases.
Because we're only comparing
the predicted probability to a much lower number.
It's effectively easier to classify the event.
Changing the threshold changes
all of the confusion matrix counts.
It changes the number of
true negatives and true positives.
It changes the errors,
the number of false positives and false negatives.
Let's calculate the confusion matrix combination
counts directly using the scikit-learne function.
We'll have TN_lower, FP_ lower,
FN_ lower and TP_ lower,
getting the result of confusion matrix
where we take df copy.y to
numpy df_copy.pred_class_lower_threshold_to_numpy
and apply the Ravel method.
The accuracy TN_lower+TP_lower,
all divided by the sum.
Of all four counts or
really the number of rows. That's all this means.
Again, the accuracy changes
because our classifications change.
How about the sensitivity?
The sensitivity is the number of true positives
divided by the total number of events observed,
the number of true positives
plus the number of false negatives.
Decreasing the threshold increases the sensitivity.
The model is essentially
94% accurate when the event occurs.
When the event occurs by lowering the threshold,
the model is really good.
What happens to the specificity or the number of
true negatives divided by the total number of non events,
number of true negatives
plus the number of false positives.
Decreasing the threshold
will decrease the specificity.
Remember, our original specificity
at a threshold of 50% was 76%.
Now with the lower threshold,
the model is accurate
just 40% of the time given the event does not occur.
Or if we view it in terms of FPR,
FPR is one minus the specificity,
the false positive rate is now higher.
Decreasing the threshold will increase the FPR.
Increasing the FPR is a bad thing.
It means the model is incorrect.
It's classifying the event
even though the event does not occur.
We can arbitrarily make the model really
bad in terms of FPR by decreasing the threshold.
This bad thing, though,
is counterbalanced by
the sensitivity becoming really high,
becoming very close to one.
Decreasing the threshold gives you a benefit.
You're really good at classifying the event,
but that also means you're going to
be missing the non event.