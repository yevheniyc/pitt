Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/oDxK3/understanding-logistic-regression-models

English
Now, you know how to explore this data.
You know how to look at marginal distributions,
you know how to look at relationships,
categorical to categorical,
categorical to continuous,
and continuous to continuous.
I'm not going to do that here.
If we had time, we would,
but the main thing we're going to focus on
is fitting logistic regression models.
Now, one thing we probably should check is,
what are the ranges or scales
or bounds on the continuous inputs.
Let's actually just do that one thing, df.describe.
The standard deviation on all of them,
x1 through x4,
you can see it's all around one.
The mins and maxes,
they're all about the same.
This is something that is
important for the same reason why it's important to look
at the scales for K before you cluster the variables.
I already have this example set up where
the continuous inputs are
all roughly scaled to be the same thing.
We don't have to worry about
standardizing in this example.
But let's go ahead and fit some models.
Let's start with
the simplest possible logistic regression model.
The event probability is
a constant and does not depend on any inputs.
This is the simplest possible model.
I'll name it mod_aa,
smf.logit well, it has a formula.
It has data and we fit it.
Data is df,
and a constant average.
In this case, a constant event probability, y ~ 1.
Again, this does not mean
that the output is equal to one,
or that the average is equal to one,
it means you have a constant average,
a constant event probability.
A single parameter is estimated, the unknown intercept.
Remember, this is for predicting the log odds ratio.
This does not mean the event probability
is a constant negative number.
It means the log odds is a constant negative number.
We can also fit a model with linear additive features,
which includes continuous and categorical inputs.
We are just adding the effect.
Let's name it mod=bb,
smf.logit.
There's a formula.
There's a data set. You fit it.
The data is df,
the formula,
y ~ x_1+x_2+x_3+x_4+x_5.
We are adding the effect of each input to another.
We have a slope multiplying each input.
We have an intercept,
which represents the reference category,
and we have a slope multiplying the two dummy variables.
Remember, the number of
dummies is the number of categories minus one.
Of course, we could also check
the p-values to examine
which features are statistically significant.
X_1 and x_2 are considered to be
statistically significant features in the model.
But logistic regression models can also use
non-linear features derived from
the inputs and interactions between inputs.
Let's do one more model here, mod_cc,
smf.logit, which has a formula,
which has data that gets fit.
The data still is df,
but now I'm going to hit "Enter" to go to a new line.
Unless use a much more complicated formula,
y ~ x_5,
the categorical input asterisk,
interacts with the linear additive features
of the continuous inputs.
Now let's add in polynomials,
quadratics derived from each of the continuous inputs.
This model now has many more features.
We have the main effects.
Here's the main effect of x_1,
main effect x_2,
main effect x_3, main effect x_4.
We have the dummies.
But then we also have,
does the slope with respect to
x_1 depend on the categories of x_5?
So on and so forth.
We also have non-linear features
derived from the continuous inputs.
Even though this model
has all of these additional features,
we can still check the p-values to find out if
any of those features
are considered to be statistically significant.
Notice x_2 is considered statistically significant.
The squared feature from x_1 is,
and the squared feature from x_2 is.
But you might be asking yourselves,
why these three specific models?
The simplest possible, an unknown average,
your conventional logistic regression model,
linear additive features.
But why this specific one
that has these specific features?
Well, to answer your question,
either way, I chose this one.
There's no reason why I
couldn't have chosen something else.
Let's now define a function that allows us to fit and
calculate the training set performance
for pretty much any formula.
This function is now going to do
everything from last week and this week.
We will define a function named fit_and_assess_logistic.
It will have an argument for the model name.
It will have an argument for a formula.
It will have an argument for a training dataset,
and it will have an argument for a threshold value.
The first thing we need to do is to fit a model,
smf.logit, which has a formula,
which has data and gets fit.
The data now is not df,
but the train data that I provide to this function.
The formula is now the argument a_formula.
To calculate the performance,
let's make a deep copy of
the training data set, named train_copy.
Now, I can calculate on
the training set the predicted probability,
which is the models predict method on the training set.
I now want to classify,
convert my predicted probability into a classification.
Pred_class gets assign np.where,
train_copy, pred_probability greater than the threshold.
If it's true, return the event 1,
if it's false, return the event 0.
Let's now pull out the number of true negatives,
the number of false positives,
the number of false negatives and the number of
true positives by applying confusion matrix.
We need to take the training set observed output,
which I like to convert to a numpy array and compare that
to the predicted class on the training set,
which I like to convert to a numpy array,
and the whole confusion matrix must be converted
to a 1d array via.ravel.