Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/qilb3/understanding-accuracy-in-binary-classification

English
Again, it's intrinsically coming from the question,
did we correctly classify the observation?
Do you see in row 0,
we observed the event,
but because the predicted probability
is less than the threshold of 50%,
we classified the non-event.
The classification is, therefore,
not the same as what we observed.
Therefore, this is an incorrect classification.
Look at row 1.
We observed the event.
The predicted probability is 51%,
and because that is greater than the threshold of 50%,
we classify the event.
Well, we classified the event,
we observed the event.
This is a correct classification.
Look at row 2.
We observed the non-event, y=0.
The predicted probability,
20% is less than the threshold of 50%.
Therefore, we classify the non-event.
We classify y=0, we observed y=0.
Therefore, this is a correct classification.
Let's do this one more time.
Look at row 4.
We observe the event.
We predict the probability of the event to be 90%.
That is greater than the threshold of 50%.
We, therefore, classify the event.
We classify y=1, we observe y=1.
Therefore, this is a correct classification.
The reason why I wanted to step through those handful
of observations was to highlight to you,
accuracy does not care
why or how the classification is correct.
It only cares that the classification is correct.
Whether it's correctly classifying the
non-event or correctly classifying
the event, accuracy doesn't care.
It simply just wants to know, is it right?
Let's make this concrete.
Let's add a new column that
stores if the classification is correct.
Df_copy, and I'll name this correct_class.
It gets the result of a conditional test.
Df_copy.y == df_copy.pred_class.
Now before I run this,
this syntax looks a little weird,
because I have an equal sign and then an equals equals.
I want you to remember,
so let me put in a new cell before I run this one,
equals equals does not assign anything.
Equals equals is a conditional test.
It's saying, does the value on
the left equal the value on the right?
When we have Pandas series,
we are not saying,
is the entire object the same on the left and the right?
Instead, we are applying the equals equals
conditional test to
every single element between the series.
Again, look at the zeroth.
Y is one, pred_class is zero.
Therefore, they are not the same.
But look at the other rows, 1 and 1,
0 and 0, 0 and 0, 1 and 1.
The next four are all the same.
This line of code is
creating a new column, correct_class,
that is going to store that Boolean result of true false.
Now if I want to know
the number of correct classifications,
I simply need to take my correct_class column and sum.
Because a Boolean true false
can be viewed as true is one and false is zero.
When you sum, you get
the number of correct classifications.
Let's confirm that with a call to value counts.
There are 78 trues.
But this value of 78,
it's tied to the specific dataset we're working with.
There are 115 total rows.
Well, if I was instead working on
an application with 1,115,
78 would not have the same context as 78 out of 115.
Instead of reporting
the number of correct classifications,
we are instead interested
in the proportion of correct classifications,
rather than the count.
Df_copy.correct_class.value_counts, normalize.
We correctly classified roughly two-thirds of the time.
Again, the proportion is
essentially removing the sample size effect.
Whether we have 78 out of 115 or
780 out of 1,115,
we can still talk about the proportion being similar.
But instead of working with the result of value_counts,
if I take correct_class and apply the mean method,
it's now doing the same thing.
It's counting the number of
trues and it's dividing by the total number of rows.
Because correct_class is true false or one and zero,
we can apply the mean method to know the accuracy,
the proportion of correct classifications.
However, I very rarely
create a new column
to store if the classification is correct.
Instead, I calculate the accuracy by applying
the np.mean function directly to the conditional test.
Do you remember that conditional test we started with,
df_copy.y == df_copy.pred_class?
Remember this is what we began this discussion with?
Apply np.mean to that conditional test.
The result is the same thing as
applying the mean method to that Boolean column,
which is the same result as getting
the normalized count for
the true classifications applied to that column.
Again, the reason why I prefer to use this approach is
because it means I don't have to
create the correct_class column.
I did that here just to
demonstrate we're going from asking
are these two things the same to summarizing them.
In summary, the accuracy
is a proportion between zero and one.
The closer to one the accuracy is,
the better the model performed.
The more accurate the model
is compared to the observed value.
Accuracy is a summary statistic.
It is the number of
correct classifications divided by
the total number of observations.
Again, I prefer just doing this,
meaning you have a column for the observed output.
You have a column for
the models classification or the predicted class.
You run the conditional test,
are these the same, true or false?
You calculate the mean. That's it.
That's accuracy. This is
the most common binary classification performance metric.