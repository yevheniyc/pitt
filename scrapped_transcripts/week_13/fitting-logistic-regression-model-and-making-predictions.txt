Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/sVsLu/fitting-logistic-regression-model-and-making-predictions

English
But what I want to do now is
to fit the logistic regression model.
Fit a model with a linear relationship
between the log odds ratio and the input.
Let's do that with the formula interface.
We will sign it to the object fit GLM,
SMF.logit, because again,
this is a binary classification problem.
We are not using OLS SMF.logit,
this makes sure the model predicts the probability to be
bounded between 0 and 1
via the log odds ratio transformation.
SMF.logit, we have a formula,
we have data, and we will need to fit the model.
The data is DF and the formula, y tilda x.
As discussed last week,
although it says the output is a function of the input x.
Remember, our output now is
binary zero or one non event or event.
We are actually predicting
the event probability via the logodds ratio.
The first clue that this ran correctly and
it worked was because we're not in a regression problem,
an optimization had to be
executed and it completed successfully.
Again, in 2120,
we discuss why this optimization has to occur.
For right now, you just have to know you must use
this optimization algorithm to actually fit this model,
even though no such optimization algorithm is
needed for the linear model in regression.
As discussed last week.
This logistic regression model,
it has coefficient estimates,
it has standard errors on the coefficients.
It also has p values on the coefficients to
allow us to identify if the coefficient is
statistically significant, but again,
let's go back to last week
and let's bring in the my coef plot function,
so again, I'm just going to copy this to save time.
This is the exact same function
we have been using now for
several weeks so that
we can visualize
the coefficient summaries to see that yes,
the x input is statistically significant.
Or really, it's statistically significantly positive.
Great. We now have the model,
so let's go ahead and let's make
predictions on a new dataset.
Let's visualize the predicted probability
on a new visualization grid,
just like we did last week.
But this time, when I make my visualization grid,
which I'll just call dfviz.
I'm not going to use
extreme values for the bounds of my input.
Here again, I'm going to create
the data frame, using a dictionary.
I'm going to use np.linspace just as we did last week.
But this time, let's use
something a little more reasonable and by the way, here,
if we need to remember the bounds,
we can always use dotdescribe the min and
the max on the input x negative two to 2.4.
Last week, when we visualized it,
we used the bounds of negative 5-5,
but that was to reinforce
the predicted probabilities always between 0 and 1.
This time, I want to just
use something a little more reasonable,
instead of using the actual min and max,
let's just go ahead and manually type in negative 2.75 to
2.75 and we'll use
101 evenly spaced points between those two.
Great. Let's now make the predictions.
Dfviz, and remember,
we are predicting the probability.
Our fit_glm.predict dfviz,
we are predicting the probability
of the event given the input.
Now, the next figure that I'm going to make,
I'm going to copy and paste in
from some code that I have prepared.
You don't have to type this right now.
The reason is because I want to use this figure to
reinforce what's going on
when we predict the event probability,
so you have the code here if
you'd like to make it after the fact.
But again, making this figure
isn't as important right now
as the learning concept that I want you to
walk away from walk away with from this figure.
On the x axis is the input x.
On the y axis,
it's labeled y because I included
the training set that was used
to fit the model as markers.
The output is either one for
the event or zero for the non event.
As we discussed last week,
there are no observations
of the output in between 0 and 1,
because the output is binary.
The black curve is the predicted probability.
That predicted probability, as we have discussed,
is always between 0 and 1.
But I've also included in some reference lines.
I've included a reference line at
a predicted probability of 25%,
at 50% and at 75%.
I also have some vertical reference lines just to
help with this specific example at negative one and one.
I want you to first focus on all of
the observations in the training set
when the input is less than -1.
Do you see how there's a lot more markers for y = 0,
or the non event compared to y = 1 or the event?
If there are more non events than events,
the proportion of the event is very low.
Well, look at what the model is
predicting for the event probability.
The event probability is predicted to be quite low.
In fact, it's less than 25%.
To put it another way, if I told you,
I'm going to flip a coin and there's
a 25% chance that coin will turn up heads.
If I flipped the coin 100 times,
you would expect there to be
more tails than there would be of heads,
because the probability of
heads is less than the probability of tails.
Well, that's exactly what
the logistic regression model is showing.
When the model is predicting
very low probabilities of the event,
we very rarely see the event occurring.