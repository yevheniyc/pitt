Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/2hMAV/understanding-the-impact-of-threshold-variation-on-classification-metrics

English
We also learned about the specificity or the true negative rate.
Specificity, the number of correct classifications when the event
does not occur, or true negatives divided by the total number of non events.
The total number of times the event does not occur.
True negatives plus false positives.
This tells us the model has a higher accuracy when the event
does not occur compared to when the event does occur.
Or we can express the specificity
via the false positive rate or FPR.
FPR is 1 minus the the specificity.
So the model is wrong about 24% of the time when the event does not occur.
Okay, so now why did we need to review this besides just reviewing it?
Well, in order to classify,
we have to take the predicted probability and
compare it to a threshold.
The accuracy, the sensitivity, the specificity,
the false positive rate, or just more generally, the confusion matrix.
The confusion matrix and its summary
stats, which includes accuracy,
depends on the assumed threshold value.
Changing the threshold may
change the confusion matrix,
which means the accuracy,
sensitivity, specificity, and
false positive rate may change for a model.
Essentially, the model is fixed when it is fit.
That model predicts the event probability,
but our classifications depend on
the threshold that we have used.
We can therefore change the classifications
by changing the threshold.
So let's see that now.
Let's put in a new section varying the threshold.
Let's start by increasing the threshold above 50%.
Let's change the threshold
from 50% to 75%.
Now, we will only classify the event
if the predicted probability
is greater than 75%.
But what's really important,
we do not need to refit the model
when we change the threshold.
So the model is in effect fixed.
So let's take our copy of the training set and
let's add a new column which stores
the classification based on a higher threshold.
So, DF copy thread class, and
I'll name the rest of this underscore high threshold.
This gets assigned NP where the conditional test is
still looking at the predicted probability, but
I'm now raising the threshold.
And I could have chosen 60%, 70%, 99%,
but for our illustrated purposes here, I'm going to use 75%.
If this is true, return the event y equals 1.
If it is false, return the non event 0.
We originally used the classification threshold of 50%,
so look at the one row, row one,
index value, index key, location one.
The predicted probability here is 51%.
Well, when the threshold was 50%, the predicted probability is greater than it.
So the model classifies the event, but
now with a threshold of 75%,
even though the predicted probability is 51%,
it's not greater than the threshold.
Therefore, we do not classify the event.
We have changed the classification,
even though the model itself is the same.
Classification is fundamentally a post processing step.
You classify after you have predicted all the probabilities.
You can change your classifications by changing the threshold.
The confusion matrix therefore changes.
Let's create the confusion matrix again and
visualize it using the seaborne heat map.
So we need the cross tabulation, where we take the cross
tabulation between the observed output and the predicted class.
But now the class is coming from the high threshold.
Let's include in the margins.
Let's annotate true.
Let's set the font size to be 20.
We have everything out.
Format equals 3D.
That's right, format equals 3D.
Guilty show.
Oops, I forgot to set ax equals ax.
There we go.
Okay, we have the confusion matrix.
And if I scroll back up, you can see that the colors of the cells,
the filled in tiles, they seem different.