Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/moKkv/comparative-analysis-of-multiple-classification-models

English
Hello, everyone. As you can see,
I have Jupyter opened up to this week for the class.
Now, before we get started,
please note that I have a CSV file here,
which was not present in the previous recordings.
This CSV file is
the data that we will work with in this example,
meaning we are not using the data from Week 11.
If you don't have Jupyter opened,
please pause the recording and do so.
Once you have, I want you to come and launch
a new Jupyter Notebook with a Python 3 kernel.
Let's change the name of this notebook to
Week 12 classification multiple models.
Just as when we were working with linear models,
after talking about the different performance metrics,
we can use to measure performance,
let's now use those metrics to
compare the performance of multiple models.
That's why this notebook has that name.
Let's now put in the header information comp
of 2,100 Week 12,
fitting and assessing the performance
of multiple classification models.
This notebook brings together
everything from Week 11 and Week 12.
It really does bring together everything that we've
talked about over the last two weeks.
We need programming skills
covered since the beginning of the semester.
Let's now import the modules.
You need the big 4.
You need numpy as np,
you need pandas as pd,
you need matplotlib.pyplot as plt,
and you need seaborn as sns.
You also need to import
the statsmodels formula.api as smf.
But then we also need three functions from
SK Learn to streamline
calculating the classification performance.
We're not going to manually calculate the performance
or we will minimize manually calculating
the performance here in this example.
From sklearn.metrics, import confusion_matrix.
From sklearn.metrics, import roc_curve.
From sklearn.metrics, import, roc_auc_score.
Remember, the confusion_matrix allows us to
calculate accuracy sensitivity, specificity,
false positive rate for a given threshold ROC curve,
and its quantitative version,
the ROC AUC allows us to calculate the behavior across
many different threshold values.
Let's now read in the data.
Use new data not from Week 11.
Let's assign the data to the df object, pd.read_csv.
I'm not navigating backwards.
I'm bringing in
the week_12_binary_classification_csv data set.
This DataFrame has six columns
where every column starting with
x is an input and y is the output.
As you can see, the output is an integer,
x_5 is an object.
The other four x's are floats.
If we check the n unique method,
the output y has two unique values.
Even though it's an integer,
it's a number, it only has two unique values.
It is a binary variable.
The object variable x_5
has three unique values. It's categorical.
The other four that are numbers
all have a very large number of
uniques they are continuous variables.
Of course, we could look at the D types directly,
and to confirm it,
the value counts for y,
two unique values zero and one.
The output y = 1 is the event,
which occurs just 34% of the time.
Our binary outcome is still y = 1 for the event.
But notice in this example,
the event now occurs less frequently than the non-event.
We can calculate that observed proportion
of events, two different ways.
The reason why this is so critically
important is because when
we calculate the model accuracy,
it actually doesn't matter if the accuracy is above 50%.
What matters is is the accuracy better than
the observed proportion of the event or in this case,
one minus the observed proportion of the event?
You could see the event occurs 66% of the time.
If we would have a model that has an accuracy of 66%,
that doesn't mean it's a good model.
Because 50% is only a reference point
if the event occurs 50% of the time.
That's an important thing to keep in mind.
Now, I'm not going to go through
a complete exploratory data analysis,
a visual exploration of this data.
The last thing, though, I will show you here
is the bar chart for the categorical input,
just to show you that
one of the categories does
have a higher frequency than the other two.
But we don't have
a very large disproportionate difference.
We don't have one of the categories that is 10 times
less than the accounts of another.