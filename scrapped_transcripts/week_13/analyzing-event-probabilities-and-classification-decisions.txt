Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/sHAEY/analyzing-event-probabilities-and-classification-decisions

English
Look closely at the result.
For the left most interval,
when the input is less than -1,
we only see one event out of a total of 19 rows.
The proportion of the event is roughly 5%.
Here I'll switch the colors to make them match.
Now, go to the other extreme.
When the input is above one,
it's on the far right hand side.
Out of the 16 observations,
14 of them are the event,
so the proportion of the event is very high.
Lastly, go to the middle interval,
when the input is between -1 and 1.
We have 80 observations,
33 of them are the event,
so the proportion is something in the middle.
Look what just happened.
We saw from our predictions.
When the predicted probability is low,
the number of events appears to be low.
When the predicted probability is high,
the number of events is high.
When the predicted probability
is in the middle around 50%,
we get a mixing between events and non-events.
Well, that's exactly what's taking
place when we look at it even from this discretized view.
Proportion of events is
low when the predicted probability is low in here.
Let's even confirm this one more way.
We'll name it average_pred_prob.
We actually haven't calculated that yet.
We can't do that. We'll return to that in a second.
But as you can see, visually,
for the inputs associated with low probabilities,
the event occurs infrequent.
Input with very high probabilities,
the event occurs very frequently.
Now, why did I want to bring this up?
What was the purpose of going
from something we talked about last week,
predictions to all this
discretizing and binding and all of that?
Or why does this matter?
The logistic regression model does
not classify event or non-event.
The logistic regression model
predicts the event probability.
The event probability represents what we expect to see.
It does not represent what an individual observation is.
The predicted probability is
the expected proportion of events.
That's why this matters.
We are not predicting
what an individual data point will be.
Like, for example, look at
this one data point right here.
We saw the event,
even though the model is
saying the chance of the event is very low.
If we would observe a lot of
observations in this interval of the input,
we anticipate to see the event very rarely,
and that's exactly what's happening.
But how can we
make a classification of a single observation?
This comes down to making a decision.
We must convert our predicted probability from
a number 0-1 to a class or a binary value.
We must decide which output category
the observation is based on our predicted probability?
This sounds like a very challenging thing.
But making the decision is very straightforward.
Making the decision is essentially common sense.
Classifications are created by comparing
the predicted probability to a threshold value.
If the predicted probability
is greater than the threshold,
we classify the event.
However, if the predicted probability
is less than the threshold,
we classify the non-event.
That's it. Classification is an if statement.
This is even how neural networks and
deep learners handle classification.
The models are predicting probabilities,
and yet we classify using a simple if statement.
That if statement, that conditional test is very simple.
Let's see how to do that.
Let's predict the training set
now instead of the visualization grid.
We already have that copy of the training set,
so we'll stick with using this.
As a reminder, this training set has 115 rows.
Make the prediction and assign
the predicted probability to a column.
Df_copy pred_probability.
Again, this is a new column.
You can see it does not exist in the current DataFrame,
and if I run this line, I get an error.
I'm using this approach to assign a new column.
Fit_glm.predict.
But I want to predict the data used to fit the model,
so I'm giving it the training set directly.
Df_copy now has this new column pred_probability,
which did not exist before.
Look at the zeroth row.
The input is negative.
We however, observed the event,
even though the predicted probability is low. It's 27%.
Look at this other.
Let's see. Let's look at another one here.
How about down here?
The input is positive.
We observed the event.
The predicted probability is 52%.