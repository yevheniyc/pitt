Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/D2YmJ/understanding-roc-curves-and-roc-auc-in-model-evaluation

English
Now, the ROC curve,
You see this reference line of 45 degrees,
meaning the false positive rate
is equal to the true positive rate?
You never want to see this.
The ROC curve is basically one of
the only graphical performance tools
where the curve falls along a 45-degree line,
that is a really bad model,
because that means the errors are
perfectly trading off and the model,
it's literally just random guessing,
it has no idea what's going on.
The ideal shape for
the ROC curve is a right angle step change,
something that looks like this.
This ideal shape means
the model is very accurate when the event occurs,
and the model is never
wrong when the event does not occur.
The ideal shape is a square.
The worst possible shape is this straight line,
diagonal line or a triangle.
Now, the ROC curve is a graphical thing,
which means we can look at it,
but it's not a hard number.
The ROC curve can be converted to a quantifiable number
by integrating the area under
the curve or calculating the area under the curve.
This number is known as
the ROC AUC for area under the curve.
In our current example,
the area under the curve is
everything below that step line.
Now, you don't ever have to
calculate that area under the curve.
There's a function that does it
from sklearn.metrics import roc_auc_score.
It's syntax is consistent with the ROC curve function.
You need the observed output,
and I like to convert it to
NumPy and you need the predicted probability,
and I like to convert that to NumPy.
Our model has an ROC AUC of 76% or 0.76.
The ideal ROC AUC is one,
and the worst possible value is 0.5.
Again, the idea comes from you have this square shape,
and so the area of the square,
the width is one, the height is one, so the area is one.
The worst possible shape is
this 45-degree line so you
have a triangle that has a width of one,
a height of one, 0,5 times 1 times 1 is 0.5.
So the worst value is 0.5 and the best value is one.
In summary, the confusion matrix,
and all of its summary stats, accuracy,
sensitivity, specificity,
and FPR require a threshold to be assumed.
Once you assume a threshold,
you can converge
the predicted probability into a classification.
You can measure the performance
against the known observed output.
However, you can change
the classification by changing the threshold.
From a performance perspective,
increasing the threshold improves
certain metrics at the expense of other metrics.
Likewise, decreasing the threshold
hurts certain metrics while improving other metrics.
The ROC curve is a graphical tool to represent the trade
off between the metrics
at all possible threshold values.
The ROC curve can be quantified by the ROC AUC.
Now, also from a practical point of view,
as you've seen in this example,
I was constantly showing you how to do something
manually and then showing
you a simple function that will create it for you.
I did it manually so you
could understand what was going on in
that function and why it's so
important and then showing you how to
use the simplified function call
to pull out the information that's important.
I hope you liked this example.
This is the last performance
metric for classification that we
will use in this semester in 2100,
2120, we'll build on all of these ideas and
introduce several additional kinds
of binary classification performance metrics.