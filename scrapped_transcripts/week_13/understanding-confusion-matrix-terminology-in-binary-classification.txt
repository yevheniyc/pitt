Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/Jlx4h/understanding-confusion-matrix-terminology-in-binary-classification

English
So I want you to pay close attention to the terminology here,
because I like to use event or y = 1,
For the thing we're interested in, and non-event,
Or y = 0 for the other class.
But historically the event was known as the positive class or
the positive case, and the non event was known
as the negative class or the negative case.
The confusion matrix is its names,
its labels follow the historical conventions.
A true positive, we classified the event, but
we did not, or sorry, and we observed the event.
So true positive, we classified the event and we were correct.
False positive, we classified the event, but we were wrong.
We were incorrect, we were not right.
So whenever you see positive, that's telling you what the model says.
True or false tells you if the model was wrong.
Likewise, true negative,
we classify the non-event and we were right, true.
False negative, we classify the non-event, but we were wrong.
So again, anytime you see in the confusion matrix
these terms, positive or negative,
that's actually in reference to the model prediction.
True or false tells you if the model was correct, yes or no.
If you ever forget what name means what,
just go back to the confusion matrix itself.
Start with what is getting predicted.
We are classifying y equals one, the event.
We observe the event, we were correct, so
the classification was positive and the classification was correct.
So TP for true positive.
Conversely, if we would have a classification of y equals zero,
the non event, we have a negative classification, we observe the non event,
the classification is right, correct, we have a true negative.
Now, as a side note,
I like to use event and
non-event instead of the positive and
negative terms because positive sounds good.
I often work in problems where the event I
want to classify is a bad thing, such as a failure.
So that's why I don't like to use positive negative,
because like in my most of my applications, I would say,
I would have to say we have 27 positive classes that are correct.
That means we observed a machine failing 27 times,
and the model correctly classified that positive event.
And that just sounds bad.
That sounds confusing.
So I like to use the event and
non event language because it doesn't put a connotation on good or
bad, because good or bad is context dependent.
Now, I think visualizing this confusion matrix
with the labels really helps you know what's going on.
But Scikit learn has a function that calculates the true positive,
true negative, false positive, and
false negative counts for you.
This function, which comes from sklearn
metrics, is named confusion matrix.
So from sklearn.metrics import confusion_matrix.
The arguments for this function are the zeroth
argument, the observed output, y.
And I like to force this to be a numpy array.
The one argument is the predicted class,
which I like to force to be a numpy array.
The result is a 2D array.
There are two columns and two rows, and look at the counts, 51, 16, 21, 27.
If I scroll back up 51, 16, 21, 27.
This is a useful function, but it loses its context
because we have lost the specific variables and
their categories when we did the cross tabulation.
So I personally like to use Pandas and Seaborn to make the heat map,
to make the confusion matrix because I get all the labels.
That way, I don't have to remember what this cell of 16
actually corresponds to, but this is another way to do it.