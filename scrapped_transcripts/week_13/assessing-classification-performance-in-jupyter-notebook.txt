Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/uZQzl/assessing-classification-performance-in-jupyter-notebook

English
Hello, everyone. As you can see,
I have Jupyter open to this week for the class.
If you don't have Jupyter open,
please pause the recording and do so.
Once you're back, I want you
to launch a new Jupyter Notebook,
a new Python 3 Kernel.
Once you've done that, I want you to change the name of
the notebook to week_12_classification_accuracy.
The name of this notebook reflects,
we're now going to talk about assessing
the performance of our classifiers.
Let's now put in the header information.
CMPINF 2100, Week 12,
Measuring Classification Performance, Accuracy.
As with the previous video,
I want you to import modules.
You need the Big 4: Import NumPy as np,
import Pandas as pd,
import Matplotlib.pyplot as plt,
and import Seaborn as sns,
and just as with the previous video,
I want you to also import statsmodels.formula.api as smf.
Now, we're going to work with
the exact same example as the previous recording.
We will read in the data,
read the data from last week just as we did previously.
Let's assign this to the df object,
the result of pd.read.csv,
and you need to use the same syntax to
move back to Week 11,
and then select the
week_11_intro_to_binary_classification.csv file.
Again, the file path reflects we move
back then forward to Week 11, and this file.
The slashes are separating
the directory and file names out of the entire path.
You run that. As you can see,
we have read in the same data.
Let's now fit the logistic regression model.
Just as we did in the previous video,
let's name this model fit_glm, smf.logit,
logit we need to make sure we are predicting
the event probability within
its bounds via the logit transformation,
the log odds ratio.
We have a formula,
we have a data set,
and we fit the model.
The data is df,
and the formula is y ~ x.
Great, the optimization terminated successfully.
But now, we're not going
to make predictions on new data,
this video is all about measuring performance,
so let's predict the training set.
Just to be safe though,
let's make a copy of the training set
and add new columns to the copy.
I like to do this because it makes sure
the actual data used to fit the model
is never modified just in case we make some mistakes.
That's the reason why I like to make this hard copy.
But now, in this copied data frame,
let's predict the event probability at each observation.
Let's add a new column, pred_probability,
where we call the predict method on
our fitted stats models
object where we use the data frame,
the data used to fit that model.
Our copy data frame now has the input,
the observed output, and the predicted probability.
As discussed in the previous video,
the predicted probability is between 0-1,
which here we can confirm with a.described.
The min and the max are between 0-1.
Let's now classify the predictions of the training set.
We know that to go from
the predicted probabilities to a class,
we must ask the if-else statement,
we must ask, is
the predicted probability greater than a threshold?
Let's continue to use
the common or default threshold of 50%.
Add another new column,
pred_class, which gets assigned the result of np.where.
In np.where, the conditional test
is the predicted probability greater than a threshold?
I'm using the 50% threshold.
If true, return one for the event,
if false, return zero for the non-event.
We now have another new column, pred_class,
it's an integer, zero or one.
If we check the nunique,
there are only two values of pred_class.
We now have predicted our training set.
When we predict the training set,
there is a known observed output value.
We can now directly compare
our classified predictions to the observed class.
The most common performance metric
in binary classification is accuracy;
accuracy is the proportion of times
the model correctly classifies the observed output.
Accuracy is the number of
correct classifications divided by
the total number of rows.