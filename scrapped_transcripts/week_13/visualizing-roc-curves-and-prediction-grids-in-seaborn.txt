Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/yD0YZ/visualizing-roc-curves-and-prediction-grids-in-seaborn

English
Let's visualize the ROC curve for
each model as a specialized line chart in Seaborn.
So before we make this, let's convert model_name
to a category, since it's an integer,
roc_df.model_name as type category.
It's now a categorical variable.
Now, the line chart sns.relplot,
data = roc_df.
The ROC curve is the false positive rate on the x-axis,
the true positive rate on the y-axis.
I want to color by model name.
Now, this is a line chart.
I do not want to summarize, so
estimator is none and units is model_name.
This ROC curve is pretty close.
You could basically stop here because you still see,
we get those weird steps that are occurring.
And remember the 45-degree line.
If you get a 45-degree line, that means the model is really, really bad.
So that's model 0 here.
I know it's a little tough to see, so let's now use facets.
sns.relplot data = roc curve df,
x= 'fpr', y= 'tpr'.
Hue= 'model_name',
kind= 'line', estimator=None,
units= 'model_name',
col= 'model_name', and col_wrap,
how about five columns per row?
This separates them, and
you can see as the complexity is increasing,
Do you see how the ROC curve starts
moving up diagonally to the left
until that most complex model here?
It's getting the closest to the ideal, which is that right angle step change.
This is what we saw previously in the notion that
the ROC AUC was highest for the most complex model.
So the performance summary according to the training set,
the data used to fit the models,
the model with the most unknown,
the most number of unknown coefficients or
slopes to estimate is the best.
And I'm really trying to stress that the model performance on
the training set gets better as the number of unknowns increases to really try and
make you feel that this doesn't seem right.
Because it essentially means I could make a perfect model
by literally having a model with an enormous number of features.
And that model would get better.
That model, it would actually get better on the training set.
Now, one final thing, let's visualize predictions.
Let's make a prediction grid to allow
visualizing the predicted event probability for
different input combinations.
Now, we haven't looked at any of the coefficient summaries here.
So we have not tried to identify which input we
think is related the most to the output or
to really the event probability through the log odds ratio.
So let's just focus on the relationship
between the event probability and
x1 for different values of x2 and
the categorical input.
because remember, there's x1, x2, x3, x4, and x5.
Okay, so let's make this input grid,
input_grid pd.DataFrame.
Now, this DataFrame is going to be created following
the approach introduced last week when we had multiple inputs.
Those inputs, x1, x2, x3,
x4, x5, for x1 in some sequence,
For x2 in some sequence, for
x3 in some sequence, for
x4 in some sequence, and
for x5 in some sequence.
The columns will be named x1,
x2, x3, x4, and x5.
Now, the sequence for x1,
let's use np.linspace
df.x1.min to df.x1.max.
And here we'll use 101
evenly spaced values, for
x2 np.linspace df.x2.min to df.x2.max.
Let's go with nine evenly spaced values.
Now, x3 and x4 will be constants,
so I'm going to input those constants at the mean values.
The sequence, though,
is defined within a list to make it look like it's not literally a single number.
So it looks like it's an iterable sequence.
Lastly, x5, df.x5.unique.
Oops, I have an error.
Let's see where that error is at.
I see what I did wrong.
I forgot that in this list comprehension, you don't need commas at the end.
So you don't need commas in between the separate nested sequences, the fors.
So let me get rid of those commas.
There we go.
So it does look a little weird because there's no comma here.