Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/j6N1l/modeling-and-evaluation-in-logistic-regression

English
Every element in this list
is now a model or is really defining a model.
My zero model is the unknown constant event probability.
The two model is linear additive, the continuous inputs.
The three model is linear
additive for all of the inputs, so on and so forth.
Let's use a for loop to iterate over all formulas.
We will apply
the fit and assess logistic function to each one.
By the way, if you were wondering why do
I have this mod name argument,
that's because I wanted to add in
a unique identifier for
each model besides justice formula.
Let's initialize an empty list.
Results list equals an empty list,
and now for m range ln formula list.
For m in range,
m is now a sequence zero up
to the excluding the full length of formula list.
Results list a pen.
Fit and assess logistic.
The name of my model will be or sorry,
the integer m. The formula that I'm going
to use is the mth element from formula list.
The training data is df and
the threshold that I'm going to use is 0.5.
Remember, I need a specific threshold
in order to calculate the confusion matrix,
which lets me give the accuracy
sensitivity specificity of FPR.
The ROC curve, however,
tries out all possible thresholds.
This way, I'm able to get an idea of what happens to
the performance if I don't specify one common threshold.
That's the real reason why I like the ROC curve so much.
You're literally able to examine what happens to
the model regardless of the threshold.
Now that we've set this up, let's run it.
A lot of stuff gets displayed because we're getting
a fit for each one of
the elements in our list, each formula.
We get displayed all of these terminated successfully.
All of those models fit successfully.
There are 17 elements in the list because there are
17 models that we fit in formula list.
Each element is one model.
Combine or concatenate all results
into a single data frame.
Let's name this data frame results df,
pd.concat results list, ignore index equals true.
Results df now provides one row per model.
I'm showing that model with its formula.
But because some of the formulas are so crazy,
I also have just a unique identifier.
Here's model 3 versus here's model 16.
But what's particularly useful if
we go back up, remember,
I also pulled out the number of parameters,
the number of coefficients estimated.
I know how many slopes
that had to get estimated for these models.
The simplest model is just an intercept.
It has one coefficient.
But look at that last model.
There are 81 coefficients estimated with it.
Which model is the best according to the training data?
Well, we have actually
a bunch of different performance metrics.
For example, we could sort by the accuracy.
I want to sort in descending order.
Ascending equals false.
The model with the highest accuracy
is the model with the most complex formula,
the model with the number
of coefficients to have to get estimated.
That model had a training set accuracy of 81%.
However, what if I sorted
by ROC AUC, ascending equals false.
Remember, accuracy requires one threshold.
But ROC AUC tries out all possible thresholds.
Well, ROC still says
the best model is the one with the most coefficients.
Essentially, what's happening here is
the training set performance is
getting better as the number of coefficients increases.
We can actually confirm that if we make a scatter plot
between coefficients and one
of these metrics like accuracy.
If we define L plot to be scattered,
in general, as the number of coefficients increases,
the accuracy is getting higher.
It's the same story if we would instead focus on ROC AUC.
The plot isn't exactly the same,
but it's still telling a similar story.
We need to estimate
more coefficients when we
add or include more features in the model.
Our complex model has
features coming from polynomials and interactions.
By including all of
those interactions and these polynomial features,
we have generated so many features that
there are 81 unknowns that need to be estimated.
What we are learning here is that the number of unknowns
seems directly related to
improving the training set performance.
Now, before we wrap up,
let's visualize the entire ROC curve for each model.
This way, we can confirm that the ROC AUC metric is
also identifying the complex model as the best.
Our previous function ignored
or only gave us the ROC AUC,
so we need another function that
returns the complete ROC curve for each model.
I'll name this function to find fit logistic, make ROC.
It has the exact same arguments as the other one,
mod name, a formula, train data,
but it does not need a threshold because
the ROC curve tries out all thresholds for us.
We first need to fit
the logistic regression model
using a formula on some data and fit it.
The data is not df,
it's train data, and the formula is a formula.
Let's use the same approach where we made
a deep copy of the training set.
On the training set,
we add a new column, cred probability,
which is equal to the predict method
of the model applied to the training data.
Now, we can get the FPR, the TPR,
and the threshold for
the ROC curve by calling the ROC curve function.
This needs the observed output converted to numpy and
the predicted probability also converted to numpy.
Again, the ROC curve,
does not require a single threshold.
Therefore, it does not need a classification.
Behind the scenes, it's going to
classify for us for all possible thresholds.
Now let's just create
a data frame to handle the bookkeeping,
pd.dataframe coming from a dictionary,
TPR is TPR, FPR is FPR threshold is threshold.
Then just add in two more columns,
model name is mod name.
Model formula is a formula,
and then we return that dataframe.
Let's make a new list.
Rocklist for n in range the length of the formula list.
The ROC list.10 fit logistic make ROC
m formula train data df.
Again, I'm getting the mth element
from the formula list,
and I'm naming that model, that integer.
All of the models get fit again,
and then we can combine through pd.concat
the ROC list into a new data frame, ROC df.
This dataframe has
the true positive rate, the sensitivity,
the false positive rate, the FPR,
and the threshold for all of the models.