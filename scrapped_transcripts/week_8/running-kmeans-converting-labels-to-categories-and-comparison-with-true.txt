Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/lysHd/running-kmeans-converting-labels-to-categories-and-comparison-with-true

English
I like to treat cluster labels as categorical variables.
Thus, let's convert the integer
to the category data type.
The Pandas category data type is a specialized
data type for categorical variables.
It's not an object or string.
It is a special data type because it knows and
understands all of the unique values.
To do this, we will assign something new to the k three
column where I take the k three column and force it to be a new type.
Using the astype method, I'm forcing it to be a category.
Now, if we check the info, you can see it's a category data type.
It's not an integer.
Because k three is a category which is distinct from object,
it actually has some additional attributes.
The cat attribute has the categories.
These are the unique values.
It's telling you the unique values are integers zero, one and two.
Now, when we manipulate k3, it will often just look like as it was before.
We can do value counts, we can create bar charts.
We can even color our
strip plot by k three.
But now, because k3 is a categorical, we don't get
the default numeric color scale, that sequential scale.
We get the default object or categorical color palette or
qualitative color palette.
So when you're visualizing cluster analysis results,
I really recommend converting the cluster labels to categories.
That way, you get to use categorical color palettes by default.
Now, because this is a simple example where we know the true groupings,
we can ask how do the k means results compare to the true answers?
You can never do this in a real data set.
However, you are clustering
the rows based on numeric columns.
Thus, you are not using the categorical variables in the clustering.
You can therefore compare the cluster results with the known categories,
whether those are actual true groupings, like what you're about to see,
or if there's just some other kind of grouping you could compare to.
Now how can you do these comparisons?
Well, you could use a dodged bar chart,
but I prefer to use heat maps.
Meaning we need to calculate the cross tabulation
between our known grouping and the cluster assignment results.
This is the exact same organization as when we looked at the combination
of two categorical variables.
The known groupings are down the row.
The cluster assignments from k means are down the row.
The known groupings are across the columns.
But I like to include the total or
margin counts.
We can do that with the additional argument called margins to pd.crosstab.
And this is something we have not seen before, but margins=True.
You now get the marginal counts for
the known groupings five, five, and five.
You get the marginal counts for the cluster assignments six,
five and four, and then you get the total count.
So look closely here.
Let's focus on the a group.
All five observations of the A group are in cluster 0.
Next, look at the C group.
All five observations of the C group are in
cluster two, but now look at the B group.
There are five observations from B, and yet, one of them,
Are in cluster 0, four of them are in cluster 2.
And so what that also means is the five from c are in cluster one.
Again, the combinations let us know how our k means result
compared to the known clustering or the known groupings.
But let's visualize within a heat map.
Fig, ax = plt.subplots,
sns.heatmap data = pd.crosstab
df_copy the true groups,
df_copy the clusters, margins=True.
I want to annotate, and I will set the font size.
How about to 20?
Since I'm showing the counts as annotated text,
I'll remove the color bar ax=ax.
This is the exact same set of information as in the printout.
I just find it to be a little easier because we get the zeros as
some dark color and the non-zeros as another color.
Again, look at the known B group.
There are five observations in it, and
yet one of those five gets associated with
the same cluster as the five from a four.
The remaining 4 from the B group are separated away from that 1.