Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/I77Bc/understanding-kmeans-clustering-variability-within-cluster-variance-and

English
On the left, I have the previous assignments.
On the right, I have the current assignments.
The previous version basically split
the data as a cross, down the middle,
vertically and horizontally.
But this current result splits the data as an X diagonally.
You can get a different set of clusters because the initial guess is random.
This is really important with K-Means,
because we literally need to randomly restart the algorithm many times.
Because there's no guarantee we will get the absolute best approach or
best set of clusters since it's all just initialized randomly.
You therefore never run K-Means once and only once.
You have to run it many, many times.
I personally like to run it 25 times, if not 100 times,
whenever I use the K-Means algorithm.
But now, as you can see, because I have to specify the number
of clusters, how do I know four clusters is the best?
Well, we don't.
In this case though, because there are just two variables, two dimensions,
and I can kind of look at it, I can get a sense of, gee, I think there's one,
two, three, four, five, six, seven regions, seven groupings,
these kind of six-flower petals and then the center point.
So let's try out, one, two, three, four, five, six, seven, seven clusters.
And now, initialize the assignments, update the centroids,
and iteratively, reassign and reupdate.
And let's just keep running the algorithm.
As we iterate,
you can see how the groupings were kind of walking around the flower petals.
It kept iterating, it kept changing until it stopped
assigning the points to different clusters.
As you can see here, with our seven clusters,
we have now created groupings that are smaller than when
we had groupings based on two clusters, three clusters and four clusters.
In K-Means, the goal is to minimize
the variance within each cluster.
We want to have the neighbors in one group to be very close to each other and
thus very far away from other groups.
We're trying to minimize within cluster variance.
But you might be saying to yourself, okay, well,
that sounds great, but what if I chose one, two, three, four,
five, six, seven, eight, nine, ten, eleven clusters?
If I then initialize and then iterate,
won't this minimize the cluster variance within
cluster variance compared to having seven clusters?
Because after all, we have more clusters and the answer is yes,
but this is something we'll return to in the programming example.
It will be easiest to understand this when we go through the programming,
but conceptually, let's go back and use one, two.
Do you see how large each grouping is?
The variance is quite large here.
The metric in K-Means is known as the total within sum of squares.
The total within sum of squares,
and this is analogous to
the variance across the groups.
We want the within cluster variance to be as small as possible.
The more clusters we have,
the smaller the within variance is.
Okay, so qualitatively, again, the major elements
of K-Means, it works with numeric variables.
It defines similarity based on distance.
It wants to find data points close to each other to put in a group, and
data points far away to put in another group.
You initialize randomly.
You then iterate, reassigning and recalculating centroids.
You stop iterating when you've reached convergence.
You're no longer changing the assignments.
But because the initial guess is random, we have to run the algorithm many times.
Okay, I hope you liked this qualitative introduction.
Again, the slides walk through in much more detail how
we go about getting the total within sum of squares and
what happens at each individual iteration.
But that's all for this video.
In the next one,
we will actually program the K-Means algorithm using existing functions.
Thank you.
Good luck.