Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/vYkZl/running-kmeans-understanding-proximity-based-grouping-and-cluster-variance

English
Essentially, KMeans has mis
labeled or mis identified.
One of the observations.
But is K means wrong?
And the answer is no.
Because KMeans has no idea
that there is in fact a known group.
It wasn't told it at all.
Remember, we gave the KMeans
algorithm only the x variable.
It had no idea about the other one.
How does it execute the groupings, the clusterings?
KMeans groups by distance.
The closer observations are,
the more similar KMeans feels they are.
I use feels specifically,
because that's literally what it's doing.
There is no known true assignments.
It's making its best guess based
on minimizing the variance in each group.
But let's visualize what caused the incorrect label.
To do that, let's use catplot,
df copy x equals x.
We will again color by the KMeans result.
We will again use the strip plot where jitter is false,
but now set the column facets to be the true group.
Now, these markers are a little tough to see,
so I'm going to come in here and set here s equal to 50.
S is hard coding the size of the marker.
Let's make them even bigger, s equals 50.
True group B has these four in cluster two,
but it also consists of one labeled in cluster zero.
Because cluster zero had 1, 2, 3,
4, 5, 6, data points.
Why is this one data point in
cluster B getting associated
with points actually in cluster A?
Let's use the marker shape via
the style argument in SNS relplot.
SNS relplot, data equals df copy x equals x.
But now, I'm going to use a weird hack.
I'm going to say y equals a constant one,
q is k3,
style is true group.
Let's make the figure a little wider,
aspect 1.5 plt show.
Let's raise the marker size a little bit to help us out.
I'm drawing in the separations between the true groups,
and the true groups are the shape.
So do you see this x? This point here,
is actually associated with the green four x's.
So cluster two should have contained this blue.
x But based on what KMeans saw,
this data point is closer to the center of
the zero cluster compared
to the center of the two cluster.
Because of that, KMeans feels
this point is associated with the A group.
The main takeaway from this is
KMeans has no idea about any known groupings.
All it knows is how far
apart each of the data points are from each other.
It wants to associate
closeness distance to be the way to define similarity.
Now, what happens if you use more clusters?
Let's try eight clusters.
We need to go about the same set of actions.
We need to initialize the assumptions,
KMeans with n clusters equal to eight,
random seed, the number of initial random guesses,
and then the max number of iterations per guess.
The only difference between
this line of code and what we did previously,
n clusters equals eight rather than three.
Let's now assign the result to
the k8 column where we have a Panda series,
k8 fit and predict x where index equals df copy index.
If we check the info method,
k8 is an integer?
There are eight unique values.
But to support the coloring and c borne,
less force, k8 to be a categorical variable.
Now, it is a category just as k3.
This makes it very easy to color our strip plot.
What we can see here,
as well as with value counts,
some of the clusters have one and only one data point.
Those clusters therefore have zero
within cluster variation because there literally,
one data point in it.
It's not using the unbiased estimator
of variance because it's
literally just summing the squared distance
as discussed in the presentation slides.
The max number of
clusters equals the number of rows in the dataset.
This would give one observation per cluster.
Essentially, each data point is its own group.
Let's execute this.
KMeans n clusters is 15.
Random state 121,
n init 25, max iter 500.
Df copy, k15,
wrap a pd Series of km15 fit and predict x,
the index is df copies index.
Again, the result is an integer.
If we check the number of unique values,
there are 15 unique values.
Let's force k15 to be a categorical variable.
If we do the value counts one data point per cluster.
This is essentially meaningless.
If every data point is its own cluster,
we have not found any meaningful groups.