Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/wP16E/optimizing-kmeans-clustering-preprocessing-knee-bend-plot-and-cluster-analysis

English
The blue, Is cluster 0.
The x or Chinstrap,
Do you see these blue x's here,
those are Chinstrap penguins that are labeled as
the same cluster as the Adelie species.
They have more in common with the Adelie
than they do with the other Chinstraps,
that's what the labeling is saying.
Are they Adelies?
No, they are not, but
their numeric features are more similar to the other set of groups.
But now, how do we figure out the optimal number of clusters?
We need the KNEE BEND plot.
We have to calculate the total within sum of squares.
We need to specify a range of clusters starting from 1 cluster up to about 30.
We will iterate for k in capital K, where we initialize the k-means for
a given number of clusters, you set the seed,
you specify the number of random restarts,
you specify the max number of iterations.
You then fit using these cleaned and standardized features,
you calculate the total within sum of squares by
extracting out the inertia attribute.
So in practice, when you are running k-means clustering,
you must first check, are there missings.
You must decide how to deal with them.
The simplest way is to drop all rows that contain missings.
This is throwing away data,
but it's in order to provide to you a complete set of cases.
You then pre-process, you remove the magnitude and
scale for numeric columns in the cleaned dataset.
You apply k-means to the standardized columns in the clean dataset,
begin with two clusters, then use as many clusters as some known grouping variable,
compare the counts for the cluster assignments with the grouping,
and then figure out the optimal number of clusters using the knee bend.
So visualize the KNEE BEND plot.
fig, ax = plt.subplots() ax.plot(k,
tots_within), I want blue markers that connect
the lines, ax.set_xlabel('number of
clusters') ax.set_ylabel('total
within sum of squares' ) plt.show().
Since this is a real dataset, we don't have as sharp
of a curve as we've seen in the previous examples,
instead we get this smoother curve where yes, the total
within sum of squares keeps decreasing as we add more and more clusters.
But it's clearly starting to give us some point of diminishing returns,
going from 5 to 15 clusters reduces the total within
sum of squares less than what we saw from going from three to five clusters.
I like to try and identify something that seems to be roughly flat,
so anything more than ten really seems like overkill here,
and then I try and identify two major knee bends,
here's one, here's another one at 5.
This first knee bend is at 3, and then here's 5, and then 6.
So again, there is no true, perfect, or right answer,
you have to forget that you're going to get the perfect answer, there is none.
This is very subjective.
But anything, in my opinion, from 3 to 6 is definitely valid,
you could maybe even make a case for 7 or 8, 9,
nothing more than 10, but let's just see what happens.
What if we would use five clusters based on the knee bend?
So clusters_5 = KMeans(n_clusters=5,
random_state=121, n_initial=25,
max_iter=500).fit_predict(X).
penguins_clean_copy['k5'] =
pd.Series( clusters_5,
index=penguins_clean_copy.index).astype('- category').
Now put in the pairs plot where
you set the hue based on k5, and
again, make sure you remove
the sample size effect.
Having five clusters is dividing two
out of the three clusters in half.
You can see that really clearly for the orange to the red,
as well as between the purple and the green.
Now, can you make a guess as to maybe why it's doing that?
Remember, there's male and female, so
maybe more clusters are not only breaking things up by species,
but also between males and females.
Again, this just gets at, there is no complete perfect answer,
we have to come up with something that seems the most reasonable based on
the information that we have.
That's everything for this video, it's a complete, realistic application of
k-means that requires cleaning the data and pre-processing it.
I hope you liked seeing it, especially interpreting the results.