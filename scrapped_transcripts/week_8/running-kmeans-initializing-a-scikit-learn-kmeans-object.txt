Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/0S7Ue/running-kmeans-initializing-a-scikit-learn-kmeans-object

English
The function to execute
KMeans comes from the scikit-learn module.
scikit-learn is rarely imported directly.
Instead, we import functions from scikit-learn.
You will never see me import scikit-learn.
It's too large.
It has too many things in it.
Instead, we usually pick out the function we will need.
For KMeans, the function is named KMeans,
and it comes from the sklearn.cluster module.
scikit-learn or commonly sklearn has many submodules,
one of them being cluster.
Rather than writing import,
you start with from,
from sklearn.cluster, import the function KMeans.
Notice I was not completing here.
From sklearn.cluster, import KMeans.
KMeans, it's called a class.
But as you can see, we will
execute it as if it's a function.
All functions from scikit-learn
follow the following recipe.
You must initialize the object,
and this is where you specify
the assumptions for how the method is executed.
Then you fit the object,
which is where you learn what you need to learn.
Lastly, you predict or transform.
We will discuss when you predict versus transform.
But qualitatively, you can think of them
as doing the same overall action where
you return the values
you want from a fitted object.
Initialize and let me put this in all caps.
Initialize, fit,
and then predict or transform.
Let's begin by initializing.
In KMeans,
the assumptions are the number of clusters,
the random seed which controls
the randomness or the initial guess,
the number of initial guesses,
and the number of iterations for each initial guess.
Those are the four main assumptions.
There are others, but those
are the four we will focus on.
KMeans you have to tell it how many clusters to use.
The initial guess is random,
so you want to control the randomness.
As discussed in the previous video,
the final result will depend on the initial guess,
so you need to run it many times.
It is an iterative algorithm.
It iteratively reassigns observations to
the nearest cluster and
then it has to recalculate the centroid.
You have to pre-tell it how many iterations to use.
Let's begin with three clusters,
since we know there are, in fact,
three groups in this small application.
I will assign the object to km3,
calling the KMeans function.
Now, these four assumptions,
the n_clusters argument controls the number of clusters.
The random state controls
the randomness by setting the seed.
For whatever reason, I typically use 121,
but you're allowed to use whatever random seed you want.
How many times will we
rerun the algorithm or how many initial guesses,
n_init, how many initial guesses will we use?
I like to use 25.
This does not mean how many clusters to try.
It says, how many times will you try three clusters?
Twenty-five times.
I want to run the algorithm 25 times.
Then for each initial guess,
how many iterations will you use at most?
I like to use 500,
meaning if the algorithm has not
converged after 500 iterations,
it just stops and it gives us the result.
Notice, I'm not setting something
small like five or even 10.
I'm setting a rather large number of
max iterations to help make sure it is stabilizing.
It is reaching convergence on its own.
Again, the four major assumptions, how many clusters?
Control the randomness because you will
randomly restart it 25 times,
and you will run from
each initial guess a maximum of 500 iterations.
Again, why do you need these four assumptions?
The previous video discusses it.
The presentation slides available
on Canvas go into it in detail,
and the supplemental readings
can give you further insight as to why.
Now, running that line,
it really doesn't "do anything".
It just initializes the object.
We have not provided any data when we initialize.
We give the data when we fit
the object using the assumptions.
Here I will reassign km3,
where km3 is fit using X.
Remember, this is that NumPy two-dimensional array,
as many columns as variables,
as many rows as data points.
Now, on Windows, unfortunately,
you're going to get this super annoying warning message.
We can remove it.
Doing a few environment variable actions
or suppress the warnings,
but I will not do that in my examples.
I am okay with leaving the warning.
It's very annoying, but that's just the way it is.
Here, I have fit the object.
For KMeans, fitting means it
identified the clusters associated with each data point.
To return the cluster assignments or cluster labels,
we need to predict the data.