Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/bT5kD/simplifying-data-visualization-and-clustering-with-wine-and-sonar-datasets

English
There are some variables that have a lot of correlation.
Other ones have a little lower,
and then we have some moderate anti-correlation.
The goal of PCA is can we exploit those relationships?
Let's find out.
But before we execute PCA,
we need to check the magnitude and scales.
Use the wide format plotting options.
Wine_data kind = box.
Let's make this figure rather wide.
One of the variables dominates the scales.
We must standardize to remove the scale effect.
Let's extract out the numeric columns,
wine_data, select_dtypes number, copy.
Xwine StandardScalar,
fit and transform, wine_features,
Just to confirm pd.DataFrame,
Xwine were the columns are
the same names as the wine features,
kind = box, aspect 3.5 plt show.
All of the scales and magnitudes have been removed.
Execute PCA and return to newly created variables.
Again, should we try more,
that's something for later.
But for now, pca_wine,
PCA, initialize the assumptions.
I want to fit and then transform Xwine.
Convert this to a dataframe,
pca_wine columns, pco1,
pco2, sns.relplot data pca_wine df,
x = pco1, y = pco2.
Now, here's what's really neat.
We've gone from having 13 by 13 pairs of
scatter plots to really two variables,
who scatter plot looks like
an arrow head or a Greek letter Lambda,
an upside down V,
a triangle, whatever you want to call it.
Let's now add in the known groupings,
pca_wine df, cultivar = wine_data.cultivar,
sns.relplot data = pca_wine_df,
x = pco1,
y = pco2, hue = cultivar.
We are now seeing how
the three cultures are basically separated.
There's a grouping of
positive values on pc1,
there's a grouping of negative values on pc1,
and then there's a grouping of
mostly positive values of pc2.
The blue positive values of pc1,
the green negative values of pc1,
and the orange, mostly positive values of pc2.
Run KMeans with three clusters,
and visualize the three cluster labels
and with the newly created PCA results.
Clusters_3 KMeans nclusters 3,
random state 121, and init
25 max_iter 500 fit_predict, Xwine.
Pca_wine_df k3, pdSeries,
clusters_3, index, pca_wine_df index,
force it to be a category.
Create the scatter plot.
Pco1, pco2, hue is k3.
What do we see? Groupings where
we have a group of positive pc1,
we have a group of negative pc1,
and we have a group of mostly positive pc2.
Again, instead of having to visualize the results in
the original data space with 13 by 13 scatter plots,
we can look at everything with
just two newly created variables.
Our very last example,
a really big example.
On Canvas, we have the Sonar dataset.
Copy the link.
Use the Sonar data.
Paste it into the cell and now read
in the data sonar_url.
But as specified in the motivation video,
there's no header. 61 columns.
As discussed in the motivation video,
the column names are all just numbers,
so let's convert those names,
convert the column names to strings to be easier,
sonar_df.columns, a list comprehension X,
placeholder 02d, where the placeholder
d for d in sonar_df.columns.
Now, everything starts with an x,
which is easier to deal with.
Sonar_df.nunique, the last column has
two unique values M and R,
it's an object, it's categorical.
This data set has no missings,
so we don't have to worry about anything.
We can even check with max zero missings.