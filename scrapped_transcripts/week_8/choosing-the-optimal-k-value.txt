Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/WqSq3/choosing-the-optimal-k-value

English
But now, what if we did not know
the number of clusters you tried?
I always recommend starting out with two clusters.
You should do this just to see what happens.
I always recommend visualizing this.
The steps are identical to what we just did.
I'm going to assign the result to the clusters_2 object,
n_clusters 2, random_state, set the seed.
Specify the number of random restarts,
specify the number of iterations per restart,
fit and predict the data set.
Let's assign the result to the K2 column.
Doing so within a PD series, clusters_two.
Index is the index of the copy DataFrame.
You know what?
Let's just force that column to be a category.
It has two unique values and here are the counts.
Let's visualize the cluster results as a scatter plot.
Why is it always useful to start out with just two?
We're asking, what is
the most obvious way to separate the groups?
One option could have been here.
That would have created
a very small amount of variation to the left of the line,
but we have a fairly large amount
of variation to the right of that line.
Instead, a better approach is to
allow the left of
the line to have a little more variation,
but doing so really
reduces the variation to the right of the line.
The first way to break up the data
is really through this diagonal dividing line.
But now, how do we find the optimal number of clusters?
Well, it's just like what we
did with one variable example.
You do not need to manually try out
many different number of clusters.
Try two clusters, then try
one more representative number
based on your exploration
or based on some known grouping.
Then use the knee bend plot to
figure out a more or a better number of clusters.
The code I'm about to use is literally the
same as what we
did in the previous video, except for one thing.
I have 300 total data points.
Therefore, I will not set the max number of clusters to
be 300 because that's an enormous number of clusters.
We know that's not interesting because that's
just saying every data point is in its own cluster.
Instead, pick a moderate to large number of clusters.
I will typically go up to 30, maybe 50.
But I usually go up to 30.
The goal of clustering is to hopefully find
some relatively small number,
two to maybe 10 clusters.
That's what you're hoping for.
Will it always happen?
No. But that's what
you want to focus your exploration on.
Now we need a four loop for k in K,
km, initialize the KMeans,
for n_clusters as k,
random_state, set the seed,
set the number of initial guesses,
set the max number of iterations per initial guess.
Km, you will fit by applying the data,
then append where you pull out the inertia attribute,
the total within sum of squares.
While this runs,
as a reminder, we loaded in the data set.
In our case, we could visualize the known groupings.
Then we executed KMeans,
where we initialize, then fit and predict,
where we specify our assumptions.
I started out using
five clusters because of the known groupings,
and I compared how well the cluster labels
compared with the known groupings.
If we weren't sure what to do,
we should always begin with
two clusters just to see what happens.
Then we go through and do the search.
Now we can visualize the knee bend plot as a line chart.
Fig, ax, plt.subplots,
ax.plot, K, tots_within.
Using a blue line,
where I connect the markers, ax.set_xlabel,
number of clusters, ax.set_ylabel,
total within some of squares, plt.show.
I have a typo.
Set_ ylabel. There we go.
Again, if we have one and only one cluster,
you're basically just getting the variation of
the entire data set. That's not interesting.
If we have many clusters,
we have fewer data points per cluster.
That's not interesting.
We're looking for where there's a point of
diminishing returns where it's no
longer really worth it adding more clusters.
At five clusters, there's literally nothing new added.
We keep reducing the total within sum of squares,
but then at five clusters,
it literally just goes flat.
According to the knee bend plot,
we would want to use five clusters.