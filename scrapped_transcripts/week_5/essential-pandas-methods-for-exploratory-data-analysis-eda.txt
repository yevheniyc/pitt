Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/Z3jve/essential-pandas-methods-for-exploratory-data-analysis-eda

English
Hello everyone. In this video
we're going to do a quick review
of critical methods for
exploratory data analysis in Pandas.
As you can see, I already have
my Jupyter session opened up.
I'm in the same directory
we've been working in this week.
If you don't have Jupyter open,
please pause the video,
launch it, and then come back.
Once you have done that, let's
start a new Jupyter Notebook.
Going to name this one Week 05 essential EDA methods.
Then put in the header information CMPINF 2100 Week
05 Exploratory Data Analysis
or EDA Pandas methods.
Let me put in a few state lines here.
We have used many different Pandas attributes
and methods over the last two weeks.
Let's review the most essential ones
that you will use when you begin exploring data.
I really want to stress that this is
how you start or begin EDA.
These are the kinds of actions you should
always do regardless of the application.
As with the other examples,
we have to import numpy as np and pandas as pd.
Now let's read in the data.
Let's continue to work with
the joint dataset that we created previously.
Df gets assigned pd.read_csv, joined_data.csv.
Now this is a small dataset.
Yes, we can just display the object.
We can literally examine visually
each column and each row to find out what's here,
but we cannot look at a dataset that has
thousands to hundreds of thousands
or even millions of rows.
We can't. We cannot look at
a dataset that has dozens to hundreds of columns.
We just cannot go through
every single thing you have worked with
this little example now
this week to introduce all of the methods,
but we can't rely on
this visual inspection visually checking.
We've been able to confirm the methods are working,
but we now need to know what are
the basic actions that
we should perform or any data analysis task.
This is how you begin exploratory data analysis, or EDA.
EDA always needs to start the same way.
You begin with the most basic and fundamental information
about your data frame.
It's shape, how many rows and how many columns.
Then what are the names of the columns?
Then what are the data types of the columns.
You can use the info method
to get a summarized view of this information.
I particularly like info because it
tells you the rows and columns and then it
gives you a short summary
of how many columns per data type.
I really like this aspect.
You can get that same kind of information by
applying my favorite Pandas method to dtypes.
Dtypes is a series.
We can apply the value counts
method to the D types attribute.
Object, there are four,
float there are three,
integer there is one.
This is really useful if you find
yourself in an application with dozens and dozens
of columns where you don't want to just
print out or display the info just to
see this summary so you can use this approach to get
a simple table view
of the number of columns per data type.
Now Info also gives us
another really critical piece of information.
It tells us the non-null count per column.
Non-null means the number of non-missing.
This is really important but I don't
like that View that it organizes this as non-missing.
I wish Info gave the number of missing.
Now, why do I say that?
Well, I have to know how many rows there are in
the dataset to know
how many columns do not have missings.
I would rather to see no missing displayed as zero.
We know methods that will help us though,
to achieve that goal.
We know that if we apply the.isna method,
all cells, all values are converted to Booleans.
A false means the value is not missing,
whereas a true means the value is missing.
Then we just need to apply
the sum method in order to
get the number of missings per column.
Now we can easily see the F and
the H columns are the only ones without any missings.
You can now use this series to find
out which or display which specific column has
exactly zero missings because
this is a series and we can subset it by
providing the conditional test
all columns where the number of
missings is less than one or zero.
We can also display all columns
where the number of missings is greater than zero.
They have at least one missing.
This is useful if you, again,
have a very large number of
columns and you really just want to highlight
maybe there's only a handful that have
zero missings or maybe
there's only a handful that have missings.
You can highlight those.
Once you know the number of missings per column,
the next thing is to know the number
of unique values per column.
The number of unique values is
provided by the nunique method.
Some columns may have a large number of unique values,
and again, this is relative to the number of rows.
Others may have a small number of unique values.
One thing that is important to note though is that
the nunique method does not
treat missings as a value.
By default, nunique is actually dropping the NA.
There's an argument drop NA,
which is a little confusing because other
methods like mean call this skip NA.
I don't know why pandas switches
to the drop NA terminology,
but drop NA by default is true.
You can change that.
You can force the drop NA to be false;
meaning if you switch drop NA to false,
then the missing is counted as a value.
Remember Column F did not have any missings,
and so its number of unique values is the
same regardless of how we handle missings.
But other columns like E,
which has missing values,
there is one more value
in the count when we say drop NA is false.
Does this really make a big, major difference?
I think it is useful to examine for
variable or for columns with one or two unique values.
If you use the default nunique method and you
see there are some columns with one or two unique values,
it's important to then consider
what happens if you do not drop the NA.