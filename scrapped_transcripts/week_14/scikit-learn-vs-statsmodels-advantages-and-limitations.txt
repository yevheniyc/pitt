Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/2kB4e/scikit-learn-vs-statsmodels-advantages-and-limitations

English
Why use sklearn?
Now, this seems like, what's the purpose of using this thing?
Especially since, after all, all I did was try and convert it to match stats models.
First, some downsides to sklearn.
As already mentioned, sklearn
does not provide standard errors,
it only provides coefficient estimates.
We therefore, cannot identify
statistically significant coefficients,
and thus features, we cannot identify
the confidence interval on trends from sklearn.
The upsides to sklearn.
Sklearn is focused on calculating performance metrics,
that's what sklearn is best suited for.
It has a dedicated method, .score, for scoring,
or grading, or measuring the performance.
It can literally save you a lot of steps compared to statsmodels.
So, let's calculate the training
set accuracy for statsmodels.
Let's make df_copy or
df_copy gets assigned df.copy(),
we need to predict the event probability,
fit_stats_formula.predict( df).
We need to then force, we need to then categorize or
classify np.where( df_copy.pred_probability
greater than the threshold, let's use the default.
We can now calculate the accuracy.
The .score method for LogisticRegression,
Calculates the accuracy by default,
it assumes a default threshold of 50%.
So to get these training set accuracy,
all I have to do, skmin_loss_score.
Give it the feature array, Xdesign_mat,
give it the output array, yobs_mat.revel () ).
These two numbers are identical.
So this one function, this one method call
encapsulates really, three steps in one line,
as long as we have the feature and output arrays.
This is really the benefit of scikit learn,
you are focused on measuring performance as quickly as possible.
Making predictions with sklearn,
the .predict() method returns the classification,
assuming a 50% threshold.
Okay?
The .predict() method requires the feature array,
it does not work with the data frame.
So let's go ahead and demonstrate this, I'm going to create
a new column in my dfvlz data frame, pred_class_sklearn_none.
This is coming from scikit learn with a none penalty,
sk_min_loss predict( Xgrid).
Remember Xgrid, let me put this in here, Xgrid,
we created it previously from the matrix.
If we check pred_class_sklearn_none.value_counts,
there are 2 and only 2 values, 0 for the non-event, 1 for the event.
And this is the same as our classifications from statsmodels,
by the way, it's just labeled a little different.
To get the probability of the event,
we need to use the .predict_proba() method.
However, this method does not return the event probability,
it's instead, as you'll see,
going to return a matrix or a 2D array.
Let's assign it to an object, pred_grid_sk
= sk_min_loss.predict_proba( Xgrid)
pred_grid_sk.shape, do you see the two columns?
One column corresponds to the probability of the 0th class,
the other column corresponds to the probability of the one class.
The class orderings are provided
by the .classes_attribute,
sk_min_loss.classes.
0 class is the 0th column, the 1 class is the 1 column.
We need to provide a conditional test to find
out which column is the event probability.
Okay, so pred_grid_sk[ :5,
sk_min_loss_classes_== 1].
We can use this because the output is the binary outcome, is 0 or 1.
If it was a string like yes or no, you would have to use the appropriate value.
Also, if you check,
.classes == 1].shape.
This is a 2D array, so you need to
convert it into a 1D array with .revel.
You can now add in the pred_probability_sklearn_none]
pd.Series, where the index comes from dfviz.index),
pred_grid_sk, all rows for
sk_min_loss.classes == 1].ravel, oops.
_classes_dfviz, meaning we can now finally,
visualize the predicted
probability from scikit learn,
which we have converted to act just
like statsmodels, okay?
So, again, the primary reason for showing you this video was
to introduce how to fit models using scikit learn's logistic
regression to match statsmodels, that's really what this was about.
We need arrays for the features and the output, and we must set penalty as none,
solvers, lbfgs, and fit intercept is false, that's the easiest way to do it.