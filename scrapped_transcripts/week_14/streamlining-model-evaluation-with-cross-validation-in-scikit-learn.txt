Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/gN68R/streamlining-model-evaluation-with-cross-validation-in-scikit-learn

English
Hello everyone.
As you can see, I already have Jupyter opened to this week for the class.
This video is going to be a little different,
because rather than creating a brand new notebook,
we will launch a previous notebook, save it to a new name, and then modify it.
The reason why I want to do that is I want to demonstrate how
scikit-learn offers one more way to streamline things even further.
So we will launch the week_13_cv_logistic_sklearn
notebook modify it to show how we can very efficiently
evaluate the performance with cross validation.
So if you don't have Jupyter opened up, please pause the recording and do so.
And then once you're back, rather than launching a new notebook,
I want you to click on the the week_13_cv_logistic_sklearn.ipynb file.
This will relaunch the notebook, and
now instead of changing the name up here,
we should go to file save as.
And now we're going to make it a brand new notebook,
going to name it week_13_cv_sklearn_cross_val_score.
So save, okay, once you go to kernel, restart and clear output.
Yep, restart and clear output, and now let's just go through and run the cells.
I'm doing so manually, you don't have to, but
I'm just going to do it manually to revisit everything that we did.
Remember, in this notebook, we defined a function that trained and
tested the logistic regression model.
Across validation, we made use of these psychic learn
model that we initialized to fit so we can easily score it.
However, maybe when we created this function, you thought, gee, I thought this
was supposed to be streamlined, there's still a whole lot of programming here.
After all, we still had to iterate over the splits, and
you're right, there's still a lot of programming.
It was less programming than executing the cross validation with stats models,
but it still was a lot of programming.
After doing all that and then iterating over all the different formulas,
we then were able to see that, yes, after training and
testing five times for each model, we could appropriately identify
which models were overfit versus which models were the best.
Okay, all right, so now we concluded in the previous video making
the model selection and then predicting.
But before doing that, I'm going to insert a new cell and
make a section header even more streamlined with cross_val_score.
The cross_val_score function from
sklearn is a very efficient way for
executing cross-validation.
It manages the training and
scoring on the test set across all folds for you.
So it's going to take care of a vast majority
of the code that we had to make in our train and
test logistic with cv function.
This function, let's import it.
So import cross_val_score from
sklearn.model_selection.
So this is a really important function and that's why I wanted
to give it its own video to demonstrate just how important and
useful it is, cross_val_score, great.
Let's demonstrate its use before putting it inside our own function.
As a scikit learned function,
it needs the design matrix or feature array.
So it needs that, it does not work with a formula.
Let's apply it to the model with linear
additive features as an example.
So formula_list[3], remember, this is the model with linear additive features.
Let's use d matrices.
We'll name the output array y3,
the feature array x3 dmatrices (formula_list[3], data = df ).
x3.shape, it has 7 columns and
we can get a little glimpse here, the first 5 rows.
So it's a little weird, but here's the the intercept column of ones, okay?
All right,
the `cross_val_score()`
function has 4 main arguments.
It needs an initialized sklearn model,
it needs the feature array,
it needs the output array and
it needs a cross-validation scheme.
Okay, so cross_val_score.
Now the initialized model, we created this previously sk_min_loss.
So actually here I'll even just type it out, sk_min_loss.
This is initializing the assumptions of the model,
this is set up to be consistent with stats models.
Then we need the feature array x3,
then we need the output array and convert it to a 1d array.
Then we need the cross validation scheme that's
controlled by the cv argument, I'll call kf.
When you run this, the result is a numpy array and as you can see,
there are 1,2,3,4,5 elements in the array.
There are 5 elements because we are using five fold cv,
the cross_val_score function took our model,
split our entire data set five times,
fit the model within each folds training set,
and test and scored the model on the test set.
In fact, if we look at our manually
calculated test set scores for model 3,
if we look at these cv_results.loc,
we'll use two conditions here.
(cv_results.model_name ==3) and
(cv_results.from_set ==testing).