Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/7CiIz/key-material-for-module-14

English
Hello, everyone. Welcome to
the 13th week of the semester.
This week is dedicated to model selection.
Because over the last few weeks,
you might have been wondering, how do we
actually pick which model to use?
We know how to generate
all these features, but what's the point?
Well, that's what this week is all about.
You will learn how to actually select the best model.
What the last few weeks have really
been leading to is the fact that
you can create a model that behaves really,
really well on the training data
by simply adding a whole lot of features.
Using a whole lot of interactions,
interacting polynomials with other inputs,
interacting polynomials derived from
continuous with categorical inputs.
The more features you make,
the more unknown coefficients that must be estimated,
the better the model behaves on the training data.
But we have yet to address if we can trust that model.
We have yet to address,
will this model do well on previously unseen new data?
This week, we'll show you how
cross-validation is a procedure
for helping you identify the best model.
To do that, you need to learn about overfitting.
You need to learn how
overfit models memorize the training data,
causing them to do really poorly on new data.
You will learn how cross-validation
is attempting to approximate new data
by randomly splitting the entire dataset
into dedicated training and test sets,
and replicating that process multiple times.
We, therefore, have to use a lot of
the same concepts from
way back at the beginning of the semester,
where we were randomly generating
replications and then averaging.
You will learn how to apply
cross-validation to stats models.
What you will see is this is a lot of work.
There is a lot of programming that you have to do.
There's a lot of little minute details
that are required of
you to appropriately cross-validate
a stats models object.
The reason is because stats models is focused on
statistical significance
and standard errors in confidence intervals.
You will then learn how you could much more efficiently
apply cross-validation to a scikit-learn trained model.
But in order to do that,
you need to learn about a different interface.
Because scikit-learn doesn't work
with a formula interface,
it works with arrays.
You will also see how
scikit-learn logistic regression differs
from stats models logistic regression,
and you learn how to force
scikit-learn to act or behave like stats models.
But once you have learned how to do that,
you will ultimately see how everything that we have to
do to cross-validate a stats model object,
which is several dozen lines of
code actually gets converted
into one line of code in scikit-learn.
This is a very important week.
It really sets up how you can
make use of these models in practice.
It builds on everything from the last couple of weeks.
Please take your time in the examples.
I tried to point out where, I think,
you really have to program with me
versus you can watch and learn the concepts.
The real major importance are how you actually
cross-validate these models and how
you identify which model is the best.