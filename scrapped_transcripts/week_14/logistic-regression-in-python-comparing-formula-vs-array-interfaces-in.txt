Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/5d7G1/logistic-regression-in-python-comparing-formula-vs-array-interfaces-in

English
So the formula versus array interfaces.
I personally never use the statsmodels array interface.
The formula interface is faster to work with.
You do not need to create
the array objects for training or for making predictions.
Plus, you get to use the nice names for everything.
The regression coefficients are automatically named
consistent with the features they
multiply when you use the formula interface.
So I personally think
there's no reason to use
the array interface with statsmodels.
However, we introduced the array interface
here because scikit-learn does not work with formulas.
Scikit-learn has its own set
of predictive model functions.
Scikit-learn can fit linear models for
regression and
logistic regression models for classification,
but you must provide the output
and feature arrays to scikit-learn.
That's the real reason why
the array interface is being
shown here is to support using scikit-learn.
So let's now work with scikit-learn directly.
The logistic regression function
from scikit-learn is imported below.
So we need to import the appropriate function,
from sklearn.linear_model import logistic regression.
If you were working with a linear model for regression,
you would instead import from
sklearn.linear model import linear regression.
So linear regression for the linear model,
logistic regression for binary classification.
So just to show you,
but in this recording,
we're not working with linear regression.
Scikit-learn predictive model
or predictive model functions,
follow the same workflow as the pre-processing functions,
we worked with earlier in the semester.
So you remember back when we were
standardizing and then working
with K-means and hierarchical clustering,
and remember when we were doing PCA?
Well, predictive models like
logistic regression follow the same workflow.
The object must be
initialized and then the object is fit.
Once the model is fit,
then you can predict or measure performance.
So then you could basically do
whatever you want with that model.
However, we have to show
several important aspects or
qualities of working with
scikit-learn and logistic regression.
We will first see the default and conventional usage.
Remember, when the scikit-learn object is initialized,
this is where we set the assumptions of the model.
Let's first use the default assumptions.
We will first use the default assumptions.
We will also use
the conventional or typical usage for the features.
The conventional approach when
fitting predictive models with
scikit-learn is to remove
the bias or intercept column of ones.
So do you remember that the zero-th column of all ones?
This is what statsmodels wants you to use.
But scikit-learn typically does not want you to.
Or that's the way most people describe it as.
We can remove the intercept or bias column by
including a -1 in the formula definition.
So instead of typing y tilde x,
we will type y tilde x-1.
That means, remove the bias column.
So let's use dmatrices.
So use dmatrices to create
the arrays for the default
and conventional sklearn formulation.
I'll call it y_sk.default,
X_sk.default dmatrices,
my formula y tilde x-1 one data = df.
Now my design matrix,
my feature array only has one column.
That intercept column of ones,
the bias column has been removed.
We only have the features derived from the input.
That said, the output is unchanged.
The output is still a 2D array.
We will therefore need to apply dot ravel to
convert it to a 1D array.