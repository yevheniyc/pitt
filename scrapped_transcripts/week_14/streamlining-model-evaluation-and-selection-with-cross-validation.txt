Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/rwOML/streamlining-model-evaluation-and-selection-with-cross-validation

English
The first element 56%, second is 70%.
The third is 63,
then 71.67, then 60%.
As you can see here,
cross_val_score is literally doing
everything that we had previously done.
However, it's only returning the test set because again,
we actually don't care about training set performance.
Let's now wrap or contain
cross_val_score within a function
that lets us easily apply any formula.
Again, as a reminder,
this is what we had.
Let's now make a function that is
literally just a few lines of code to show you
just how streamlined this is
, def, logistic_cross_val_score.
It will have the following arguments.
The name of the model,
a_formula, the initialized model.
It needs the data,
and lastly, the cross-validation scheme.
We need to create
the feature and output arrays using the formula,
just as we did originally, y,
X = dmatrices,
a_formula, data = data_df.
This was done previously.
But now, train and test within
each fold and return the test set scores.
Test_result, cross_val_score, init_mod,
feature array, the output array,
the cross-validation scheme, and that's it.
This one line of code is doing
everything that we did by initializing the object,
running the for loop to iterate over the folds,
first split the data to training and testing,
fit the model on the training set,
score the model on the test set.
That's it. One line of code,
and then we need some bookkeeping.
Let's put everything into a DataFrame.
We have the accuracy stored in the test_result.
We have the model_name, stored by mod_name.
We have the model_formula, stored by a_formula.
But now to know the number of coefficients,
we don't actually have any fitted model.
This is the initialized model,
but nothing here is fit.
But we do have the feature array.
The number of columns of
the feature array equals the number of coefficients.
We can get the number of coefficients
by just accessing the number of columns of that project.
Then we return the result.
Let's test it out, logistic_cross_val_score model 3,
formula_list 3, init_mod is sk_min_loss,
so the scikit-learn model
that is consistent with stats models,
the training dataset and our
cross-validation for the entire dataset
and our cross-validation scheme.
We now have the number
or the accuracy in each test set per
fold and we have the number of coefficients.
Actually, I forgot. We can add in the fold _id.
Take the index plus one.
There we go. Now we know the fold.
This is intrinsically just the test score.
Let's now iterate over all models.
We'll initialize the results,
cv for cross-validation score list.
For m in range of the length of the formula_list,
let's do little print statement,
formula ID, placeholder d. Placeholder is
the m iterating variable and now cv_score_list.append,
logistic_cross_val_score m, formula_list m,
init_mod, sk-learn,
the model that's consistent with stats models.
Data_df is df, cv = kf.
Again, we will get some warnings, but everything works.
Let's combine all of the results into a DataFrame,
pd.concat, cv_score_list, ignore_index, true.
Visualize the average test
set performance for each model,
sns.catplot, data = cv_score_df,
x = model_name,
y = accuracy,
kind = point, join = false.
As discussed previously, by convention,
we want the plus or -1 standard error interval
or 68% confidence interval.
This figure corresponds to
the testing set results
that we created in the previous video.
By this procedure, we are
giving up looking at the training set results,
and honestly, that's okay.
Because we have tried out so many models,
some of them very, very simple,
simple, constant event probability,
some of them of middle complexity
and some that are very complex.
All we need to do is to
compare the average performance on
the test sets in order for us to
identify the best model on new data.
When you're working on your projects,
this is really all you need to do.
You can use cross_val_score to manage it.
You can use dmatrices with a formula
to create all of your features,
and cross_val_score will calculate
the performance metric for you.
In this case, because it's
classification, we get the accuracy.
The rest is all of the same.
We identify the best model,
the one has the best performance on average.
We fit that model on the entire dataset.
Then we could do everything else that
we want just as we talked about the end of
the last video. That's it.
That's using cross-validation to
identify a model that does well on new data,
not being fooled by
a model that memorizes the training data.
Essentially, what you're doing here is,
this is the Goldilocks problem.
The bed is too soft.
The bed is too hard. The bed is just right.
The porridge is too cold,
the porridge is too hot,
the porridge is just right.
You're trying to find the model that
is just complex enough to capture what's
going on without being
fooled by everything else that's in the data.