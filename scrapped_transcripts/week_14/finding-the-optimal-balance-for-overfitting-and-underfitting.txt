Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/ss00h/finding-the-optimal-balance-for-overfitting-and-underfitting

English
But I will first overlay
the average with the fold result.
I'm going to make a figure.
You don't have to make this one.
I want to include it,
though to try and give
more context for what
we are ultimately interested in looking at.
This figure is known as a strip plot,
data = cv_results,
x= model_name, y= accuracy, hue= from_set.
Then I'm going to change the transparency, ax=ax.
I'm using access level functions here
to overlay to create composite graphics.
Composite graphics is discussed in
2130, the visualization class.
That's why you don't have to make
this one join false ax=ax.
I'm combining the strip plot with the point plot.
I wanted to show that to you to
illustrate the middle point,
if you will, of the individual
across all the fold results.
For Model 15, its average in the training set
is high because all
of the individual fold results are high.
But the average in the test set is
lower because each of
the individual fold test set results are lower.
That's why I wanted to overlay these two.
But again, let's just focus on the average.
Summarize the cross-validation results
using a point plot,
sns.catplot data cv_results,
x= model_name, y= accuracy,
kind= point, join= false.
Oh and I forgot to add in
hue= from_set, plt.show.
This is telling us for very simple models,
which the unknown constant or
intercept only model is the simplest.
I can't see the difference
between the training and testing results.
Very simple models do as well on the test on new data,
pretty much as well as they do on the training set.
However, look at the very complex models,
the far right hand side.
These models have the most number of coefficients.
In fact, if we pull up the results from last week,
Model 15 has 69 coefficients,
Model 14 has 46 coefficients.
Those two models have clear separation
between the training set and test set performances.
The test set, the new data has
worse performance compared to the training set.
A model that has
substantially worse performance on
testing than training is known as overfit.
An overfit model is a model
that is memorizing the training data.
It's good at replicating what it saw,
but it's really bad
at understanding what's actually going on.
The way I like to joke about it or to talk about it
is that an overfit model memorizes the data.
It's like cramming for a test the night before the exam.
If it sees a test question,
it has never memorized.
That model doesn't know what to do.
It doesn't generally understand anything,
so its performance is really bad.
The overfit model that crams for the exam can only tell
you exactly what it
memorized because it doesn't generally understand.
It will therefore appear to do well,
but that's only on the data it's memorizing.
Another way to view it.
This overfit model got really good at
classifying if a stock will
crash before the housing bubble.
It was really good in 2007.
But then you used that model in 2008,
and you lost a lot of money because all it was
doing was memorizing what it had observed.
The point of model selection is therefore
not to find a model that does well on the training data.
The point of model selection is to find a model that does
well on new data not used in training.
Model 6, although on the training set,
it does worse than multiple models.
Model 6 does better on average on new data
than all of these other models that
appeared to do well when they were allowed to memorize.
More frequently, though, and in practice,
the above figure only shows the test set performance,
so we don't even include the training set.
More frequently, what you will create is
a point plot just on the testing set.
Here I'm filtering just
to the rows we're from set is testing.
I'm saying kind= point,
joint= false, plt.show.
We don't even consider
the training set performance
because it ultimately doesn't matter.
We don't care about it.
If you include it, this just really confirms which models
overfit versus which models underfit.
Very simple models underfit data.
They can't explain anything,
so they do just as well in training as in testing.
Overfit models memorize.
They can only explain the training set.
They can't do well on new data.
We want to find the happy middle ground,
a model who is simpler that does
better on new data than any other model.
Yet it's complex enough
to find patterns and explain things.
But now, by default,
the point plot shows the 95% confidence interval,
and this is really
useful when you're studying averages across groups.
But when comparing models,
the convention is to show
the 68% confidence interval because that
corresponds to the one standard error interval.
The 95% confidence interval
is the two standard error interval.
To change it, you have to set
the error bar argument to
a couple where the zeroth element is ci,
and the second element is 68.
These are smaller error bars.
We're comparing models,
we're not focused on the 95% confidence interval,
we're focused on the 68% confidence interval.
Again, we're looking for
which model has the highest average.
But the standard error interval is telling us
which models have effectively on average,
the same result, or they are within the margin of error.
When models are within the margin of error of each other,
choose the simpler model.
The model with the fewest
unknown regression coefficients,
that's known as the one standard error rule.
Choose the simplest best model.