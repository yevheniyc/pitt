Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/0kpXI/model-selection-and-interpretation-from-cross-validation-to-final-fitting-in

English
Again, to highlight everything,
Model 15 was the best on the training set.
But look, on average on the new data,
it does worse than an intercept-only model.
Model 6, it did worse than Model 15 on the training set,
but it does the best on average on new data.
There are multiple models that are within
the margin of error of the performance of Model 6,
but we will choose the model
with the fewest coefficients,
and that does correspond to the Model 6.
Let's confirm the performance
based on the number of coefficients because after all,
we actually have that here, we
stored that num coefficients
; sns.catplot(data = cv_results).
Let's actually focus just on
the test set again from_set equals testing.
We're filtering, x equals num coefficients now,
y equals accuracy, kind equals point, join equals false.
I want the color of the hue
to be based on the model name.
Again, the error bar,
ci, 68 plus or minus one standard error interval.
Here's Model 6.
It has nine regression coefficients.
Any of the other models that are within the margin of
error of this best model
has more regression coefficients.
It has more unknowns.
Therefore, they are more complex.
So choose the model that's simpler or the simplest.
This model doesn't have the categorical variable.
It has linear main effects,
and it has quadratic features.
Now, the actual model selection, again,
choose the model that does the
best on average on new data.
Once you've done that, you must now fit
the model one final time using the entire dataset.
Cross-validation is being used to give
us an idea of how reliable the performance is.
This is a fair assessment because all of
these performance metrics are
calculated on new data. It's fair.
But when we ultimately want to use the model,
we fit it on the entire dataset.
I'll name this best_model, smf.logit.
It has a formula.
It has a dataset, and we fit it.
The data comes from df and the formula,
well, it's Model 6.
We can identify statistical significance
by comparing the p-values of 0.05.
We can identify the ranking,
which features are more important by effectively
examining their absolute value.
You take the best model,
you take the parameters,
take the absolute value,
and you sort the values in descending order.
But you were really only interested in
those that are statistically significant.
Of the statistically significant features,
x2 and x1,
those are the only inputs
that have features that are statistically significant.
This tells us x2 is linear effect,
its regression coefficient is close to one in magnitude,
then followed by the quadratic feature for x1,
then followed by the quadratic for x2,
then the linear main effect for x1.
You can use the above approach to identify
the ranking of the features
when the inputs are standardized.
This data set I have already done that for you,
that's why we could do it here.
Then the last thing that we want to do visualize
the predictive trends for the inputs.
Last week, we examined the trends with respect to x1.
Let's do the same thing here.
Even though we saw that x2 is
the most important or is the highest ranking,
let's stick with what we did last week.
Last week, remember, we
concluded by visualizing the trends,
and last week it was a little difficult for
us to identify the most important variable.
But now, it's not so
bad because we have identified the model,
it does the best on new data,
and we have studied the statistically
significant features for that model.
But just for simplicity,
let's go ahead and use the same input grid.
Just to speed things up,
I'm going to go back to last week
and just literally copy this.
I'm literally taking this from the example
from last week, pasting it in.
There we go in this grid and unique.
You can see x1 is 101 values,
x2 is nine, and I've included in all three for x5.
Let's now make a deep copy of that,
name it dfviz, input_grid.copy,
dfviz pred_probability, best_model.predict, input_grid.
We can now visualize the predicted probability of
the event with respect to the primary input,
which in this case is x1
since it has the most unique values.
Y is pred_probability. Hue,
I'm going to color by
the categorical x5 and use the facets based on x2.
It's a line. The estimator is none.
The units within each facet will be defined by
x5 and I want to wrap have three facets per row.
Now, the first thing that you should see
is we only see one color in each facet,
meaning x5,
the categorical input, isn't doing anything.
Well, that's because this model
doesn't have the categorical input.
Our model selection procedure
told us x5 doesn't seem to matter.
The relationships of the probability with respect to x1,
remember how we saw x1,
its quadratic feature is
statistically significant and important.
You see the parabolas.
The probability increases, then
decreases within each facet.
We also saw x2 is important,
including its quadratic feature.
Do you see how the size of
that parabola is increasing as x2 increases,
but then it goes down?
It increases then decreases.
That's the quadratic effect with respect to x2.
We now have therefore
tried out all of these different models.
We have used cross validation to
select the best model that we expect on new data.
We've identified the inputs
that are statistically significant.
We've tried to rank which features
are important based on that,
and we've studied the trends to see they are
consistent with our interpretations on a new dataset.
We therefore have a complete
predictive modeling application.
I know this video covered a lot.
The main takeaways are the steps
used to execute the cross-validation within each fold.
You take the training set, you fit the model.
You then predict and calculate
the performance on the training set and test set.
Really, the test set's what matters.
It also illustrated how to use try and except to
manage when you have errors because you
might have errors in real problems,
like in your final projects.
Then you summarize the performance
across the folds for each model to
identify the model that does the
best on average on new data.
Once you've done that, you fit it one
final time on the entire data set.
You interpret it just as we talked about previously.