Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/qEsLh/adjusting-penalties-and-matching-coefficient-estimates

English
So to force scikit-learn logistic regression to be the same as statsmodels,
you must turn off the penalty to match statsmodels.
Please note, you do not need to do
this with the LinearRegression function in sklearn.
This is something specific to LogisticRegression in scikit-learn.
So my main joke is anytime you read a book and they talk about scikit-learn,
they'll say how everything is consistent.
It has this really great interface.
No, that is completely and totally false.
LinearRegression has completely different sets of assumptions and
style compared to the LogisticRegression assumption and style.
Because LogisticRegression has a penalty, LinearRegression does not.
Okay, so again, next week, you will learn why that is.
For now, let's just see how to turn it on.
In the fitted scikit-learn object, there's an attribute penalty.
This tells you the penalty it's using.
We want to force the penalty to be none.
A none penalty means the coefficients
are estimated in order to minimize a loss.
Think of it as finding the coefficients that minimize the error.
Okay, right now, it's not actually doing that.
That's why I feel scikit-learn
is very dangerous when you use
the defaults because it is easy
to forget what the defaults are.
I always recommend specifying the penalty argument and
the solver argument so you know exactly
how the coefficients were estimated.
So I really, really harp on this.
I feel this is really important because it's very
easy to forget what the assumptions are, okay?
We will therefore force scikit-learn to
minimize the loss just like statsmodels.
So this is what statsmodels does.
And we will have the intercept estimated
as if it's a regular coefficient.
So we are literally going to have everything organized just like
statsmodels.
I'm going to assign it to an object, sk_min_loss = LogisticRegression.
The arguments, you must specify, the penalty,
set it to None, solver, set it to lbfgs.
Okay, I just want you to type that in,
lbfgs, then fit_intercept=False,
.fit(xdesign_mat, yobs_mat.revel).
If you check the penalty attribute,
nothing is shown because the penalty is none.
If you check the coefficient attribute,
we have two values because we forced the intercept to be estimated.
And now, let's compare the estimated coefficients with statsmodels.
fit_compare, I'll name this one sklearn_none,
pd.Series, where the index,
Is the index for fit_compare.index.
The value is sk_min_loss.coef_.revel.
When we use the default sklearn, look at the value of the slope, 1.10.
But now, when the penalty was forced to be none,
the slope, 1.18, statsmodels, 1.18381.
When we set penalty to none, scikit-learn, 1.18381.
Look at the intercept, -0.44951,
statsmodels, -0.44951.
The estimates are not perfectly identical, but
they match down to four to five decimal points.
We have therefore converted sklearn to behave very,
very similar to statsmodels, okay?
Again, next week, we will discuss what this penalty is doing,
because it really does want its own discussion.