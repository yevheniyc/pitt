Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/CrxzU/understanding-folds-and-data-partitioning-step-by-step

English
Again, notice, five lines are
printed because we're using five folds, five splits.
We still have the 80% for
training and the 20% for testing.
Let's now execute the cross-validation.
I want you to start out following along,
and then I will tell you when you can stop.
We will execute the
cross-validation and store the splits.
This way, we can visualize which
data points are used in the training splits,
and which data points are used in the testing splits.
You will typically not see this done,
but it will help you
learn what cross-validation is doing.
Let's begin by initializing
lists that will store the splits.
Train list is a blank list,
an empty list, test list is an empty list.
We will now call the split method for
the stratified cross-validation procedure
within a for loop.
Let's now apply the splits to subset the rows
and thus identify the train and test splits.
For train_id, test_id in
kf.split( input_df.to_numpy), df.y.to_numpy.
Again, this is a for loop.
Now, I'm not going to print something.
Instead, I will append to my training list,
and append to my testing list.
Now, what am I going to append?
train_id is an array of row indices.
Therefore, I will take
the entire dataset and use the.iloc
attribute to identify the rows
that will be used for training.
Let's do a.copy.
Likewise, in the testing set,
I'm taking the entire dataset and using the.iloc
attribute to identify the rows selected for testing.
However, before I run this,
I'm going to first to reset the index.
That way, I create a new column that corresponds
to the row index in the dataset.
Notice df.reset_index,
the index attribute becomes a regular column
,.iloc identify the rows
based on their index values, their index positions.
Give me all columns,
force the deep copy.
I want you to run this.
If we check the length of our training list,
it is length 5.
Likewise, the length of our test list is length 5.
We split the data five times.
Therefore, we have five sets of
training and testing datasets.
Let's confirm the number of rows,
and we'll do this with a list comprehension.
This is also to introduce
a new very important term for you.
Every time we split,
that's called a fold.
The fold contains a certain number of rows for training,
and the fold contains
a certain number of rows for testing.
If you look in the list comprehension,
in each fold, how many rows are there for training?
Two hundred and forty.
240/300 is 80%. In each fold,
how many rows are used for testing?
Sixty. 60/300 is 20%.
Each element in the list is a DataFrame.
See, here's the row index,
same thing for the test set.
Since this one prints out a whole lot of stuff,
I'll just use the head. There we go.
Let's now add a column for the fold ID,
or the number of times we split the data.
I'll use a for loop for this,
for fold_id in range the length of the train_list,
access one of the elements in the training list,
and now add a new column called
fold_id = fold_id+1 because
the range here is starting at 0.
I want the first fold to be named fold 1.
The zeroth fold, I'm naming it fold 1.
That's the historical convention.
Let's do the same thing for the test set,
range, length, test list.
Test_list the fold_id,
make a new column, fold_id, fold_id+1.
Test_list, hit the 0 and now it says this is fold_id 1.
Lastly, combine everything together,
the train_splits_df = pd.concat(
train_list, ignore_index = True).
Test_splits_df = pd.concat( test_list
, ignore_index = True).