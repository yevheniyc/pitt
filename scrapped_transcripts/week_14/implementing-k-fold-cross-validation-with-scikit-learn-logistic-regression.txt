Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/BeeGB/implementing-k-fold-cross-validation-with-scikit-learn-logistic-regression

English
Scikit-learn though seemingly doesn't crash,
we can now combine all results together.
Cv_results, pd.concat,
results_list, ignore_indexTrue,
We even have the results for the model that has 81 coefficients.
We can check from cap plot.
X equals model name hue equals from set, kind equals count.
Our dodged bar chart shows us each model was trained and tested five times.
The accuracy for each model in each fold
is now shown below via a strip plot.
SNS Capplot data equals CV results X
equals model name y equals accuracy,
q equals from set, this is just like
what we made from stats models.
But now, because model 16 is here, each dot is one fold.
So in one of the folds, Model 16 had a training set accuracy of over 85%.
And there was include and
there was another fold where the training set accuracy was over 90%.
But look at what happened in the test set.
This model, which last week we saw was the overall best on the training set,
never has a holdout set accuracy a test set accuracy above 65%.
Again, this model is very over fit.
There is a substantial difference in training set
performance compared to test set performance.
Model 16 is over fit.
But as already discussed,
we are focused on the average performance.
So let's instead visualize the results as a cap plot or, sorry, as a point plot.
Model name Y equals accuracy,
q equals from set kind equals point, join equals false.
And as mentioned previously,
let's focus on the one standard error interval.
So CI 68 Model sixteen's average performance is so
low, it's worse than the unknown constant average.
Okay, this model is absolutely terrible,
even if it seems like it's really good on the training set.
Again, it is memorized things.
It is over fit, It will not work on new data, not at all.
So I hope you liked seeing this.
This is really bringing together,
everything that we have seen up to this point.
For classification.
We're again identifying model six as the overall best.
So model selection,
Select the best model and
then fit on the entire data set.
So the best model here, model six, is just like the results we had originally.
Let's create its feature arrays.
Formula list six data equals df sk
best mod sk minloss dot fit xbest y best
revelle here are the coefficients and
then we can make predictions just as we did originally.
So here, just to show it,
we need to make the prediction grid.
So I will just copy this to save a little time,
Make predictions on a new visualization grid, paste that in,
Make the feature array for the grid.
So from patsy,
import the matrix this
formula this is a string.
Strings are sequences.
I can tell it give me everything except
element zero that removes the y variable.
The input grid does not have the output variable.
Therefore we need to remove y from the formula
in order to make the visualization grids feature array.
So remove the output from the formula
to make the visualization grid feature array.
I'll name it x grid.
The matrix formula list six one
colon data equals input grid.
This has the same shape as the feature array for
the training set or, sorry, the same number of columns.
We can now predict the visualization grid with the best model.
Let's return the probabilities.
Red grid SK best SK best
predict there [INAUDIBLE]
Are two columns.
We need the classes attribute to tell
us which column is which class.
If I make a copy of the input grid,
I can now assign the predicted probability to a column.
There read grid sk best all rows,
but apply the conditional test to find
the column equal to the event of interest.
And now I can just visualize things just as we did for
our stats models result.
Line estimator none units each line
is defined by x five call rap three.
We have the exact same set of predictions
As we saw with our stats models resolve.
We are therefore able to make use of the functionality of
Scikit-learn to efficiently train all of these models,
identify the best ones, and still visualize their behavior.
Again, the downside of scikit learn no statistical significance.
We need stats models for that.
Scikit-learn lets us easily calculate performances.