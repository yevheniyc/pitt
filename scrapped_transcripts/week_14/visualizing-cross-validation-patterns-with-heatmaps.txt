Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/4Arkd/visualizing-cross-validation-patterns-with-heatmaps

English
Great. Now we have all of
the training rows in every fold.
Likewise, we have all of the testing rows in every fold.
This is all I feel you need to
follow along with if you're programming with me.
The rest I want you to watch and understand the concepts.
If you want to follow along, that's great.
If you want to program with me, that's great.
But what I'm really interested in
right now is to answer what's going
on with cross validation or which rows are
randomly selected in each fold for training.
I'm going to visualize this with a heat map.
I'm going to create the figure.
The seaborne heat map is
an axis level function so I
need to initialize the figure and axis objects.
Here's my fig size,
and I'm making this a rather wide figure, sns.heatmap.
I will create the heat map through
the cross tabulation function pd.crosstab,
train_splits_df.
I'm interested in the cross tabulation
between the fold_id and
the index because remember,
the index is the row ID that was selected.
Provide that to the current axis, plt.show.
Let's see here. I have a typo.
You know what it is? It's probably because
of my index column being named
this way. That's all right.
Let's go ahead and change train_splits_df.rename.
We will rename the following columns index
to rowid inplace true.
There we go. Now train_splits_df, rowid.
Let's now use train_splits_df.rowid.
I still have,
let's see where did I mess up here?
It was just a simple mistake.
My subplot needs to be subplots.
There we go. Now it works.
Awesome. Now, this figure, at first,
it's going to seem a little
odd because you have the color bar on the right,
and it's a gradient, scale by default.
But in the graph itself,
we only see two colors.
We see this dark color and the very bright color.
What this is telling us is when you
see the dark black purple,
there is zero count for
the combination of this fold_id and this rowid.
However, the bright color is a count of one.
What this is therefore telling you is which
fold is a particular row
in the dataset selected for training.
If you look closely here, fold_id 2,
rowid 0 is not selected,
but it is selected in every other fold.
Rowid 1, however,
is not selected in fold_id 4,
but it is selected in every other fold.
Let's confirm this is the case by
focusing on fold_id = 2.
So train_splits_df.loc, we will locate the rows based on
values, train_splits_df.fold_id == 2.
Look at rowid,
1, 2, 3,
4, 6,
rowid zero is not here,
rowid 0 is not
selected for training in fold_id 2.
But rowid 0 is selected for training in every other fold.
Train_splits_df.loc, we're going to use
two conditions so I need to separate them,
and this condition will be an and, not an or,
but train_splits_df.fold_id does not equal 2,
and train_splits_df.rowid == 0.
One, two, three, four rows are returned.
The fold_id is 1, 3, 4, 5.
Again, fold_id 2 is removed.
Actually here, we can even
reinforce this one more time, train_splits.loc,
train_splits.rowid == 0,
only four rows are returned,
fold_id 2 is not here.
The rowid = 0 is held
out from the training set in fold_id 2.
The rowid = 0 is part of the test set in fold_id two.
Let's go to the test_splits_df,
and let's rename the column index to be rowid.
Force that to be in place.
Now test_splits_df.loc, test_splits_df_rowid == 0.
Notice one and only one row has been returned.
The zeroth row is held out once and only once.
This is true for all observations.
Test_df.rowid value counts.
Every single row is held out once and only once.
However, every single row
is trained or is
included in the training set multiple times.