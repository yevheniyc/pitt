Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/sCcs6/evaluating-predictions-and-accuracy-within-each-fold

English
Now that we have fit the model,
let's predict the training set within each fold.
Now this is something that's not typically done with cross validation.
I'm going to do it here for teaching purposes and you'll see why at the end.
Let's go ahead and make a deep copy.
This is exactly what we did last week.
We're making a copy of the training set.
Let's add in a column the predicted probability.
So we need amod.predict train data.
Then we need to make the classification.
We need to convert the event probability into a binary outcome.
We will do that using the np where,
just as we did last week, np where.
If the predicted probability
is greater than the threshold,
we return the event.
Otherwise, we return the non event.
I can use 1 and 0 here because I know in this application
the binary outcome is encoded as 1 or 0.
Okay, now let's predict the interesting part.
Predict the test data set within each fold, the thing we really care about.
Let's make a deep copy and now add in a column for
the pred probability amod.predict.
We do not provide the training data set, we instead provide the test data set.
This test data set is 20% of the original data because I'm using five fold cv.
Then we need to convert the predicted probability into
the classification using the same process np ware
testcopy.predprobability greater than threshold.
Return the event, otherwise the non event.
Now, last week we used the confusion matrix function from scikit-learn.
That way we could get the accuracy, the sensitivity,
the specificity and the false positive rate.
We also use the roc auc score function to give us the area under the roc curve.
But for right now, let's keep it simple.
Let's focus on the accuracy as our performance metric.
So we're only going to calculate one thing,
calculate the performance metric
on the training set within the fold.
Train res, this is a list that was initialized to be empty,
so I will append to it the accuracy.
And the accuracy can be calculated,
as we saw from last week, by comparing if the observed
output is the same as the predicted class.
So I'm using the bracket notation to filter to one column
in my copy data frame using the equals equals operator,
and then applying np mean to get the average, the mean,
the proportion of times the predicted class matches the observed output.
And then likewise,
we calculate the performance metric
on the testing set within the fold,
test res.append np.mean test copy
y name == test copy pred class.
The overall structure of predicting and assessing is the same.
The only difference is whether we're using the training set versus the testing set.
Okay, now I'm going to hit enter but
then use the keyboard shortcut Ctrl+left bracket to dedent.
I'm now outside the for loop.
We need bookkeeping to store the results.
I first need the training set results.
I'm going to create a data frame using a dictionary that has a column accuracy or
a key accuracy which is equal to the result in train res.
I'm adding in a column from set training and
then adding in another column fold id.
This list will have as many elements as the number of times we split the data.
Therefore, the index is going to go from zero to the number of folds minus 1.
So I'm going to add 1 to the index.
Now, fold id will be 1 to 5, in this case.
Do the same thing for the test results,
accuracy test res,
test df from set equals testing.
Test df fold id,
test df index plus 1.
Combine the splits together.
Res df pd.concat train df test df ignore index=True,
add information about the model.
Okay, so now I also want to just include in what's the name of the model, mod name.
What's the formula for the model, a formula.
How many coefficients the num coefs were estimated?
I can use the length of amod.params because this is a stats model object.
And then just to be consistent with last week,
let's also include in the threshold value that we used.
That way we are mirroring the bookkeeping information that we used last time.
Last time, though, we only had a single value for
the performance metric because it was calculated only on the training set.
We now have five values for
the training set accuracy and five values for
the test set accuracy because I'm using five fold cv.
And then return the resolving data frame.
Let's test this function on a single model to make sure it works.
Let's use the intercept only or constant model.
So the simplest model, so train and test logistic with cv.
I'll give its name as 0 formula will be the 0th element from our list.
The entire data set will be df.
Now, the reason why we originally made the input names was we can set x names.
We can also set the output name.
The cv object will be kf and
let's just use the default threshold of 50%.
Oops, it looks like I have an error.
It fit successfully, but notice it says local variable test copy.
Something's wrong here.
Look at this.
I had an error.
Instead of test copy.copy,
this should be test data.copy, test data.
This highlights the importance of testing your function
on a simple case before you apply it on the entire thing.
And so that's why it is really critical to do this.
All right, I made the change.
Let's rerun that cell.
And now come back in here and rerun.
There we go.
We have one, two, three, for, five, five successful terminations.
And it's just displaying the return data frame.
If you notice, we have training 1 through 5.
We have testing 1 through 5.
Here is the training set accuracy in each fold.
And here's the holdout test set accuracy in each fold.