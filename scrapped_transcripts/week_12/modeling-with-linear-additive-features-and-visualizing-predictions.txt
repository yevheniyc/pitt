Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/pHIV5/modeling-with-linear-additive-features-and-visualizing-predictions

English
But now let's fit
the linear additive features with both inputs.
I'll name this model fit_x1x2_add.
Again, smf.ols. There's a formula.
There's a dataset, and we fit.
The data is still df.
But the formula now is not just y Tilda x1.
It's not just y Tilda x2.
It's y Tilda x1+x2.
We are adding their vects together, x1+x2.
You run this function.
If you check the coefficients,
we now have slopes being estimated for both x1 and x2.
Look at the slope on x2.
This is not a negative number.
It is a positive number.
What that means is just because you fit
a model using a subset of the inputs,
when you allow for additional inputs into the problem,
the sine of the slope
and the estimate of the slope itself may change.
X, the slope multiplying x1 and the slope
multiplying x2 are getting estimated together.
They are not estimated separately.
That's a really important concept.
You cannot just fit your model say for x2 and then say,
I have a negative slope,
it doesn't matter, and then fit the model for
x1 to know what the slope is on both x1 and x2.
You have to account for both of them.
Again, we can check the standard errors and the p-values.
The p-values on x1 and x2,
only x1 is statistically significant.
It's less than the common convention of 0.05,
and so even though the sine of the slope,
multiplying x2 is now positive,
we're not 100% confident in that slope.
It's tough to be able to identify that slope,
given the small dataset that we are working with.
Remember, we only have 35 data points here.
Given this small dataset, it's tough to see it.
Let's now visualize,
because I'd rather just visualize it
than display the confidence intervals.
Again, the estimate on
the slope multiplying x2 is now positive,
because we are accounting for x1.
Let's now make predictions.
Predictions involving multiple inputs
require data frames that
have all inputs used to fit the model.
I like to visualize the relationship
between the output and the most important inputs.
I'll just say input. How do
we know which input is most important?
Well, when your inputs
have roughly the same magnitude and scale,
which by construction, they do in this example,
because remember, we said they both
had means zero, standard deviation one.
If they wouldn't, you would need to standardize them.
But once they have been standardized,
the input that has
the largest magnitude slope is the most important input.
Because remember, the slope is literally
how much does the average output change
for a one unit change of the input?
If that magnitude is small,
it's a small change.
If the magnitude is large,
it's a large change.
This can be the case whether you
have positive or negative slopes,
so that's why it's important to examine the magnitude.
If we pull out the parameters,
if we sort values,
there we go, ascending false.
I want them in descending order,
but we need their absolute values.
Notice I'm pulling out
the coefficient estimates from the.params attribute,
wrapping that around the np.abs function.
This stands for absolute value,
then sorting the values ascending equals false.
We can see x1.
It has the highest magnitude.
Create a data frame with
many values of x1 for a single value of x2.
I'll name this one df_viz_1, pd.DataFrame.
The data frame will be created from a dictionary that has
key x1 by itself. Let's just do that.
Key x1 by itself.
I want to have many values of x1.
Df.x1.min up to df.x1.max, num 251.
I'm using np.linspace just as we
did last week to create
the visualization for the predictions,
and we can even put in a little buffer,
go a little bit less than
the min and a little bit greater than the max.
But let's now add in x2 at a single constant value.
What's a good constant value?
How about the average?
I'm using the mean of the training set of x2.
Now, the training set
bounds on the most important input and
the training set average of the rest
of the inputs are used to define my prediction grid.
When we make predictions,
we want to include the trend, the average output,
the uncertainty on the trend, the confidence interval,
and the uncertainty of a single measurement
, the prediction interval.
Remember, this is created in two steps.
We have the prediction.
We have the get_prediction method,
so I'll assign this result to predictions_1
, fit_x1x2_add.get_prediction, df_viz_1.
Now, the second step, I'll name it,
how about pred_x1x2_add_summary,
predictions_1.summary_frame.
The result is a data frame.
One row is one prediction.
We have the trend in the mean column,
the lower and upper bounds on the confidence interval,
the lower and upper bounds on the prediction intervals,
or on the prediction interval.
Now let's visualize the predictions using ribbons.
Ribbons for the uncertainty intervals,
and lines for the trend.
Fig.ax, plt.subplots.
We'll have the prediction interval.
We'll have the confidence interval.
We'll have the trend.
We will set the labels,
and then show the plot.