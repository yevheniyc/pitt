Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/csUdf/interpreting-dummy-variable-coefficients-in-linear-models

English
But now, what do these coefficients mean?
Let's understand what these coefficients mean.
Dummy variable slopes, fit_a.params.
Intercept 98, the dummy slope multiplying B.
The slope multiplying the dummy for x=B is four.
Well, let's go ahead and look
at the average output for each category.
Df.groupby('x'), aggregate avg_y,
it's equal to the mean method applied to the y call.
I'm grouping by x,
and now I'm aggregating calculating the average of
y for each unique value of x.
Look at the top row when x=A.
When x=A, the average value is 98.7.
Now I want to save this.
The avg_y_at_x=A.
Let's go ahead and take df.loc.
I want to take df.x=A,
y mean, avg_y.
There we go. I split
my data frame manually using this condition.
Let's now go ahead and assign the result
to a df_summary where I
group by x and
aggregate to calculate the average value of y.
Now, with my df_summary,
I can add in a new column, the avg_at_x=A.
I'm doing this because I want to calculate
the relative difference to x=A.
Df_summary.avg_y minus the summary of avg_at_xA.
To make it a little easier,
let's round the columns to about three decimal points.
Now, obviously, the difference
between the avg_at_x=A with itself is zero.
That's what this is telling us.
But let's look at the row below it.
Sorry, I should have just named this
the difference_to_xA, 4.474.
Let's look at the coefficients.
The intercept is 98.73.
The dummy slope when x is B is 4.474.
The dummy slope when x is C is 3.176.
The dummy slope when x is D is 1.795.
The dummy slope when x is E is 54.
The dummy slope when x is F is negative 44.
Can you see what's happening?
The dummy slopes,
the slopes multiplying these new features,
are the relative difference between
the avg_at_x=A and the average
at each of those categories.
Now, why is x=A special?
When dummy variables are defined,
a reference category must be defined.
All dummy slopes are
calculated relative to this reference point.
By default, the reference category
is the first alphabetical category.
That's why x=a became the reference point.
The intercept is the average at the reference.
The reference becomes the intercept,
and all of the other dummies
are the relative behavior around that reference point.
This means when we
examine statistical significance, we are asking,
is there a statistically significant difference
in the average output from the reference category?
If you check the p-values,
and you examine if the p-values are less than
the common threshold of 0.05,
everywhere where the p-value is less than 0.05,
those are average differences that we can trust.
Because by definition,
the slope multiplying the dummy is the difference.