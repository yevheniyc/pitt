Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/iOlZg/introduction-to-linear-additive-features

English
Hello, everyone. As you can see,
I have Jupyter opened
up to the directory for this week for the class.
If you don't have Jupyter started,
please pause the recording and do so.
Once you're back, I want you
to launch a new Jupyter Notebook,
a new Python 3 kernel.
Let's go ahead and name this notebook,
Week 11, features additive.
In the previous video,
we've introduced two main features
that we can derive when we have multiple inputs,
we can have additive features or interactions.
This video demonstrates how to fit
models with additive linear relationships,
how to make predictions,
and how to interpret the results.
Let's go ahead and put in our header information,
CMPINF 2100 Week 11,
and we'll name this working with
linear models with additive features.
Now, we will generate our own data for this example.
This will again review
the assumptions of the linear model
because they are that important.
We will fit the model using stats models.
We will interpret the results via the coefficients,
so the coefficient summaries.
We will interpret results through predictions,
and we will use
specialized stats models graphics
to get further insights into the behavior.
Let's go ahead and import modules.
We need the big four,
import numpy as np,
import pandas as pd,
import matplotlib.pyplot as plt,
and import seaborn as sns.
We also need to import statsmodels.formula.api as snf.
We will use the stats models formula interface
to fit linear models for regression.
Linear additive features.
We will work with two inputs,
but the ideas scale to many inputs.
As we saw in the previous recording,
linear additive means you
add the effect of each input together,
and each input is multiplied by
a slope in order to
calculate the average output or trend.
When we have just two inputs,
there are a total of three regression coefficients,
the intercept and the two slopes.
Thus, to create our own data,
we must specify the regression coefficients.
We could use any values,
but let's use the same three coefficients
from the previous recording and intercept of -0.25,
a slope multiplying Input 1 of
1.95 and a slope multiplying Input 2 at 0.2.
As we talked about in the previous video,
input 1 has a slope roughly 10 times
larger than the slope multiplying input 2.
In the previous video,
we were visualizing trends,
but now we will make random input data.
Randomly generate input values for this problem.
For simplicity, let's assume
both inputs have standard normal distributions.
Standard normal are Gaussians with mean
zero and standard deviation of one.
We'll use a relatively small example
with just 35 data points.
So n = 35 is the number of observations.
Since everything's going to be random,
we need to initialize our NumPy random number generator,
and let's assign it to
the rg_object, np.random.default_rng.
Let's set the C to 2,100 for reproducibility.
Now, call the random number generator for two inputs.
I'll assign these inputs to the df_object.
How about let's do it to input_df?
It's assigned pd.DataFrame, created from a dictionary
where there's key x1 and key
x2.The value for the x1 key will be rg.normal,
and the value for the x2 key will also be rg.normal.
The mean or location will be 0.
The scale or standard deviation will be one,
and the size will be equal to
n. We'll do the same thing for x2,
location of 0,
scale or standard deviation of one,
and the size is equal to
n. Input_df is therefore a DataFrame with 35 rows,
and x1 and x2 are both floats.
We can see that there we have zero missing numbers,
right, because we randomly generate the data ourselves.
Now that we have the inputs,
let's calculate the trend or average output,
assuming the linear additive features.
Let's make a copy, though.
Let's say df = input_df.copy.
That way, my original input data frame
is completely separate.
Now in df,
I'm going to add a column named trend,
which is the intercept plus
the slope times x1 plus the slope times x2.
I'm not using interactions here,
just adding the effect
of x1 to x2.