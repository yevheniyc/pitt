Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/Uf1dA/understanding-multiple-input-model-visualization-and-interpretation

English
But now one more thing to note about this figure,
if you look closely at the red dots,
they are not on a straight line whereas
our visualization that we made is on a straight line.
The reason is our visualization
literally had one and only one x2 value.
But in this figure,
this is whatever the values are on the training set.
You will not easily see
a straight line if you have
a situation where you have many inputs at play.
That's something to keep in mind.
You can roughly see the straight line here because we
know x1 dominates the output-to-input relationship.
Another way to see that is let's visualize
the training set fit with respect to 'x2'.
Let's make the same actions
where we initialize the figure and
axis objects, then sm.graphics.plot_fit(fit_x1x2_add),
but now I want to visualize with respect to x2,
ax = ax, plt.show.
If you were wondering why we spent the time in
the previous video looking at
the relationships with respect to x2,
it was ultimately to get to this figure.
It doesn't seem like anything is going on here.
But that's because you are not looking at
the relationship of x2 for one value of x1.
Each one of these dots has
a different value of x1 and x2,
so you're actually getting the influence,
the effect of both x1 and x2 here.
This figure is actually more aligned
with when we made that initial scatter chart.
You're just seeing what
the fitted or training set predictions look like.
One more class of figure,
we can examine the training set predictions or fits,
and the residuals,
and the partial regression plot in one figure.
I'll describe what the partial regression plot
here is once it's made,
but to create this one,
you only need to initialize
the figure and not the axis object,
so fig = plt.figure, then sm.graphics.plot_regress_exog,
exog stands for exogenous.
You give it the fitted model object,
you specify the input you want to focus on,
and then which figure you're assigning it to.
By default, you get
this rather small-looking set of subplots.
I typically like to raise
the figure size to something bigger.
The fits are shown in the top left,
the residuals are shown in the top right,
and you want the residuals.
This is the error. With respect to that input,
you want the residuals to have no trend.
You're ideally looking for
no clear trend in the residual.
But now, the partial regression plot.
This is the trend or
average output given the value of that one input,
but it accounts for or
controls for the other inputs.
This trend line that you're looking at is
accounting for the fact x2 is in the model.
The one on the right is
another variation of the partial regression plot,
the component to component,
but I'm just going to focus on
the partial regression plot here on the left.
Now, this plot, it doesn't look all that interesting.
But look at the partial regression plot for x2.
Again, the syntax initialize the figure, plt,figure,
and I'll set the fig size again to that
larger value, sm.graphics.plot_regress_exog,
for exogenous , fit_x1x2_add, x2.
Now, I'm using x2 and not x1,
fig - fig, plt.show.
Again, the top left,
this is what we looked at previously.
The top right, the residuals are
the error with respect to the input,
we don't want to see any clear trend.
But now look at the bottom left.
Do you see that black line?
That's the trend given x2,
but controlling for x1.
Look at the slope,
the slope is positive.
Why is the slope positive?
Well, remember, we have
a positive slope estimated
that multiplies x2 when we account for x1,
and that's exactly what we
had when we generated the data.
We said there's a positive slope.
We couldn't estimate it perfectly
because we have a limited data set.
But when we did not account or control for x1,
we had a negative slope,
so this partial regression plot is
a way to visualize how your trend or
average output behaves with respect to
one input but accounting
for or controlling for all other inputs in your model.
That's not what's going
on when we visualize the lm plot originally,
this lm plot is not accounting for x1,
it's only accounting for x2.
Now, is this bad?
No, this is just the limitation of lm plot.
It's not really intended for you
to fit your entire model,
it's for you to have some rough idea about
the potential relationships for a single input at a time.
The relationships accounting
for multiple continuous inputs,
well, you need the model that is allowed to
estimate their influence or control for their effect.
I hope you liked seeing that.
I really wanted to spend the time to
build up to interpreting these figures,
to interpreting the predictions,
that's why the first video
was so long to get to this point.
Now, I want to go ahead and let's save
our randomly generated data so we can use it again,
so let's save df, so df.to_csv,
let's name that file linear_additive_example.csv,
and we will not include the index.
You should now have the linear_additive_example.csv
file in your working directory.