Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/mnyEG/visualizing-coefficient-summaries-and-alternative-encoding-methods-in-regression

English
Now, if you go back to
our example from last week where we
fit multiple regression models,
you would have seen, remember that function
that allows us to visualize the coefficient summaries.
I'm going to copy this function again.
Again, it's using the approximation of
the 95% confidence intervals
two times the standard error.
I'm going to put that in. I just want to show
the coefficient plot for this model.
Here's F's dummy, here's E's dummy,
I's dummy, and J's dummy.
Their 95% confidence intervals do not include zero.
This information is,
therefore, telling us,
essentially what we observed in the box plot,
and especially what we observed in the point plot,
E, F, J,
we even identified as being
statistically significantly different on average,
even though visually, maybe we couldn't see it.
But that's what this is showing us here,
E, F, I, J,
same thing Oh,
the reason why I is considered
is because the average here
is less than that
lower bound on the 95% confidence interval.
I always like to be safe when I'm visually
exploring to see if the confidence intervals overlap.
But that's part of the reason why
this is visual exploration,
and here's the quantified effect from the model.
If you look closely, I,
it's the closest of those to
having its confidence interval contain zero.
If we predict with our model,
it will only predict at the observed categories,
so this is something that's really, really important.
If we make an input_grid, actually, here,
let me put in, let's call these predictions.
If we make an input_grid pd.DataFrame,
let's use a dictionary with a key X,
and that key is df.x.unique,
when you predict with this model.
Actually, we'll just do this,
fit_a.predict(input_grid), nine and
only nine values are provided.
Let's now add those predictions to our summary,
and let's call it predict_from_dummies,
fit_a.predict(input_grid).
The average here,
oops, actually, if you notice,
these are out of order.
Sort_values.
We need the unique,
and we need to sort
their values by x.
There we go.
Inplace true, ignore_index true.
I'm glad I demonstrated that.
You could see these predictions
don't line up with the averages,
but that was because originally,
I didn't have the categories in the same order,
so all because of the way it unique were.
But now with them in the right order,
when I make the predictions,
I can literally just now add
those predictions to the corresponding row
in my df_summary data frame.
There we go. Now,
here's the average y for x is G,
here's its average predicted from the model.
Here's the average with x is J,
here's the prediction from the model.
You can see they're the same things.
Even though the coefficients,
the dummy variable slopes are the relative effect,
the predictions from the model are
fundamentally the intercept plus the slope.
That's the way all linear models work.
Intercept plus slope times the input,
but the input is either zero or one.
Now, there's one other way, an alternative.
An alternative approach is to remove the intercept.
This is known as the one-hot encoding.
One-hot encoding is very
popular amongst the Python community.
I, however, despise it,
and I'll show you why here.
It's the same function call.
We have data as df.
But now, the formula,
y ~ x,
but to remove the intercept,
we need the special -1 formula object or
formula argument or formula operator,
to remove the intercept using
the -1 operator in the formula interface.
The minus operator removes a variable from a data frame,
-1 means remove the intercept.
Now, if we check the coefficients,
the way the coefficients are written are different.
You do not see that T here.
T is Statsmodels' way of saying,
this is a dummy variable slope.
The bracket by itself is,
what is the average output when x,
the input is this category?