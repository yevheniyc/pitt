Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/n7951/setting-up-and-understanding-binary-classification-with-logistic-regression

English
Hello, everyone.
As you can see, I already have Jupyter opened up.
If you've been following along,
you should have the Jupyter notebooks that we have programmed in this week.
But if you look closely at the bottom of my File Explorer,
you will see a new CSV file.
I would like you to please download this CSV file and
put it into the directory for this week.
For the class, we will use this data set to learn the basics of the syntax for
fitting binary classification models using logistic regression.
If you don't have Jupyter open, please pause the recording and do so.
And when you come back, launch a new Jupyter notebook with the Python 3 kernel.
I want you to name this notebook week_11_logistic_fitting_statsmodels.
Let's now go ahead and put together our headers.
CMPINF 2100 Week 11, introduction
to fitting logistic regression with statsmodels.
Logistic regression is a model type for binary classification.
However, logistic regression is a generalized,
Linear model, or GLM.
Therefore, almost everything from linear models, LM, applies.
We can use everything about creating features,
additive features, non-linear features
derived from inputs, so on and so forth.
There are several key differences
which we will learn about next week.
Okay, so we will see those key differences next week.
For now, let's go ahead and import the modules.
You need the big four, import numpy as np, import pandas as pd,
import matplotlib.pyplot as plt, and import seaborn as sns.
We also need the stats models formula interface.
So import statsmodels.formula.api as smf.
Once you've done that, let's read in the data and assign it to the Df object.
So df gets assigned pd.read_csv.
Week eleven intro to binary classification now,
in this data set, there are 115 rows in two columns.
The input is named x and the output is named y.
As you can see, both input and output are numeric data types.
The y variable is an integer, but
if we check the number of unique values,
you will notice y has two and only two unique values.
If we apply value counts, even though the output is
encoded as an integer, one or zero, these two and
only two unique values mean this is a binary classification problem.
Now, will you know if you have a binary classification
problem based solely on the data type of the output?
The answer is no.
You need to know the context.
A great clue is the data type, followed then by the number of unique values.
And so if you are told you're working on a regression problem, and yet
you find out that your output has two and only two unique values, especially when
you have hundreds, if not thousands to hundreds of thousands of data points.
You need to go back and ask, are you sure this is a regression task?
Or maybe is this really a classification task?
We could, of course, also create the bar chart with sns.catplot,
x='y', kind='count' to reveal the same thing.
Now, our output is not perfectly balanced.
We don't have a perfect one to one count.
The counts are not the same of the y equals 1 or
event versus the y equals 0 or
the non-event, but they are pretty close.
Why this is important is something we will talk about next week.
Now, because the output is numeric.
If you apply the .describe method, The average output,
this is really the proportion of times the event has occurred.
You can see that by directly applying the mean method to it,
which if we would apply value counts to y, and
then set the normalize argument to true the proportion of
rows where y equals one is equal to the average output.
That is because we are summing and dividing by the length, and
when the values are zero or one, summing is the equivalent of counting.
So that's why the average or the proportion from value counts for
y equals 1 is the same as the average.