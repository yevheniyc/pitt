Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/jJ0i3/understanding-the-fundamentals-of-logistic-regression

English
Now, why does this matter?
Why am I talking about?
Well, even without getting into the math,
probabilities are fundamentally bounded.
You cannot have a probability
less than zero and you
cannot have a probability greater than one.
A probability is a fraction
or a decimal between zero and one.
This concept has a drastic impact
on how we model binary classification problems.
The reason for that is
we are modeling the average output.
But if we would use
our simplest or conventional best fit line approach
of the average intercept plus slope times input,
this equation is completely okay with negative trends.
After all, you've seen that over
these last few weeks where we
can predict negative trends.
This equation is also
fully okay with trends greater than one,
and you've seen that over the last few weeks.
That means there's nothing
special about this equation that will
keep the trend within
the bounds of the event probability.
You cannot use this formula
to predict the probability of the event.
Unfortunately, this is done in many fields,
and it is published in many top tier journals.
This is especially true in accounting and econometrics.
This is known as the linear probability model,
but it's fundamentally wrong.
You cannot actually use this,
even though it is used and
published in many journal articles.
Just because it's in a journal,
it does not mean it's right.
Now, what do we end up doing?
We need to perform or execute a transformation.
We will not directly model the event probability.
The kind of transformation to use, well,
there are in fact multiple varieties,
but a very popular one is to use the log odds ratio.
The log odds is what we will be
predicting or modeling or regressing.
It's the log odds where we will
set our intercept plus the slope times the input.
This equation, intercept plus slope times input,
no longer equals the average output.
It equals the log odds ratio.
But what is the log odds ratio?
Well, the log odds is
the natural logarithm of the odds ratio.
The odds ratio or OR
is the probability divided by one minus the probability.
If you've ever heard anybody say,
what are the odds of that?
That's usually used to actually ask,
what's the chance for the probability of something?
But the odds ratio
itself does have a mathematical definition.
It's the probability divided
by one minus the probability.
If the probability is 50%,
50%/1-50%, 0.5/0.5 is equal to 1.
The odds ratio is equal to one if the probability is 50%.
The log odds ratio then applies the natural log.
But in terms of the notation that I have
used throughout this presentation
and even over the last few weeks,
the average output is called Mu.
In our current context,
that means I'm predicting the event probability,
so the odds ratio is Mu/1-Mu
or the average divided by one minus the average,
the log odds ratio,
the natural log of the odds ratio or the log of Mu/1-Mu.
In all my notes,
whenever I write log,
that's sometimes written as LN for natural log.
But after all in NumPy,
np.log is the natural log.
If you're a MATLAB user,
it's the log function.
If you're an R user,
it's the log function.
That means natural log.
That's why I write log.
The best fit line, if you will,
or our linear model is now applied to the log odds ratio.
We are not directly predicting the probability.
Instead, we are predicting a transformation of it.
The log odds ratio has no bounds.
It can be between negative infinity
and positive infinity.
Any real number, whole integer,
or decimal is okay.
Another name for the log odds ratio is the logit.
It actually stands for log odds unit.
Therefore, we are regressing the logit,
the log odds of the event probability.
Our regression model is asking,
how does the input relate to the log odds?
The slope is how much does
the log odds ratio change for one unit of the input.
Again, we are regressing
the logit of the event probability.
If you look closely at this formula,
it looks very similar to what
we have been working with over the last few weeks.
This approach is fundamentally
generalizing everything that we
have talked about with linear models.
Everything about features, everything about
statistical significance all applies.
Now, where does the name logistic come from?
If you invert the logit,
you get the logistic function.
We need the logistic.
Because in order to ultimately predict
the event probability or the average output,
which we care about, we need to
invert the log odds ratio.
The logistic is the inverse of the logit function,
and that's why this class of
model is known as logistic regression.
Whenever you hear logistic regression,
even though regression is in the name,
this is not a regression method.
It is not to predict continuous outputs.
Logistic regression is a binary classification model.
The model is still predicting the average output,
but we know that the average
corresponds to the event probability.
But probabilities are bounded,
and so by applying the logit or log odds transformation,
we make sure we are
respecting those bounds of zero to one.
That transformation is what fundamentally allows
us to make use of all of our linear modeling techniques,
and thus, logistic regression is
a type of generalized linear model.
Again, as I mentioned, everything
about features, nonlinear features,
interactions, additive, categorical inputs,
all of that stuff applies.
If you've wondered why I've
spent so much time on linear models,
it's because pretty much all
of the concepts around working
with them lead us into binary classification tasks.
Again, the purpose of this discussion was to
introduce why we need to
make changes when we go from
regression to classification and where this term,
the logistic, comes from.
The next recording shows you how easy
it is to fit logistic regression models.