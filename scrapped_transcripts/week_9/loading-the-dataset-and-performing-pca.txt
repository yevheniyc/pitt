Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/qlim6/loading-the-dataset-and-performing-pca

English
Hello, everyone. In this video,
we're going to review important aspects
of KMeans clustering.
We will also introduce another kind of
clustering algorithm to contrast
the clustering results with KMeans.
As you can see, I already have Jupyter opened up.
I am inside the directory for
the class for the new week, Week 9.
If you do not have Jupyter launched,
please pause the recording and do so.
Once you have done that, come back and
launch a new Jupyter Notebook.
Let's change the name of
this notebook to week_09_review_cluster.
Now, let's put in the header information,
CMPINF 2100 Week 9,
review KMeans and introduce hierarchical clustering.
Let's import the modules.
We need the big four that we have worked with now
for quite some time: numpy as np,
pandas as pd,
matplotlib.pyplot as plt,
and seaborn as sns.
But I also want to import functions from
Scikit Learn to support KMeans.
That's from sklearn.preprocessing import StandardScalar,
from sklearn.decomposition import PCA,
and from sklearn.cluster import KMeans.
Next, we will read in data.
Last week, at the conclusion of the week,
when we talked about more realistic examples,
one of those examples was the wine data set.
I have opened up already
the HTML report for the pca_intro notebook.
I'm using this notebook because it
contains the URL to the wine data set,
as well as the names that I want
to use for the columns in the wine data set.
The first thing I'm going to do is
copy the URL for the wine data,
paste it into this cell, run that cell.
Then I'm going back to my HTML file,
I'm copying the entire cell that has
the wine_names variable defined as a list of strings.
Then paste that in.
If you scroll through,
you'll see the entire list has been pasted.
Run that. Now we can read in the wine data using the
pd.read_csv function from the URL
where we set the names of the columns to be wine_names.
After reading it in, I will often like to use
the.info method to just confirm everything has worked.
I will not go through the marginal histograms for
the conditional distributions for
these variables in this video.
That was done last week, but here,
I want to just focus on clustering
all 178 rows using all of the numeric variables.
If you noticed, when I highlighted the variables,
the columns as I said are numeric,
I left out cultivar because as discussed last week,
cultivar has three unique values.
Let's convert cultivar,
as we did last week
to the category data type from Pandas.
Now cultivar is not an integer,
it is a category,
it's a non-numeric variable.
We now have the data imported
read in with the data types that we need.
As discussed last week,
before we can execute KMeans,
we NEED to remove the MAGNITUDE and SCALE effects.
We therefore need to standardize
the numeric columns BEFORE we can cluster.
The first thing we should do is extract
or select all of the numeric or number columns.
I'm assigning all of
the number or numeric data type columns
to a new DataFrame wine_features,
and I'm making use of the.copy method to ensure
that this is a deep copy.
As you can see from the columns,
cultivar is not present.
It was not selected because we
converted cultivar to a category.
Let's now standardize the numeric columns.
I'm going to assign the result to the Xwine object,
where I initialize StandardScalar and
then fit and transform in a single line of code.
Xwine has the same number of
columns as the DataFrame that
we provided to it, 13 columns,
13 columns, but the result from
the Scikit Learn StandardScalar function is an ndarray.
Next, let's execute PCA
to support and streamline visualizations.
When we use PCA,
it is best to have already
standardized all of your numeric columns,
and so I will apply the PCA function to
fit and transform the standardized columns in Xwine.
As discussed last week,
when we are focused just on using PCA for visualization,
it is common to set
the n_components argument equal to two.
Let's now fit and transform Xwine.
Even though Xwine had 13 columns,
here, we are telling it,
give us two columns,
but these are not any of the original columns.
These are NEW variables
that were created based on
the original standardized features.
Let's take a look at them.
In order to do that,
we will create a new DataFrame
converting the pca_wine ndarray,
and we will name the columns pc01 and pc02.
We do not have the original 13 numeric variables.
We have Principal Component 1 and Principal Component 2.
We can now visualize the relationship between them,
pca_wine_df, x = pc01, y = pc02.
We can see that kind of arrowhead or triangle shape.
We can also use
a pairs plot to
not only show the relationship between them,
but also see their marginal histograms.