Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/qTO5e/key-material-for-module-9

English
Hello, everyone. Welcome to
the ninth week of the semester.
This week is divided into two key areas.
The first reviews important areas
associated with unsupervised learning
that were introduced at the end of Week 8.
After that is completed,
then we finally begin
supervised learning or predictive analytics.
When you review unsupervised learning,
you will review the practical steps needed
to apply KMeans clustering and
then find the optimal or best number of
clusters using the Wine dataset,
so we will repeat actions from last week.
However, then you will contrast KMeans with
an alternative kind of
clustering method called hierarchical clustering.
The practical implications of
hierarchical clustering are
demonstrated through visualization,
and its important assumptions
are reinforced through the visuals.
You will learn about the main terminology of hierarchical
and how it contrasts and differs with KMeans,
as well as what's similar
between hierarchical and KMeans.
The programming examples are
reinforced with the presentation that walks through
the major details in greater detail
through simpler applications on
the one-dimensional problem from last week.
This first video is rather long.
I apologize about that,
but that was the only way to bring everything
in together to keep the story flowing.
This first video on clustering is rather long.
Then you review PCA on the sonar data.
I really like the Sonar data set because there
are 60 continuous variables.
They are highly correlated,
and you get to use PCA to exploit that correlation.
But in addition to just reviewing the major steps,
this video discusses PCA in more detail.
Not necessarily the math behind it,
but more of how you can use PCA to help
you when you have a very large number of columns.
It shows you what happens when you do
not set the number of components to return,
and it provides examples for how to
use the results to explore your data.
Then we start supervised learning
or predictive analytics.
Most textbooks and classes
introduce linear models or linear regression,
the same way where you're provided a dataset,
and then you fit a model to it,
and then the assumptions are discussed.
Well, I don't like to do it that way.
I like to work in reverse.
I like to demonstrate the assumptions using
all the programming skills
and concepts that we have seen in this course.
Do you remember at the beginning of the semester
when you were generating a bunch of
random numbers and calculating
the standard errors on averages?
You're going to be doing the same thing here.
What you will find is
the exact same skills allow you to study
regression problems because all of
those aspects are inherent
to the assumptions of the linear model.
You will use NumPy, Pandas,
and the NumPy random number generator to randomly create
your own example to reinforce all of the assumptions.
You will see that by using
visualizations to summarize the data
to literally see all aspects of the assumptions
of the linear regression model.
Again, this is backwards compared
to how most textbooks talk about it,
but I feel it reinforces what the model
thinks can happen in greater detail,
rather than focusing on just
here's a data fit a model to it.
We need to know those assumptions
before we can go about fitting of the models.