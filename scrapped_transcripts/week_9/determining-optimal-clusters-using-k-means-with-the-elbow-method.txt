Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/IW9Kl/determining-optimal-clusters-using-k-means-with-the-elbow-method

English
Okay, now that we have PCA executed, let's go ahead and apply K-means.
So last week, we saw how to execute
K-means using three clusters.
And we used three clusters because cultivar has three categories.
This week, let's identify the optimal
number of clusters using the knee bend plot.
To use the knee bend plot,
we need to calculate the total within sum of squares for
many different, or possible number of clusters.
K-means does not tell you the number of clusters to use,
you have to just give it a data set and specify the number of clusters.
To figure out the best or optimal number, you literally have to try out a whole lot.
Last week, we saw that we should do this by initializing an empty list,
totes within standing for total within sum of squares.
And we have to define a range for the possible number of clusters,
starting with one where we have just a single cluster, which isn't interesting.
But that's what we should start with,
up to a large number of clusters, so here I'll go up to 30.
Most extreme case would be one data point per cluster, but I often
don't feel it's really necessary to literally go out to do that with K-means.
because that would mean we have to run the algorithm in
this case 178 times because there are 178 rows.
Okay, let's now set up the process using a for
loop for k in K, which is a sequence.
Let's initialize the K-means object, n clusters equals k, random state.
Remember, we have to control the randomness, set the seed.
We need to deal with randomness because we will randomly initialize or
restart the algorithm multiple times.
And it's an iterative algorithm, we need to set the max number of iterations.
Those were the main assumptions that we discussed last week.
Then let's fit using these standardized features.
So here I'm initializing and
fitting the K-means in one line of code.
And then appending on the next line two totes within list
the total within sum of squares, which is contained in
the inertia_attribute of the K-means fitted object.
So the inertia underscore, and the underscore is critical, you need it,
is the total within sum of squares the main metric of the K-means algorithm?
As mentioned last week, when you run this you
will get this annoying warning message and we could remove it.
But I personally just don't feel it's necessary,
just let it spit out the warning to the screen, that's okay.
And we will wait until it completes, it's still thinking.
We can see that because of the asterisk in the in statement.
Once it's done, we will then visualize the total within sum of
squares versus the number of clusters.
Here we go, it's done.
So let's visualize the total within sum
of squares versus the number of clusters.
Let's make the chart with Matplotlib directly rather than Cboard,
because this is really just plotting one thing versus another.
Initialize the figure and axis object, PLT subplots.
Then use the object-oriented approach ax.plotk,totes within.
And I'm now going to say I want a blue
curve that has the markers connected with a line.
Then we need to set the X label as number of clusters.
Then we need to set the Y label, the Y-axis label,
the total within sum of squares, EOT show.
Again, the X-axis, the number of clusters, the Y-axis,
the total within sum of squares.
We're looking for a kink or knee bend, an elbow bend,
because we know the total within sum of squares.
Or the variation within the clusters will decrease as we
keep using more and more clusters.
But the amount of decrease itself keeps decreasing as we add more clusters.
However, you can see with this real application,
it's not as cut and dry or as clear, if you will
compare to some of our simpler examples that we looked at last week.
So last week, remember, we clustered two variables,
and the clusters were very obvious and clear.
This is just the example from last week, week 8, K-means 2.
We had four or five distinct clusters.
And we saw the very obvious flat line occurring at five.
Well, here we don't have,
as here we don't have as definitive a flat line.
It's harder, but there does seem to be three
clusters as forming some kind of kink or elbow.
That has a decrease in the total within sum of squares that is decreased
compared to the decrease from going from one to two and two to three.
So from this figure, I would say, three clusters looks right.
But I mean, really, from the figure alone, anything from
three to really 8, 9, 10, it's difficult to pull out.
Remember, with clustering, there is no true best answer.
Some are just more appropriate than others.
And it's a fundamentally, subjective opinion.
So I would go with three here.
And now, maybe you don't like that, this is very subjective.
We literally, looked at a plot, and
we had to execute K-means many, many times to get here.
Why didn't I try 35 clusters or 50 clusters?
And that can leave an unsatisfying flavor in your mouth.