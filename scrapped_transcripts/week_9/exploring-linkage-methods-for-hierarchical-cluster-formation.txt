Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/xAE47/exploring-linkage-methods-for-hierarchical-cluster-formation

English
That is, however,
just one way we could represent the distance between two clusters.
Maybe that wouldn't have been your first guess.
But let's go back into the code and
now let's try another linkage called single linkage.
Executing this is very similar to what we did above,
but I will assign the result to hclust_single,
calling the hierarchy.single function, again,
using the standardized columns in Xwine.
Let me paste in the dendrogram code that I copied before.
That way, I have the same figure size.
But now, instead of hclust_complete, we are visualizing the dendrogram for
hclust_single.
And I will, again, say no_labels=True.
This dendrogram looks very different compared to what we saw above.
What you can see is there's a whole lot of snake or
daisy chaining where we're just adding together or
adding in one data point at a time to a cluster.
The height still represents the distance, the dissimilarity.
Higher values of height correspond to greater dissimilarities.
And if we compare the height in complete linkage compared to single linkage,
we can see the heights are quite different,
four versus something greater than ten for complete.
Now, what's going on?
In single linkage, we are representing the distance between
the two clusters as the shortest distance spanning them.
We literally find the two closest points, and we calculate their distance.
The reason why single linkage will have a smaller value for
the max height is because it's always viewing
dissimilarity as between the two closest points.
That's why you will oftentimes create these kind of snake or
daisy chains where you're finding a new point close to a group,
and then you latch onto it to grow the cluster.
And then here's a new point.
You latch onto it to grow the cluster.
Here's a new point.
You latch onto it to grow the cluster.
That's literally what single linkage is doing.
I personally very rarely use single linkage.
It works in very specific situations, but I find it not very useful.
I don't find that it helps me create meaningful clusters for
a lot of the applications that I work on.
Another linkage type, average linkage.
Let's run it, hclust_average = hierarchy.average(Xwine).
Again, let me copy the code for the dendrogram, but
change it from hclust_complete to hclust_average, where no_labels=True.
You can see the height here.
And with average linkage we're creating a handful of large clusters.
And we know they're large because of the number of data points contained in them.
But in this case, it also happened to produce a handful of very small clusters.
In fact, if you look closely on the far left-hand side,
there's one data point, this blue line, that is finally
merged into everything else just in the very last cluster.
This one data point is considered to be the most different
according to the average linkage.
Now, the average linkage, this might
actually have been your initial guess as to,
how can you define a distance between two groups?
The average linkage is the average distance between them.
You calculate all of the pairwise distances and you use the average one.
This is usually the kind of first pass guess is what you might think for
comparing two clusters.
But as you've seen here, that's just one possible way.
Highly related to the average linkage is the centroid linkage.
hclust_centroid = hierarchy.centroid applied to Xwine.
And now let's make its dendrogram, centroid, no_labels=True.
Centroid, it's similar to average in
that it's using some average distance.
But rather than literally averaging the distance,
the centroid calculates the center or centroid of the clusters and
then calculates the distance between those centroids.
It oftentimes produces very similar results to the average linkage,
though you can see here, there are differences in this case.
One similarity is that there's one and
only one observation that is essentially held out until the very end.
We have produced several very large clusters and
a handful of very tiny clusters here.
Lastly, the ward method.
This is my preferred approach, hclust_ward
= hierarchy.ward again applied to Xwine.
And now paste in the dendrogram, got hclust_ward, no_labels=True.
hclust_ward, in my opinion, tends to give the prettiest pictures.
And why is this the prettiest picture?
Well, when we're making clustering,
when we're applying clustering methods,
we would ideally like to produce a handful of clusters
that are considered to be different from each other.
This dendrogram is revealing that these three clusters,
the orange, green and red,
are different because they are only merged at a very high height.
So look at the difference here.
These three clusters are all created by roughly a height of 12,
but they themselves are not joined together until a height of about 27.
And then the final cluster is created, a height of 35.
Again, the height is the measure of dissimilarity.
The ward method is saying it thinks there are three clusters
that are different from each other.
Why are three getting recommended by SciPy,
which is just doing what Matlab would recommend you?
Well, do you see how I'm drawing these horizontal lines here?
I am cutting the tree into three branches.
And whether I make the cut at a height of 26,
24, 20, 17, or 15,
I am always producing three branches or clusters.
The recommended number of clusters is based
on cutting the tree at a height such that you
have maximum difference between the groups and
very little sensitivity to the height that made the clusters.
So whether I cut anywhere here, I'm always getting three clusters.
If I would have instead have cut down here, a small change in the height
that I make the cut at would drastically produce a different number of clusters.