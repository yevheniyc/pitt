Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/MU6Bq/gaining-more-insight-from-all-pca-components

English
Now, why does this matter?
We have used the first two PCs to help our visualizations.
Here, I'm going to create a pair plot using
the ILOC method to extract out the the first two PCs.
This is what we have focused on up to this point.
We have made plots between PC1 one and
PC2, but we now know there are more PCs.
We could consider exploring more PCs than just the first two.
This is helpful when there are dozens,
if not hundreds of variables in the data, and
those variables have some kind of correlation structure.
So instead of making a pair plot between just the first two PCs,
let's make a pair plot between.
How about the first seven PCs?
Although this is a lot of variables, it's still much,
much fewer variables compared to the 60 in the original data set.
And again, we haven't discussed in depth how to pick the number of PCs to focus on.
But as a first pass, this standard deviation plot is similar to the knee
bend plot from k means you're looking for that knee or elbow bend, and
then we will formalize this later.
You can also group or condition
PCs by categorical variables.
So let's add in to our PCA data frame the x60 variable,
that numeric or, sorry, that object variable that we saw has two values.
We can now create a pairs plot, or
actually, let's first use wide,
actually yes, let's use a pairs
plot to show the conditional KDE and
conditional scatterplot between
the pairs of PCs given x60.
I'm going to use very similar code as before,
but I will not filter the data.
Instead, I will use the VARs argument with our list comprehension.
To identify the first seven columns.
So, my VARs argument is is a list comprehension
where the action is the string PC placeholder 02d,
where the placeholder d is for d in range 1 to 7 plus 1.
I'm doing this that way, I'm giving all 61 columns so
that I can associate the hue to x60.
And now, my diag keywords,
I have to make sure that the common norm is false.
This will remove the sample size effect of the conditional KDEs
going down the main diagonal.
Last week when we introduced PCA for this example,
we looked at the scatter plot between PC1 and PC2.
So this plot right here, PC1 and PC2, but
now we're getting what is the conditional distribution
of each principal component given if x60 is m or r?
If you thought that, hey,
you know that scatter plot last week that didn't seem all that interesting.
Well, just remember it was only between the first two principal
components out of 60, and maybe there are other principal components
we should focus on to really find something informative,
like look at the conditional KDE for PC3.
There is a bigger shift between the two densities compared to some of the others.
The scatter plot between PC2 and PC3 or between PC1 and
PC3 helps give us a better picture of the differences between these categories.
So, I know we have yet to really discuss how the principal components are created,
and in truth we can't go to that detail in this course.
We do in the following course comp 2120, but
I wanted to show you, even though we're using PCA
to help us, there's even more we can do with PCA.
We don't have to just rely on the first two PCs.
We could consider additional principal components to really find
patterns in our data sets that have a very large number of columns.