Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/LAALj/interpreting-the-clustering-results-on-the-original-data

English
>> If you have a known grouping variable,
you should always compare the identified
clusters to the known categories.
Let's do that with a heat map.
First I need to bring into my PCA wine data frame
the cultivar variable, wine_data.Cultivar.
I now have the two categories, hclust_3 and Cultivar.
So I can create my heat map using sns.heatmap
by first creating the cross tabulation between
the known grouping variable, Cultivar and
my identified cluster groups from hclust_3,
and let's include in the margins, the marginal counts.
I want to annotate and then I'll set the font size and
I'll turn off the color bar because I'm annotating,
assign it to the appropriate axis.
You can see I get the scientific notation, so
I'll set the format to g and now I have simple integers.
What does this heat map tell us?
Well, cultivar one has 59 observations.
hclust_3, its 0th category, has 64 observations,
59 of them coming from cultivar 1.
Cultivar 3 has 48 observations.
The 48 are contained in hclust_3's 2 category.
But look at cultivar 2.
It has a total of 71 data points,
58 of those are in hclust_3's 1 category.
5, however, are in the 0, and
8 are spread out to hclust_3's 2 category,
meaning there are 13 data points in cultivar
2 that are getting associated with the other
categories of cultivar according to hclust.
Now why would this happen?
We have already seen what's going on with our PCA scatter plot.
You see the orange, this one orange here,
it's located kind of in the region of the blues,
and then you have all of these markers at the boundaries.
Those are data points potentially coming from cultivar
2 that have features similar to the other categories.
We could have gotten the same information using a dodged bar chart as well.
So x = cultivar, hue = hclust_3,
kind = count, plt.show.
Cultivar 1 is completely in hclust_0,
cultivar 3 is completely in hclust_2,
but cultivar 2 is spread out over the three cluster groups.
Those observations that are, quote, unquote,
misclassified would be ones you can then explore in greater detail.
We can also explore the original features.
Let's examine the conditional distributions
of the original wine numeric columns given the clusters.
I'm going to make a copy of my wine data set and
now add in a column hclust_3 to this copy.
I needed to do this because I originally assigned
the cluster label to the PCA data set, but I now want to assign
the cluster label to the original wine variable data set.
I can now examine the conditional distribution of any
one of these numeric columns given or grouped by hclust_3.
For example, let's make a box plot,
Showing how hclust_3 is related to flavonoids,
this variable here, flavonoids.
And we'll show that conditional distribution as a box plot.
Do you see the separation between the boxes?
hclust_0 has higher values of flavonoids, hclust_1 has middle values,
and hclust_2 has lower values of flavonoids.
We could have, instead, considered one of
the other variables, such as color intensity.
sns.catplot data = wine_copy, x = hclust_3,
y = color_intensity, kind = box.
Now it's hclust_1 that has the low
values of the numeric variable.
We have some more overlap between the other two clusters.
You might have been wondering, how did I know to pick these two?
Well, I didn't, but I had originally reshaped
the wide format data to long format to enable
exploring all of the original variables grouped by the clusters.
So let's go ahead and do that.
lf = wine_copy.reset_index,
rename the index column to row id and
now melt where the rows are identified by row id,
Cultivar and hclust_3.
Those are the variables that I do not want to gather up.
I want to gather all numeric columns though, and I can get that
because I've already separated the numeric columns into their own data frame,
so I'm using that data frames columns attribute.
lf now has way more rows because we've gathered or
stacked all of the original numeric columns on top of each other.
We have the original row in the data frame,
the Cultivar value and hclust label.
We can now use facets to examine
how hclust_3 is related to
each numeric variable.
So I'm associating the variable column in the long format data
to the column facet, and then I'm going to use five facets.
Let's stick with the box plot and I want to make sure the y-axis is not shared.
We could, of course, try out violins, point plots, but here,
just for simplicity, I'm using the box.
This kind of graphic allows you to go after and try and
create a story to explain what each of the clusters mean.
Cluster 0 is high values of flavonoids and low values for this other variable.
Cluster 0 has less variation for malic acid,
whereas cluster 2 has much higher variation in malic acid.
Cluster 0 has high values of proline,
where the other two clusters have lower values.
Again, bringing in the cluster results to your original
variables will help you communicate what's going on,
even though PCA will help streamline the visualization to
help you identify just how separated the clusters are.
So I know this was a long recording, I know we covered a lot.
It began by reviewing how to apply k means,
how do I find the optimal number of clusters.
But then it introduced a second style of hierarchical clustering
that still uses distances, but adds in another layer of,
how do you define similarity between clusters?
This allows us to create clusters within clusters so
that you do not have to determine upfront how many clusters to use.
It effectively tries out all possible combinations for you.
The Ward method, by the way, the reason why I like it,
it's essentially an optimization approach, because when it creates the clusters,
it's doing so to try and minimize within cluster variance.
So the reason why it typically creates these nice looking dendrograms is
it's trying to give you very tight clusters in the first place.