Source: https://www.coursera.org/learn/mds-introduction-to-data-centric-computing/lecture/mUyFz/introduction-to-hierarchical-clustering-advantages-setup-and-visualization-with

English
>> But there is another approach to clustering.
A completely different algorithm,
a completely different method known as hierarchical clustering.
Hierarchical clustering still
bases similarity on distances, but
we do not need to specify the number of clusters up front.
Instead, as the name states,
clusters within clusters are formed.
Hierarchical clustering will in effect try
out all possible number of clusters for us.
I personally very rarely use k means.
I prefer hierarchical clustering because it's
literally going to try them all out for us.
And it provides a useful visualization to interpret the clustering results,
one that I think is better than working with k means.
We will use scikit-learn quite often in this course,
but for hierarchical clustering we will use functions from SciPy.
SciPy for years was the sister module to NumPy,
oftentimes 10, 15 years ago.
If somebody said I'm using SciPy,
they really meant they're using NumPy with some other things.
But that has changed over the last five years.
Last seven years, as NumPy has become more and
more associated with deep learning.
But SciPy still exists.
It's actually where there are more direct ports.
And I mean, I'll just say a straight up copying from Matlab.
So SciPy is really the how Matlab was put into Python.
But let's go ahead and use SciPy here.
Import the functions we need from SciPy.
From scipy.cluster,
Import hierarchy H-I-E.
And then you can see I'm tab completing, H-I-E-R-A-R-C-H-Y.
See, I always spell this wrong, H-I-E-R-A-R-C-H-Y.
Hierarchical clustering
still calculates distances.
Therefore it is critical to use
standardized variables, just like k means.
So there's no difference in terms of the preprocessing you need to do.
You still must pre process.
But there is another kind of distance that
needs calculated for hierarchical clustering.
We will discuss that other kind of
distance after examining some results.
So the easiest way to understand this is to try it out and
then look at the results and talk about it.
This other kind of distance is referred to as the linkage.
And again, it will make more sense once we see the results.
We will begin with something called complete linkage.
Running the hierarchical clustering is actually quite straightforward.
It doesn't require a lot of effort.
Once you have pre processed everything, let's go ahead and run it.
I'm going to assign the result to h for hierarchical clust for
clustering, complete, equal to hierarchy.complete.
So hierarchy is really a sub module.
It contains many functions.
The one argument we need to provide to complete is the standardized columns for
the numeric variables in y contained in x1.
H complete is a NumPy array.
It's a very complicated NumPy array.
We will visualize the results with
a specialized figure called a dendrogram.
This presents the clustering results in a tree like structure.
To create the dendrogram,
I feel it's best to follow the following recipe.
You must first initialize the figure.
Then you need to assign the result to a variable.
And in the SciPy documentation, they typically assign this
to an object named Dn for stands for dendrogram.
But Dn gets assigned from the hierarchy submodule.
Apply the dendrogram function to the h clust object.
So we ran 8th hierarchy complete, assigned it to h complete.
And I'm now passing that object into
the argument of hierarchy.dendrogram.
This will be a figure, and so we need to plt show it.
So again, I have found that over the years, assigning the figure,
assigning the result of hierarchy dendrogram to an object.
Even if I never use or manipulate that object.
Gives me the cleanest results and doesn't display any weird text or
information to the screen.
So this is the way that I have done this now for many years.
The resulting figure can be difficult to look at using the default figure sizes.
And so I'm going to increase the width
of the figure, stretch it out, and
then specify a height 12, 6.
Now, there's a lot going on in this figure, but there are two major things.
At the very bottom, do you see all this text?
This is called the nodes or
leaves of the figure.
At the very top, you see this big horizontal line.
This horizontal line is called the root of the tree.
The leaves represent each individual
observation, or row in the data set.
The root represents combining
all rows into one cluster.
So the hierarchical clustering algorithm
literally tries out two uninteresting cases.
One cluster containing everything.
Again, uninteresting because we're just saying all
observations are similar and contained in one unit.
And then the other extreme case that is uninteresting.
Every single observation is contained in its own cluster.
So for the wine data set, the bottom row here is 178 clusters.
Again, this is uninteresting, saying everything is completely and
totally different.